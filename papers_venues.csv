paperID|doi|journal_name|volume|volume_id|pages|abstract|publicationVenue_name|edition_id|city_venue|title|year|authorIDs|authorNames|fields|citedPaperID|reviewerIDs|reviewsApprovements|reviewsDesc
273dfbcb68080251f5e9ff38b4413d7bd84b10a1|10.1145/1961189.1961199||||27:1-27:27|LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cairo_2011|Cairo|LIBSVM: A library for support vector machines|2011|1711460|Chih-Chung Chang;Chih-Jen Lin|Computer Science;Mathematics|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;a86171e13f84fe32212dd7fb6a1c31a34a47155f;a40f97770296c7fca2e5361cbceba3f4aae399e0;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;c6a83c4fcc99ba6753109301949c5b7cfa978079|49933077;11541096;1680188|True;True;False|desc;desc;desc
34f25a8704614163c4095b3ee2fc969b60de4698|10.5555/2627435.2670313||||1929-1958|"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."|Workshop on AI for Earth|Workshop_on_AI_for_Earth_Beijing_2014|Beijing|Dropout: a simple way to prevent neural networks from overfitting|2014|2897313;1695689;2064160;1701686;145124475|Nitish Srivastava;Geoffrey E. Hinton;A. Krizhevsky;I. Sutskever;R. Salakhutdinov|Computer Science|a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;cc5afe344cc7ed7acd68a28b9774ea8023a162dc|3172075;47909642;3469125|True;True;True|desc;desc;desc
668b1277fbece28c4841eeab1c97e4ebd0079700|10.1117/1.2819119||||I-XX, 1-738|Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.|AAAI|AAAI_Moscow_2006|Moscow|Pattern Recognition and Machine Learning|2006|1764325|Radford M. Neal|Computer Science;Mathematics|f3203d0bdefc9670ed508ca776d08aa9f024bafa;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;df2a7756382540e92895f10703cec32d50c4f316;45c9f19b1eb46095e61f3c1a9970a6161c13a861;611544418ca53cdad254df444addc7814abcfddc|2274883;1796770;1780687|False;True;True|desc;desc;desc
0b544dfe355a5070b60986319a3f51fb45d1348e|10.3115/v1/D14-1179||||1724-1734|In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_London_2014|London|Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation|2014|1979489;3158246;1854385;3335364;2076086;144518416;1751762|Kyunghyun Cho;B. V. Merrienboer;Çaglar Gülçehre;Dzmitry Bahdanau;Fethi Bougares;Holger Schwenk;Yoshua Bengio|Computer Science;Mathematics|825ca26af5a2a510dbc1a7b97587212bc98ae968;5966d7c7f60898d610812e24c64d4d57855ad86a;09622b0c84bf812814af5b64b0c83dce796899c4;92ace17730c2173e642934d64f96d359697b7a93;94549a171a61039ed1f9b5954ce42181c574ccc3;a20bfec3c95aad003dcb45a21a220c19cca8bb66;0165568bcc1a819c18564567f2ec15d859be2519;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;877374c2913b787ee9f958f39e31c75d39ebcc15;8c8215b7f8111839f0066010a530a3a9f57ba15e|2264432703;1773836;1979856|True;True;False|desc;desc;desc
4c75b748911ddcd888c5122f7672f69caa5d661f|10.1080/00401706.1999.10485951||||377-378|A machine learning system, in general, learns from the environment, but statistical machine learning programs (systems) learn from the data. This chapter presents techniques for statistical machine learning using Support Vector Machines (SVM) to recognize the patterns and classify them, predicting structured objects using SVM, k-nearest neighbor method for classification, and Naive Bayes classifiers. The artificial neural networks are presented with brief introduction to error-correction rules, Boltzmann learning, Hebbian rule, competitive learning rule, and deep learning. The instance-based learning is treated in details with its algorithm and learning task. The chapter concludes with a summary, and a set of practice exercises.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Tokyo_2021|Tokyo|Statistical Learning Theory|2021|2348306915|Yuhai Wu|Computer Science;Mathematics|72e93aa6767ee683de7f001fa72f1314e40a8f35;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;bd898f483476e3dcacf83cd85efc64e6319da0e1;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;78947497cbbffc691aac3f590d972130259af9ce;fbf1c51548ffc9b9e538befcd71529365af23d15;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;ec6200bdcc23b79a71555962cde50306c4029f1a;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;eed9fa4483cab37eacd59db0fac4b1441431ee85;b16408a97170785fb216c9e8b7920d64f478fbc8;184ac0766262312ba76bbdece4e7ffad0aa8180b;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;5e095981ebf4d389e9356bd56e59e0ade1b42e88;8c8215b7f8111839f0066010a530a3a9f57ba15e|2285465435;33870107;113947684|True;True;True|desc;desc;desc
a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|10.1109/TKDE.2009.191||||1345-1359|A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_Rio_de_Janeiro_2010|Rio de Janeiro|A Survey on Transfer Learning|2010|1746914;152290618|Sinno Jialin Pan;Qiang Yang|Computer Science|0090023afc66cd2741568599057f4e82b566137c;398c296d0cc7f9d180f84969f8937e6d3a413796;1dd6c46d868accd5acffd02e4b08b003534b924e;076af19e50f022ccbe5bf16f413f79b5c6904c05;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;d02927d4de4a2a51cced4970da04b812cbee4342;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;12439a6ff384e95ee2262ee982bc055534e30487;45c9f19b1eb46095e61f3c1a9970a6161c13a861;e50f4d3316d13841c287dcdf5479d7820d593571;e9126a98de0c39dcffe4c4f5158e037460196724;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;34f25a8704614163c4095b3ee2fc969b60de4698|1792616;152394142;1982950|True;False;False|desc;desc;desc
fa25610fb8586c2b50a3654edc5bb42fa7fc4729|10.1198/jasa.2004.s339||||567 - 567|In the words of the authors, the goal of this book was to “bring together many of the important new ideas in learning, and explain them in a statistical framework.” The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse  elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related  elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a “data modeling culture,” where the model is paramount. Breiman argued instead for an “algorithmic modeling culture,” with emphasis on black-box types of prediction. Breiman’s article is controversial, and in his discussion, Efron objects that “prediction is certainly an interesting subject, but Leo’s paper overstates both its role and our profession’s lack of interest in it.” Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a  nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called “market basket analysis.” The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book’s usefulness as a reference, and so I am happier with the book as it was written.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rome_2004|Rome|The Elements of Statistical Learning: Data Mining, Inference, and Prediction|2004|144108246|D. Ruppert|Computer Science;Mathematics|dd971c07879e1ce12b06991319528c06280eeb9b;3fa5f45ddbd5184f10bfb92e367493c5a344f207;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;877374c2913b787ee9f958f39e31c75d39ebcc15;9670485f526f2254c0f34e64d9ca06f665a0bd17;5c7e5248d9eb7f373f10277410bf8506160907ea|2517825;1780112;1905807|True;True;True|desc;desc;desc
184ac0766262312ba76bbdece4e7ffad0aa8180b|10.1109/TPAMI.2013.50||||1798-1828|The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.|NeurIPS|NeurIPS_Beijing_2012|Beijing|Representation Learning: A Review and New Perspectives|2012|1751762;1760871;145467703|Yoshua Bengio;Aaron C. Courville;Pascal Vincent|Computer Science;Medicine;Mathematics|c43025c429b1fbf6f1379f61801a1b40834d62e7;22adb2413901b74128f2a02584dafa77afbd8d60;04fd278c01df1564e741b4c6e052fc1c5924ab8d;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;12d1d070a53d4084d88a77b8b143bad51c40c38f;884895a86fe15cb9601df4a15a1475c07f28da3c;62ccd99a65bfc7c735ae1f33b75b107665de95df;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;e0535dedb8607d83cd2614317c99913378e89e26;872352b0a53ab6cbb4420f81df64d215d86c7d9b;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;9f387ce140c59a44eaeeea590087351461345164;885af28a751553be48a25b411a5d492767d4cf65;9691f67f5075bde2fd70da0135a4a70f25ef042b|1695217;145295514;1399279909|True;True;True|desc;desc;desc
339a8e4cb0eba77675711ac255ac2a5d7ede1d53|10.1093/molbev/msab120||||3022 - 3027|Abstract The Molecular Evolutionary Genetics Analysis (MEGA) software has matured to contain a large collection of methods and tools of computational molecular evolution. Here, we describe new additions that make MEGA a more comprehensive tool for building timetrees of species, pathogens, and gene families using rapid relaxed-clock methods. Methods for estimating divergence times and confidence intervals are implemented to use probability densities for calibration constraints for node-dating and sequence sampling dates for tip-dating analyses. They are supported by new options for tagging sequences with spatiotemporal sampling information, an expanded interactive Node Calibrations Editor, and an extended Tree Explorer to display timetrees. Also added is a Bayesian method for estimating neutral evolutionary probabilities of alleles in a species using multispecies sequence alignments and a machine learning method to test for the autocorrelation of evolutionary rates in phylogenies. The computer memory requirements for the maximum likelihood analysis are reduced significantly through reprogramming, and the graphical user interface has been made more responsive and interactive for very big data sets. These enhancements will improve the user experience, quality of results, and the pace of biological discovery. Natively compiled graphical user interface and command-line versions of MEGA11 are available for Microsoft Windows, Linux, and macOS from www.megasoftware.net.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Los_Angeles_2021|Los Angeles|MEGA11: Molecular Evolutionary Genetics Analysis Version 11|2021|3066242;48226007;2110208881|K. Tamura;G. Stecher;Sudhir Kumar|Computer Science;Biology;Medicine|43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be|2520493;6593936;2064366548|True;True;True|desc;desc;desc
395de0bd3837fdf4b4b5e5f04835bcc69c279481|10.18653/v1/2020.acl-main.703|JAMA|15.0|7890-1234_15|7871-7880|We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.||||BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension|2019|35084211;11323179;39589154;2320509;113947684;39455775;1759422;1982950|M. Lewis;Yinhan Liu;Naman Goyal;Marjan Ghazvininejad;Abdel-rahman Mohamed;Omer Levy;Veselin Stoyanov;Luke Zettlemoyer|Computer Science;Mathematics;Linguistics|45557cc70cd6989ab6b03e5aeb787e34299099f7;63861fbeb7ec41986b85965b9780b428d919919e;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;78989616eeeac55b202e3e4205225e7135054185;55f44d39630646f36eac91358f8f27d1bead384c;94549a171a61039ed1f9b5954ce42181c574ccc3;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;dd971c07879e1ce12b06991319528c06280eeb9b;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;58a8bead87c8c1e37460dce28285c053c270f6e7;22fe619996b59c09cb73be40103a123d2e328111;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac|1401020033;1699108;2916235|True;True;True|desc;desc;desc
7bb6bdf4ed609e5e72d4206d1b308323e73dceec|10.1162/artl.1997.3.1.63||||63-65|"An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a ""Brief History"" and ""Future Directions."" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering."|ECCV|ECCV_Cape_Town_1997|Cape Town|An Introduction to Genetic Algorithms.|1997|1403108392|D. Heiss-Czedik|Computer Science;Mathematics|4a554da55fd9ff76c99e25d2ce937b225dc1100c;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;2346d121f38fc19c77e0b062415519843f478163;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;184ac0766262312ba76bbdece4e7ffad0aa8180b;f86f1748d1b6d22870f4347fd5d65314ba800583;a88b3be9b2db0319f8880e60a131b3060dba1eb7;9d75cc322a4e06d0a3a868cb91b04219a289c12c;c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45;5a391667242b4a631acdd5917681b16a86523987;33e46a618fdb22d46951f548d6ceeb384e7f1687;6adf016e7531c91100d3cf4a74f5d4c87b26b528;d02927d4de4a2a51cced4970da04b812cbee4342;86cff4d050beb90fed2e1ceac8940c8221b120aa;825ca26af5a2a510dbc1a7b97587212bc98ae968;65d53938a12c77e7920b8eb3a49df249c978ba3f;d0ab11de3077490c80a08abd0fb8827bac84c454|1735243;1410457573;1718798|True;True;True|desc;desc;desc
fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4|10.1198/jasa.2003.s270||||781-781|Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Madrid_2003|Madrid|Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond|2003|2285968829|Christopher K. I Williams|Computer Science;Mathematics|81a4fd3004df0eb05d6c1cef96ad33d5407820df;49bdeb07b045dd77f0bfe2b44436608770235a23;ac12c9b9e35e58b55d85a97c47886a7371c14afa;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;48e752c719d33ff55b3b3bec3538727f8ce69399;b3de1062d8a462dfdc2938558258f8884abe9f4e;877374c2913b787ee9f958f39e31c75d39ebcc15;a85e512d8845bd007b0866b4a97e8341463f8190;3aa1b70fdc97ae96091c5fb39cd911015ac5253e|66083398;152891866;144002190|False;False;False|desc;desc;desc
12d1d070a53d4084d88a77b8b143bad51c40c38f|10.1613/jair.301|Nature|58.0|1234-5678_58|237-285|"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ""reinforcement."" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."||||Reinforcement Learning: A Survey|1996|1709512;144885169;1760402|L. Kaelbling;M. Littman;A. Moore|Computer Science|033f25ad905ef2ed32a8331cf38b83953ff15922;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;175e37bca3762b3a52c6a0e153060b98a251d061;f94455176857303605ad423599385a2341c568eb;f86f1748d1b6d22870f4347fd5d65314ba800583;db68a79e59291b85e10300b79c43843b651aa195;441c31274f4535a4a50892c1ad6e19eacfd17f8c|2418520;39189579;98271906|True;True;True|desc;desc;desc
a675fe5a7d99ac6f7ff91fa084462faefe616148|10.1145/950566.950595|Cell|56.0|5678-9012_56|20|Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.||||What video games have to teach us about learning and literacy|2007|34622402|J. Gee|Education;Computer Science;Sociology;Psychology|2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;1f87134a630c2dbb9a3645ba658954f00b620a77;9d75cc322a4e06d0a3a868cb91b04219a289c12c;4157ed3db4c656854e69931cb6089b64b08784b9;0e779fd59353a7f1f5b559b9d65fa4bfe367890c;01f702f8b1f9d1314587015f1f038af4d5735e77;21cea8f56a0d067d640f923b2d69e18ed5542f6d;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;86cff4d050beb90fed2e1ceac8940c8221b120aa|11323179;145336368;143816239|True;False;True|desc;desc;desc
0e90a73f03902cbe915af1aff54ea7f0b3373680|10.1609/aimag.v22i2.1566|Science|2.0|3456-7890_2|103-104|This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.||||An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods|2001|2146325920|Tong Zhang|Computer Science;Mathematics|4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;b5887d18420e8ac4f4fa4c83c4952138fd956702;bcce96a2a074448953fc61a29a84afbdfc8db55a;a88b3be9b2db0319f8880e60a131b3060dba1eb7;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;1f87134a630c2dbb9a3645ba658954f00b620a77;9d46dc975aeed3f96bddb144079b50238f746ecd;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;e4a85af3f5dc41e13dc2cae9ee851953709b764e|2842984;38174487;145158805|False;True;True|desc;desc;desc
d04d6db5f0df11d0cff57ec7e15134990ac07a4f|10.1561/2200000006|Radiology|32.0|0123-4567_32|1-127|Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.||||Learning Deep Architectures for AI|2007|1751762|Yoshua Bengio|Computer Science|b10e4deadf978d8fd6eec97ff18888629f4261ab;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;3fa5f45ddbd5184f10bfb92e367493c5a344f207;58a8bead87c8c1e37460dce28285c053c270f6e7;4c75b748911ddcd888c5122f7672f69caa5d661f;1904d633fca15140e35d893637232803b6dde6d9;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;de2be42659be5c43c1a992b5d7fe6daf14e571dd;5966d7c7f60898d610812e24c64d4d57855ad86a|8952312;50860274;143728483|True;True;True|desc;desc;desc
6ec7c724aa1d906e9e9f81c58497adddb22175b8|10.1017/CBO9780511801389.013||||I-XIII, 1-189|From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_Athens_2000|Athens|An Introduction to Support Vector Machines and Other Kernel-based Learning Methods|2000|1685083;1404459229|N. Cristianini;J. Shawe-Taylor|Computer Science;Mathematics|18d026ec5d0eebd17ee2c762da89540c0b3d7bde;35b3233e521f1e9a34837c30be1957858f8f35fe;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;c2b381b24aabf237394059fed7920cd6fd0e67b8;45557cc70cd6989ab6b03e5aeb787e34299099f7;d997919c30fa6711bc5c25cf8c8aea34fac27b91;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;7e7eb0f93c9550d7336f4bbfad5fe89604295705;49bdeb07b045dd77f0bfe2b44436608770235a23;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0|1806678;145341374;1775384|True;True;False|desc;desc;desc
df40ce107a71b770c9d0354b78fdd8989da80d2f|10.1109/SP.2017.49||||39-57|Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2016|New York|Towards Evaluating the Robustness of Neural Networks|2016|2483738;145394689|Nicholas Carlini;D. Wagner|Computer Science|c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;4e6238c8613b5b81f81552939bce33296aedfbfe;8de174ab5419b9d3127695405efd079808e956e8;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;53834f0ee8df731cf0e629cd594dce0afaaa3d97;65b16da51891a6b98140d425804c8a0fd0299219;f4a5503783487eba5c5e34b1d02c09016b244b1d;033f25ad905ef2ed32a8331cf38b83953ff15922|39589857;2231981905;1767318|True;True;True|desc;desc;desc
88816ae492956f3004daa41357166f1181c0c1bf|10.1162/089976603321780317|Nature|40.0|1234-5678_40|1373-1396|One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.||||Laplacian Eigenmaps for Dimensionality Reduction and Data Representation|2003|145520115;1770745|M. Belkin;P. Niyogi|Computer Science;Mathematics|5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;1dd6c46d868accd5acffd02e4b08b003534b924e;e50f4d3316d13841c287dcdf5479d7820d593571;9691f67f5075bde2fd70da0135a4a70f25ef042b;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;3def68bd0f856886d34272840a7f81588f2bc082;574449170f293dfa868771e9ee0403b56a19b9e9;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;a88b3be9b2db0319f8880e60a131b3060dba1eb7;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;049aca6228fb68a263369380eda6d9a4fcbdb382;8de174ab5419b9d3127695405efd079808e956e8|2722839;46502933;2116632761|True;True;True|desc;desc;desc
81a4fd3004df0eb05d6c1cef96ad33d5407820df|10.1109/TNNLS.2020.2978386||||4-24|Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Paris_2019|Paris|A Comprehensive Survey on Graph Neural Networks|2019|2109557884;2585415;31370754;2062835;48934799;144019071|Zonghan Wu;Shirui Pan;Fengwen Chen;Guodong Long;Chengqi Zhang;Philip S. Yu|Computer Science;Medicine;Mathematics||1716301;29905643;2053884612|True;True;True|desc;desc;desc
9eb715fe0347445a2d63518cbb476d345ba86233|10.1111/J.1600-0587.2012.07348.X||||27-46|"Collinearity refers to the non independence of predictor variables, usually in a regression-type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold-based pre-selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor-response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine-learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold-based pre-selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold-based pre-selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the ‘folk lore’-thresholds of correlation coefficients between predictor variables of |r| >0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre-analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them."|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Lisbon_2013|Lisbon|Collinearity: a review of methods to deal with it and a simulation study evaluating their performance|2013|2428490;2686171;143969664;2201937;145905360;2071828386;122753513;144274747;7512893;144335550;5279341;1807250;3476431;33805504;40629103;2652207;4315323;2905518|C. Dormann;J. Elith;S. Bacher;C. M. Buchmann;G. Carl;Gabriel Carré;J. Márquez;B. Gruber;Bruno Lafourcade;P. Leitão;T. Münkemüller;C. McClean;P. Osborne;B. Reineking;B. Schröder;A. Skidmore;D. Zurell;S. Lautenbach|Computer Science;Environmental Science||145034054;51952495;9706655|True;True;True|desc;desc;desc
0023582fde36430c7e3ae81611a14e558c8f4bae|10.1561/0400000042||||211-407|The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations — not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey — there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Beijing_2014|Beijing|The Algorithmic Foundations of Differential Privacy|2014|1781565;1682008|C. Dwork;Aaron Roth|Computer Science;Mathematics|395de0bd3837fdf4b4b5e5f04835bcc69c279481;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;2077d0f30507d51a0d3bbec4957d55e817d66a59;6aae0dc122102693e8136856ffc8b72df7f78386;f94455176857303605ad423599385a2341c568eb;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;7ad66cba3b7e3abae7ef33122588512a146f7f77;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;c6a83c4fcc99ba6753109301949c5b7cfa978079|1764325;1709589;153096457|False;True;True|desc;desc;desc
bd1f14e7531220c39fad8f86985cce7b283f035d|10.1017/CBO9780511809682||||I-XIV, 1-462|Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_New_York_2004|New York|Kernel Methods for Pattern Analysis|2004|1404459229;1685083|J. Shawe-Taylor;N. Cristianini|Computer Science;Mathematics|e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;b16408a97170785fb216c9e8b7920d64f478fbc8;1dd6c46d868accd5acffd02e4b08b003534b924e;ac12c9b9e35e58b55d85a97c47886a7371c14afa;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;1051280d2b825c04f27d231aba0f8284bb297880;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;668b1277fbece28c4841eeab1c97e4ebd0079700;61394599ed0aabe04b724c7ca3a778825c7e776f;48234756b7cf798bfeb47328f7c5d597fd4838c2;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;a244c47a1d4a8c2894b22807df8c7eec16cc110a;3df952d4a724655f7520ff95d4b2cef90fff0cae;c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;d63b884d5ebc739f6e1bdf861fa9276260781404;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7|40337361;39917910;145273596|True;True;True|desc;desc;desc
2e55ba6c97ce5eb55abd959909403fe8da7e9fe9|10.1073/pnas.1611835114||||3521 - 3526|Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.|CVPR|CVPR_Toronto_2016|Toronto|Overcoming catastrophic forgetting in neural networks|2016|2066516991;1996134;3422052;144056327;2755582;2228824;8181864;34660073;34505275;1398898827;48987704;2388737;2106164;2315504|J. Kirkpatrick;Razvan Pascanu;Neil C. Rabinowitz;J. Veness;Guillaume Desjardins;Andrei A. Rusu;Kieran Milan;John Quan;Tiago Ramalho;A. Grabska-Barwinska;D. Hassabis;C. Clopath;D. Kumaran;R. Hadsell|Computer Science;Medicine;Mathematics|0090023afc66cd2741568599057f4e82b566137c;dd971c07879e1ce12b06991319528c06280eeb9b;427b168f490b56716f22b129ac93aba5425ea08f;a3461eaf51016f9d6e85ea47173b27e019e801c4;9e475a514f54665478aac6038c262e5a6bac5e64;33e46a618fdb22d46951f548d6ceeb384e7f1687|1719124;10398264;2462983|True;True;True|desc;desc;desc
a9763afda62e960c35c80681f805ddecbef14a92|10.1097/00000446-198803000-00042||||395|"Preface Part I. An Overview Introduction Part II. Some Images of Organization 2. Mechanization Takes Command: Organizations as Machines Machines, Mechanical Thinking, and the Rise of Bureaucratic Organization The Origins of Mechanistic Organization Classical Management Theory: Designing bureaucratic organizations Scientific Management Strengths and Limitations of the Machine Metaphor 3. Nature Intervenes: Organizations as Organisms Discovering Organizational Needs Recognizing the Importance of Environment: Organizations as Open Systems Contingency Theory: Adapting Organization to Environment The Variety of the Species Contingency Theory: Promoting Organizational Health and Development Natural Selection: The Population-Ecology View of Organizations Organizational Ecology: The Creation of Shared Futures Strengths and Limitations of the Organismic Metaphor 4. Learning and Self-Organization: Organizations as Brains Images of the Brain Organizations as Information Processing Brains Creating Learning Organizations Cybernetics, Learning, and Learning to Learn Can Organizations Learn to Learn? Guidelines for ""Learning Organizations"" Organizations as Holographic Brains Principles of Holographic Design Strengths and Limitations of the Brain Metaphors 5. Creating Social Realty: Organizations as Cultures Culture and Organization Organization as a Cultural Phenomenon Organization and Cultural Context Corporate Cultures and Subcultures Creating Organizational Reality Culture: Rule Following or Enactment? Organization: The enactment of a Shared Reality Strengths and Limitations of the Cultural Metaphor 6. Interests, Conflict, and Power: Organizations as Political Systems Organizations as Systems of Government Organizations as Systems of Political Activity Analyzing Interests Understanding Conflict Exploring Power Managing Pluralist Organizations Strengths and Limitations of the Political Metaphor 7. Exploring Plato's Cave: Organizations as Psychic Prisons The Trap of Favored Ways of Thinking Organization and the Unconscious Organization and Repressed Sexuality Organization and the Patriarchal Family Organization, Death, and Immortality Organization and Anxiety Organization, Dolls, and Teddy Bears Organization, Shadow, and Archetype The Unconscious: A Creative and Destructive Force Strengths and Limitations of the Psychic Prison Metaphor 8. Unfolding Logics of Change: Organization as Flux and Transformation Autopoiesis: Rethinking Relations With the Environment Enactment as a Form of Narcissism: Organizations Interact With Projections of Themselves Identity and Closure: Egocentrism Versus Systemic Wisdom Shifting ""Attractors"": The Logic of Chaos and Complexity Managing in the Midst of Complexity Loops, Not Lines: The Logic of Mutual Causality Contradiction and Crisis: The Logic of Dialectical Change Dialectical Analysis: How Opposing Forces Drive Change The Dialectics of Management Strengths and Limitations of the Flux and Transformation Metaphor 9. The Ugly Face: Organizations as Instruments of Domination Organization as Domination How Organizations Use and Exploit Their Employees Organization, Class, and Control Work Hazards, Occupational Disease, and Industrial Accidents Workaholism and Social and Mental Stress Organizational Politics and the Radicalized Organization Multinationals and the World Economy The Multinationals as World Powers Multinationals: A Record of Exploitation? Strengths and Limitations of the Domination Metaphor Part III. Implications For Practice 10. The Challenge of Metaphor Metaphors Create Ways of Seeing and Shaping Organizational Life Seeing, Thinking, and Acting in New Ways 11. Reading and Shaping Organizational Life The Multicom Case Interpreting Multicom Developing and Detailed Reading and ""Storyline"" Multicom From Another View ""Reading"" and Emergent Intelligence 12. Postscript Bibliographic Notes Introduction The Machine Metaphor The Organismic Metaphor The Brain Metaphor The Culture Metaphor The Political Metaphor The Psychic Prison Metaphor The Flux and Transformation Metaphor The Domination Metaphor The Challenge of Metaphor Reading and Shaping Organizational Life Postscript Bibliography"|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_1988|New York|Images of Organization|1988|49811299;2053884612|J. Alexander;G. Morgan|Medicine;Sociology;Business;Psychology|d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;cd49acefc8d51e324aa562e5337e1c2aff067053;0165568bcc1a819c18564567f2ec15d859be2519;88816ae492956f3004daa41357166f1181c0c1bf;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;33e46a618fdb22d46951f548d6ceeb384e7f1687;e50f4d3316d13841c287dcdf5479d7820d593571;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|81840293;145002796;1766703|True;False;True|desc;desc;desc
d422df8bff4e677a3077635db116679d25142bfc|10.1126/science.aaa8415||||255 - 260|Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.|COLT|COLT_Paris_2015|Paris|Machine learning: Trends, perspectives, and prospects|2015|1694621;2066277988|Michael I. Jordan;T. Mitchell|Computer Science;Medicine|971766088dfaf63fb55e6f0190b14f28f2c98ad0;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;94549a171a61039ed1f9b5954ce42181c574ccc3;24e6c5bfe9bb0751e5708b501d04e860011b2953|1714004;7991309;2121780|True;False;False|desc;desc;desc
08b43d84e6747e370ef307e2ada50675b414514a|10.1109/TNN.2005.845141||||645-678|Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.|ICML|ICML_Paris_2005|Paris|Survey of clustering algorithms|2005|144996246;145033828|Rui Xu;D. Wunsch|Computer Science;Medicine;Mathematics|b24972552161cd9eda729e748762a73430983e3a;d0ab11de3077490c80a08abd0fb8827bac84c454;d9665992ee36699b8ae4a2e2294552cd4be9003a;611544418ca53cdad254df444addc7814abcfddc;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;8c8215b7f8111839f0066010a530a3a9f57ba15e;53834f0ee8df731cf0e629cd594dce0afaaa3d97;35b3233e521f1e9a34837c30be1957858f8f35fe;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;bb144c04b9eb44579b19d21c3d5954401408440b;9eb715fe0347445a2d63518cbb476d345ba86233;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;175e37bca3762b3a52c6a0e153060b98a251d061;2346d121f38fc19c77e0b062415519843f478163;cbac8b0d82ea8e9251d5530695841d816cb196b9;8592e46a5435d18bba70557846f47290b34c1aa5;65b16da51891a6b98140d425804c8a0fd0299219|3089272;1733689;2348963575|True;False;False|desc;desc;desc
3aa1b70fdc97ae96091c5fb39cd911015ac5253e|10.1111/J.2006.0906-7590.04596.X||||129-151|Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.|Workshop on Machine Learning for Health|Workshop_on_Machine_Learning_for_Health_Beijing_2006|Beijing|Novel methods improve prediction of species' distributions from occurrence data|2006|2686171;2346691157;2346817245;2346775264;2262660352;1830349;2130462;2312900424;4429495;2239580655;2347034162;2243570610;5249055;4453374;2252770872;2346939080;2264432703;2239585787;2252257837;30352875;2346777264;1403810063;1716301;2060434571;2346996570;46409715;2249620756|J. Elith;Catherine H. Graham;Robert P. Anderson;Miroslav Dudı́k;Simon Ferrier;A. Guisan;R. Hijmans;Falk Huettmann;J. Leathwick;Anthony Lehmann;Jin Li;Lúcia G. Lohmann;Bette A. Loiselle;G. Manion;Craig Moritz;Miguel Nakamura;Yoshinori Nakazawa;J. M. Overton;A. T. Peterson;Steven J. Phillips;Karen Richardson;R. Scachetti-Pereira;R. Schapire;Jorge Soberón;Stephen Williams;M. Wisz;Niklaus E. Zimmermann|Computer Science;Environmental Science;Biology|3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;0090023afc66cd2741568599057f4e82b566137c;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;4157ed3db4c656854e69931cb6089b64b08784b9;694bdf6e5906992dad2987a3cc8d1a176de691c9;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4|145034054;143945011;143729959|False;False;True|desc;desc;desc
4609f6bdc3beab00c9beceaa12dd8101fefe6f1c|10.1109/72.788640||||"
          988-99
        "|Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).|ICAPS|ICAPS_Berlin_1999|Berlin|An overview of statistical learning theory|1999|50560492|V. Vapnik|Computer Science;Medicine;Mathematics||40655309;3090725;1740765|False;True;True|desc;desc;desc
78947497cbbffc691aac3f590d972130259af9ce|10.5555/1577069.1577078|PNAS|63.0|4567-8901_63|1473-1480|The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.||||Distance Metric Learning for Large Margin Nearest Neighbor Classification|2005|7446832;1796044|Kilian Q. Weinberger;L. Saul|Computer Science;Mathematics|9d7c04de906823a60d3ccb5f510fd0029af5c8b0;a486e2839291111bb44fa1f07731ada123539f75;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;8592e46a5435d18bba70557846f47290b34c1aa5;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;24e6c5bfe9bb0751e5708b501d04e860011b2953;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;f986968735459e789890f24b6b277b0920a9725d;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;b8ebda42e272d3617375118542d4675a0c0e501d;64be9999b68e12d260ba7423f6b55ffd41552ad3;5c5e69387020d7ca7d49487ca841958dc5e08ce6;771479c18b586eafae21baf262a220aaa7b2eef6;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;45557cc70cd6989ab6b03e5aeb787e34299099f7;427b168f490b56716f22b129ac93aba5425ea08f;86cff4d050beb90fed2e1ceac8940c8221b120aa;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;338a891907dce447da9a0fa2f27221bd35164163;6aae0dc122102693e8136856ffc8b72df7f78386|1996134;1878461;8181864|True;True;True|desc;desc;desc
07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b|10.1561/2200000083||||1-210|Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.|CVPR|CVPR_Toronto_2019|Toronto|Advances and Open Problems in Federated Learning|2019|3115341;145057514;15519668;1702915;1702172;10754103;2039588;143676545;1709589;49326047;1410457573;40464010;2116660698;33685819;40449749;145511365;2529354;1974678;1708469;1753355;31927890;51222147;3382735;2044655623;39756252;2456863;47197693;144225970;10398264;32139366;2823893;3018662;143812875;1792616;1614034792;143615345;81080659;1718786;2064241030;1801719;1702744;2072589474;1878835;145711633;143711382;2118727912;2127057;8908922;9486035;2444919;2927870;30880777;2291135512;144897102;153096457;1815972;2110984588;49113001|P. Kairouz;H. B. McMahan;Brendan Avent;A. Bellet;M. Bennis;A. Bhagoji;Keith Bonawitz;Zachary B. Charles;Graham Cormode;Rachel Cummings;Rafael G. L. D'Oliveira;S. E. Rouayheb;David Evans;Josh Gardner;Zachary Garrett;Adrià Gascón;Badih Ghazi;Phillip B. Gibbons;M. Gruteser;Zaïd Harchaoui;Chaoyang He;Lie He;Zhouyuan Huo;Ben Hutchinson;Justin Hsu;Martin Jaggi;T. Javidi;Gauri Joshi;M. Khodak;Jakub Konecný;A. Korolova;F. Koushanfar;Oluwasanmi Koyejo;Tancrède Lepoint;Yang Liu;Prateek Mittal;M. Mohri;R. Nock;A. Özgür;R. Pagh;Mariana Raykova;Hang Qi;Daniel Ramage;R. Raskar;D. Song;Weikang Song;Sebastian U. Stich;Ziteng Sun;A. Suresh;Florian Tramèr;Praneeth Vepakomma;Jianyu Wang;Li Xiong;Zheng Xu;Qiang Yang;Felix X. Yu;Han Yu;Sen Zhao|Computer Science;Mathematics|273dfbcb68080251f5e9ff38b4413d7bd84b10a1;7ab0f0da686cd4094fd96f5a30e0b6072525fd09|2110151928;40511414;2114485554|True;True;True|desc;desc;desc
5c45a5d05ac564adb67811eeb9d41d6460c70135|10.1001/jama.2016.17216|Radiology|95.0|0123-4567_95|"
          2402-2410
        "|"Importance
Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.


Objective
To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.


Design and Setting
A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.


Exposure
Deep learning-trained algorithm.


Main Outcomes and Measures
The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.


Results
The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.


Conclusions and Relevance
In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment."||||Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs.|2016|3236386;49506408;2229081;2924473;97874426;50484974;1811430;10777941;10688956;50162175;3423678;2035210;2057376568;5791678;47191829|Varun Gulshan;L. Peng;Marc Coram;Martin C. Stumpe;Derek J. Wu;Arunachalam Narayanaswamy;Subhashini Venugopalan;Kasumi Widner;T. Madams;Jorge A Cuadros;R. Kim;R. Raman;Philip Nelson;J. Mega;D. Webster|Computer Science;Medicine|8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;b10e4deadf978d8fd6eec97ff18888629f4261ab;9f387ce140c59a44eaeeea590087351461345164;5966d7c7f60898d610812e24c64d4d57855ad86a;884895a86fe15cb9601df4a15a1475c07f28da3c;a9763afda62e960c35c80681f805ddecbef14a92;10f919b1a5161b560504c225cfb2d1b3a4768f80;338a891907dce447da9a0fa2f27221bd35164163;45c9f19b1eb46095e61f3c1a9970a6161c13a861;2521c3d76bc439c961b7003080f4a7a661949547;3def68bd0f856886d34272840a7f81588f2bc082;f3203d0bdefc9670ed508ca776d08aa9f024bafa;d133cb102ad0f81e3fd17a7db090b28afc124c4a;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;305d689afb6574ffec7b01e24431d541d0ce6f5d;a206216c3f67605ac6e25b0178c3f156dc0f7ba0|1686834;1740213;32516743|True;True;True|desc;desc;desc
a1874aafa8730bdd4b28f29d025141c13ee28b58|10.1609/aimag.v17i3.1230||||37-54|■ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_1996|Lisbon|From Data Mining to Knowledge Discovery in Databases|1996|1695784;1398381803;50860274|U. Fayyad;G. Piatetsky-Shapiro;Padhraic Smyth|Computer Science;Mathematics|5c5e69387020d7ca7d49487ca841958dc5e08ce6;49bdeb07b045dd77f0bfe2b44436608770235a23;4e6238c8613b5b81f81552939bce33296aedfbfe;e0535dedb8607d83cd2614317c99913378e89e26;04fd278c01df1564e741b4c6e052fc1c5924ab8d;36652428740cd30d245d55889f01a7fb04a91c93;b10e4deadf978d8fd6eec97ff18888629f4261ab;033f25ad905ef2ed32a8331cf38b83953ff15922;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;8db9df2eadea654f128c1887722c677c708e8a47;7bb6bdf4ed609e5e72d4206d1b308323e73dceec|10688956;2827616;144335550|True;True;False|desc;desc;desc
f762cc39a824de1360e8223222739aaa4cd4168c|10.1145/1273496.1273519||||177-184|Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients, with complexity O(n3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n3). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.|Workshop on AI for Earth|Workshop_on_AI_for_Earth_Toronto_2007|Toronto|Full regularization path for sparse principal component analysis|2007|1387902104;2328284263;1701847|A. d'Aspremont;Francis R. Bach;L. Ghaoui|Computer Science;Mathematics|1592fe924114866c1ac559bae33ea789930daa98;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;c2b381b24aabf237394059fed7920cd6fd0e67b8;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;ec6200bdcc23b79a71555962cde50306c4029f1a;a3461eaf51016f9d6e85ea47173b27e019e801c4;01b24de15cf337c55b9866c4b534596ca3d93abe;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;ff7a293e95c0d44582b7625ee2233916f15cb361;9008cdacbdcff8a218a6928e94fe7c6dfc237b24|1733797;1398980406;145269712|True;True;True|desc;desc;desc
8de174ab5419b9d3127695405efd079808e956e8|10.1145/1553374.1553380|BMJ|16.0|8901-2345_16|41-48|"Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions)."||||Curriculum learning|2009|1751762;2373952;2939803;145183709|Yoshua Bengio;J. Louradour;R. Collobert;J. Weston|Computer Science|0e779fd59353a7f1f5b559b9d65fa4bfe367890c|2334455;145139947;1792298|True;True;False|desc;desc;desc
03cb4e2cb669d3f6344a733e622f07909f87ff0a|10.7551/mitpress/3927.001.0001||||I-VIII, 1-208|"From the Publisher: 
 
""This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book. 
-- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. 
Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. 
The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting ""general purpose"" nature of genetic algorithms as search methods that can be employed across disciplines. 
An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. 
The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Los_Angeles_1996|Los Angeles|An introduction to genetic algorithms|1996|144380037|Melanie Mitchell|Computer Science|2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;3cee40494377c0e7d9c7c23a3811b481e55bce39;91c380406f5a862b5937e70e720802e5c787968d;1dd6c46d868accd5acffd02e4b08b003534b924e;7380e343dd4547e21d5118b16daf03d021d98c4e;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;f986968735459e789890f24b6b277b0920a9725d;9691f67f5075bde2fd70da0135a4a70f25ef042b;1592fe924114866c1ac559bae33ea789930daa98;a7a407968c13ced804a063259d72315a43b84f29;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c|1713164;145066193;3304621|True;True;True|desc;desc;desc
9e475a514f54665478aac6038c262e5a6bac5e64|10.1109/cvpr42600.2020.01164|Cell|68.0|5678-9012_68|11618-11628|Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.||||nuScenes: A Multimodal Dataset for Autonomous Driving|2019|3078154;88740363;33242383;22254044;1754854;2149106173;2064366548;1959810281;46718993;3258919|Holger Caesar;Varun Bankiti;Alex H. Lang;Sourabh Vora;Venice Erin Liong;Qiang Xu;Anush Krishnan;Yuxin Pan;G. Baldan;Oscar Beijbom|Computer Science;Environmental Science;Mathematics;Engineering|b5887d18420e8ac4f4fa4c83c4952138fd956702;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7|144888672;2067208983;2213770|True;True;True|desc;desc;desc
a7976c2bacfbb194ddbe7fd10c2e50a545cf4081|10.1109/tnnls.2016.2582924||||2222-2232|Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.|ICLR|ICLR_Buenos_Aires_2015|Buenos Aires|LSTM: A Search Space Odyssey|2015|3035541;2100612;2865775;1714059;145341374|Klaus Greff;R. Srivastava;Jan Koutník;Bas R. Steunebrink;J. Schmidhuber|Computer Science;Medicine|92ace17730c2173e642934d64f96d359697b7a93;36d442f59c61ea2912d227c24dee76778c546b0a;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;55f44d39630646f36eac91358f8f27d1bead384c;b5887d18420e8ac4f4fa4c83c4952138fd956702;19e8869f4c29353de0d9b52542c1fe9def4cbc7d|2613438;35096370;1734053|True;True;True|desc;desc;desc
a3461eaf51016f9d6e85ea47173b27e019e801c4|10.1203/00006450-199704001-00009||||17-19|We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …|NAACL|NAACL_Auckland_1997|Auckland|State of the Art|1997|2262448783|Markus Voelter|Computer Science;Philosophy|0b544dfe355a5070b60986319a3f51fb45d1348e;7380e343dd4547e21d5118b16daf03d021d98c4e;55f44d39630646f36eac91358f8f27d1bead384c;819167ace2f0caae7745d2f25a803979be5fbfae;9b0dd87208a03e78105491e3727213b9b8ac0419;a40f97770296c7fca2e5361cbceba3f4aae399e0;53834f0ee8df731cf0e629cd594dce0afaaa3d97;7da323e7103245eeaed32367c46abe3f4913df86;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b|47448503;2146312811;1403428213|False;True;True|desc;desc;desc
739769f4862753fc80057194456d758d2a148ee3|10.1109/TSMCB.2011.2168604||||513-529|Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.|NIPS|NIPS_Beijing_2012|Beijing|Extreme Learning Machine for Regression and Multiclass Classification|2012|145678691;2986982;3210833;2118403946|G. Huang;Hongming Zhou;Xiaojian Ding;Rui Zhang|Computer Science;Medicine;Mathematics|5ed59f49c1bb7de06cfa2a9467d5efb535103277;f86f1748d1b6d22870f4347fd5d65314ba800583;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;cbac8b0d82ea8e9251d5530695841d816cb196b9;4a554da55fd9ff76c99e25d2ce937b225dc1100c;0ba86604228b555475496e200f31878df3aabd6e;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;9670485f526f2254c0f34e64d9ca06f665a0bd17;e24b8a9531573d284647239affc6c855505b0de4;65d53938a12c77e7920b8eb3a49df249c978ba3f;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;467568f1777bc51a15a5100516cd4fe8de62b9ab|2048712;2009767;10638646|False;True;True|desc;desc;desc
e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772|10.1147/rd.33.0210|Nature|58.0|1234-5678_58|206-227|Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the||||Some Studies in Machine Learning Using the Game of Checkers|1967|7991309|A. Samuel|Computer Science|9f387ce140c59a44eaeeea590087351461345164;81a4fd3004df0eb05d6c1cef96ad33d5407820df;22fe619996b59c09cb73be40103a123d2e328111;c62043a7d2537bbf40a84b9913957452a47fdb83;1c00df1cb85fa7886b6666599eab59f2b301dd5d;5e095981ebf4d389e9356bd56e59e0ade1b42e88;fbc913faf39b1e369dfcdcfefb354d846a46573c;0023582fde36430c7e3ae81611a14e558c8f4bae|1755694;145036533;1683268|True;False;True|desc;desc;desc
1e41ed1ac234cba0138329047e16a8a424389e77|10.1108/03684920410533985|Cell|1.0|5678-9012_1|933-947|Modified reconstructability analysis (MRA), a novel decomposition technique within the framework of set‐theoretic (crisp possibilistic) reconstructability analysis, is applied to three‐variable NPN‐classified Boolean functions. MRA is superior to conventional reconstructability analysis, i.e. it decomposes more NPN functions. MRA is compared to Ashenhurst‐Curtis (AC) decomposition using two different complexity measures: log‐functionality, a measure suitable for machine learning, and the count of the total number of two‐input gates, a measure suitable for circuit design. MRA is superior to AC using the first of these measures, and is comparable to, but different from AC, using the second.||||A comparison of modified reconstructability analysis and Ashenhurst‐Curtis decomposition of Boolean functions|2004|1399279909;2285935329;2285953900|Anas N. Al-Rabadi;Marek Perkowski;Martin Zwick|Computer Science;Mathematics|a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;01b24de15cf337c55b9866c4b534596ca3d93abe;da048cdf883f2ac0551162cb1abd7e6d09e8e86a|2255301807;1767318;1388702112|True;False;False|desc;desc;desc
6f24d7a6e1c88828e18d16c6db20f5329f6a6827|10.1080/01621459.2017.1285773||||859 - 877|ABSTRACT One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.|CVPR|CVPR_Cairo_2016|Cairo|Variational Inference: A Review for Statisticians|2016|1796335;3081817;40411909|D. Blei;A. Kucukelbir;Jon D. McAuliffe|Computer Science;Mathematics|9eb715fe0347445a2d63518cbb476d345ba86233;10f919b1a5161b560504c225cfb2d1b3a4768f80;877374c2913b787ee9f958f39e31c75d39ebcc15;22fe619996b59c09cb73be40103a123d2e328111;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;f9d119346b0773ea83251598fa5305bc75bac8ab;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;bd1f14e7531220c39fad8f86985cce7b283f035d;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;3df952d4a724655f7520ff95d4b2cef90fff0cae;08b43d84e6747e370ef307e2ada50675b414514a;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c|1683459;145709776;51884035|True;True;True|desc;desc;desc
d98d0d1900b13b87aa4ffd6b69c046beb63f0434|10.1561/2200000001|Cell|23.0|5678-9012_23|1-305|The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.||||Graphical Models, Exponential Families, and Variational Inference|2008|1721860;1694621|M. Wainwright;Michael I. Jordan|Computer Science;Mathematics|07abd02f02774d178f26ca99937e5f94001a9ec9;1626c940a64ad96a7ed53d7d6c0df63c6696956b;872bae24c109f7c30e052ac218b17a8b028d08a0;bb144c04b9eb44579b19d21c3d5954401408440b;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;b16408a97170785fb216c9e8b7920d64f478fbc8;c6bbfb4fcaecc779c899af4bb52083870f4b996a;1a827052f01ef830cbc849c71e9da99791243a5f;4f975da00a5b2a2f7236e34edcb7274e5fdab937;815c84ab906e43f3e6322f2ca3fd5e1360c64285;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;22fe619996b59c09cb73be40103a123d2e328111;d63b884d5ebc739f6e1bdf861fa9276260781404;7ad66cba3b7e3abae7ef33122588512a146f7f77;668b1277fbece28c4841eeab1c97e4ebd0079700;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;8d1c588d202f150e1797ed113fba7e67bfa43ecb;b10e4deadf978d8fd6eec97ff18888629f4261ab|1398620187;2346939080;33780923|True;False;True|desc;desc;desc
5ded2b8c64491b4a67f6d39ce473d4b9347a672e|10.18653/v1/N18-1101||||1112-1122|This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.|NeurIPS|NeurIPS_Paris_2017|Paris|A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference|2017|81840293;10666396;3644767|Adina Williams;Nikita Nangia;Samuel R. Bowman|Computer Science;Linguistics|9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;4f975da00a5b2a2f7236e34edcb7274e5fdab937;f354310098e09c1e1dc88758fca36767fd9d084d;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;b8ebda42e272d3617375118542d4675a0c0e501d|1878461;1755208;122360608|True;True;True|desc;desc;desc
46f74231b9afeb0c290d6d550043c55045284e5f|10.1109/MSP.2012.2211477|Science|65.0|3456-7890_65|141-142|In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.||||The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]|2012|144718788|L. Deng|Computer Science;Engineering|4e6238c8613b5b81f81552939bce33296aedfbfe|143898643;40062372;2135837|False;False;True|desc;desc;desc
f04df4e20a18358ea2f689b4c129781628ef7fc1|10.18653/v1/D15-1075|BMJ|74.0|8901-2345_74|632-642|Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.||||A large annotated corpus for learning natural language inference|2015|3644767;32301760;144922861;144783904|Samuel R. Bowman;Gabor Angeli;Christopher Potts;Christopher D. Manning|Computer Science;Linguistics|8de174ab5419b9d3127695405efd079808e956e8;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;35b3233e521f1e9a34837c30be1957858f8f35fe;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;01f702f8b1f9d1314587015f1f038af4d5735e77;ff7a293e95c0d44582b7625ee2233916f15cb361;a7a407968c13ced804a063259d72315a43b84f29;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;01b24de15cf337c55b9866c4b534596ca3d93abe;3a84214cb69ea0b34352285029f368b75718c32b;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;799f927692a6c08c5e630bea78c087c5051528fc;222d3a63d4f81d39ea324530b57328c58f298888;d517b13f2b152c913b81ce534a149493517dbdad;f354310098e09c1e1dc88758fca36767fd9d084d;f94455176857303605ad423599385a2341c568eb|46599630;143649421;2009348|True;False;True|desc;desc;desc
d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac|10.1109/ASSPCC.2000.882463||||153-158|This paper points out the flaws in using the extended Kalman filter (EKE) and introduces an improvement, the unscented Kalman filter (UKF), proposed by Julier and Uhlman (1997). A central and vital operation performed in the Kalman filter is the propagation of a Gaussian random variable (GRV) through the system dynamics. In the EKF the state distribution is approximated by a GRV, which is then propagated analytically through the first-order linearization of the nonlinear system. This can introduce large errors in the true posterior mean and covariance of the transformed GRV, which may lead to sub-optimal performance and sometimes divergence of the filter. The UKF addresses this problem by using a deterministic sampling approach. The state distribution is again approximated by a GRV, but is now represented using a minimal set of carefully chosen sample points. These sample points completely capture the true mean and covariance of the GRV, and when propagated through the true nonlinear system, captures the posterior mean and covariance accurately to the 3rd order (Taylor series expansion) for any nonlinearity. The EKF in contrast, only achieves first-order accuracy. Remarkably, the computational complexity of the UKF is the same order as that of the EKF. Julier and Uhlman demonstrated the substantial performance gains of the UKF in the context of state-estimation for nonlinear control. Machine learning problems were not considered. We extend the use of the UKF to a broader class of nonlinear estimation problems, including nonlinear system identification, training of neural networks, and dual estimation problems. In this paper, the algorithms are further developed and illustrated with a number of additional examples.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cairo_2000|Cairo|The unscented Kalman filter for nonlinear estimation|2000|48385057;1908796|E. Wan;Rudolph van der Merwe|Computer Science;Mathematics;Engineering|93884d89dfc8c3886f642018227a43fb7b58044f;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;01b24de15cf337c55b9866c4b534596ca3d93abe|103010565;145124475;2694442|False;True;False|desc;desc;desc
49bdeb07b045dd77f0bfe2b44436608770235a23|10.1109/MSP.2020.2975749||||50-60|Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Sydney_2019|Sydney|Federated Learning: Challenges, Methods, and Future Directions|2019|145530218;2894821;145532827;145260024|Tian Li;Anit Kumar Sahu;Ameet Talwalkar;Virginia Smith|Computer Science;Mathematics|08b43d84e6747e370ef307e2ada50675b414514a;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;22fe619996b59c09cb73be40103a123d2e328111;771479c18b586eafae21baf262a220aaa7b2eef6;f3203d0bdefc9670ed508ca776d08aa9f024bafa;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;23e44c7c6929bbb1ee5bc111e81e242f4835b712;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;1696cbf7da0ee845c50591843993e6605adec177;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;9257779eed46107bcdce9f4dc86298572ff466ce;d7701e78e0bfc92b03a89582e80cfb751ac03f26;d997919c30fa6711bc5c25cf8c8aea34fac27b91;4b61c25a86083c20730c9b12737ac6ac4178c364|143668698;145193332;3294736|True;False;False|desc;desc;desc
98c25683fc8d6446448b734b1bcf08e1457f8d85|10.1093/bioinformatics/btm344||||"
          2507-17
        "|Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.|COLT|COLT_Rome_2007|Rome|A review of feature selection techniques in bioinformatics|2007|2247520360;1788277;2285903189|Yvan Saeys;Iñaki Inza;Pedro Larrañaga|Computer Science;Medicine;Biology|1696cbf7da0ee845c50591843993e6605adec177;aaf9069be5a498179cbd2932d793ea1b9d0092de;16c0ef924da1f6b510c9c783ac764156f5a3d631;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;1a827052f01ef830cbc849c71e9da99791243a5f;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;06645d735b59b14479ae1d0392136bbf44227d0f;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;a7a407968c13ced804a063259d72315a43b84f29;08b43d84e6747e370ef307e2ada50675b414514a|144505734;2250918;143931014|True;True;True|desc;desc;desc
18d026ec5d0eebd17ee2c762da89540c0b3d7bde|10.1109/JPROC.2020.3004555||||43-76|Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.|ECCV|ECCV_Tokyo_2019|Tokyo|A Comprehensive Survey on Transfer Learning|2019|1799525;51052604;108237152;120791559;1864765978;1968806;144467554;144131273|Fuzhen Zhuang;Zhiyuan Qi;Keyu Duan;Dongbo Xi;Yongchun Zhu;Hengshu Zhu;Hui Xiong;Qing He|Computer Science;Mathematics|d0ab11de3077490c80a08abd0fb8827bac84c454;5d150cec2775f9bc863760448f14104cc8f42368|2056183624;2314654;38524906|True;True;True|desc;desc;desc
2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0|10.1109/IJCNN.2004.1380068||||985-990 vol.2|It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradient-based learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on real-world benchmarking function approximation and classification problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.|COLT|COLT_Auckland_2004|Auckland|Extreme learning machine: a new learning scheme of feedforward neural networks|2004|145678691;50736254;1683268|G. Huang;Q. Zhu;C. Siew|Computer Science|2521c3d76bc439c961b7003080f4a7a661949547;0165568bcc1a819c18564567f2ec15d859be2519;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;c62043a7d2537bbf40a84b9913957452a47fdb83;1051280d2b825c04f27d231aba0f8284bb297880;0023582fde36430c7e3ae81611a14e558c8f4bae;91c380406f5a862b5937e70e720802e5c787968d;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;ac12c9b9e35e58b55d85a97c47886a7371c14afa;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;467568f1777bc51a15a5100516cd4fe8de62b9ab|48385057;2428490;144766615|True;False;False|desc;desc;desc
1dae4d61cd74cc919ecc638bde6b7125728ea97b|10.1109/TNN.2010.2091281|The Lancet|13.0|6789-0123_13|199-210|Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.||||Domain Adaptation via Transfer Component Analysis|2009|1746914;1807998;145193332;152290618|Sinno Jialin Pan;I. Tsang;J. Kwok;Qiang Yang|Computer Science;Medicine;Mathematics|339a8e4cb0eba77675711ac255ac2a5d7ede1d53;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;8db9df2eadea654f128c1887722c677c708e8a47;48e752c719d33ff55b3b3bec3538727f8ce69399;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d|49180504;1837057;1979489|True;True;True|desc;desc;desc
6981ea66000e2c98f8a81f4bef05802234d986a4|10.1145/219717.219791|NEJM|19.0|9012-3456_19|88-95|Rough set theory, introduced by Zdzislaw Pawlak in the early 1980s [11, 12], is a new mathematical tool to deal with vagueness and uncertainty. This approach seems to be of fundamental importance to artificial intelligence (AI) and cognitive sciences, especially in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, decision support systems, inductive reasoning, and pattern recognition.||||Rough sets|1995|69054960;1398620187;1754252;1783402|Z. Pawlak;J. Grzymala-Busse;R. Słowiński;W. Ziarko|Computer Science;Mathematics|a1874aafa8730bdd4b28f29d025141c13ee28b58;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;b57c54350769ffa59ff57f79ee5aad918844d298;b5887d18420e8ac4f4fa4c83c4952138fd956702;a85e512d8845bd007b0866b4a97e8341463f8190;5d150cec2775f9bc863760448f14104cc8f42368;1592fe924114866c1ac559bae33ea789930daa98;2fb23de9524b13a32d9ed7f2441c46c81558a3c8|145325584;2068286576;5580228|True;True;False|desc;desc;desc
f986968735459e789890f24b6b277b0920a9725d|10.1109/TPAMI.2017.2723009|JAMA|28.0|7890-1234_28|1452-1464|The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.||||Places: A 10 Million Image Database for Scene Recognition|2018|145291669;2677488;2556428;143868587;143805211|Bolei Zhou;Àgata Lapedriza;A. Khosla;A. Oliva;A. Torralba|Computer Science;Medicine;Environmental Science;Engineering|8515a302b8f389f8f1008cc2650e5ec0a6913e24;a27089efabc5f4abd5ddf2be2a409bff41f31199;427b168f490b56716f22b129ac93aba5425ea08f;9670485f526f2254c0f34e64d9ca06f665a0bd17|2127057;144916619;1806678|False;True;False|desc;desc;desc
0090023afc66cd2741568599057f4e82b566137c|10.1145/3457607||||1 - 35|With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.|ICLR|ICLR_Sydney_2019|Sydney|A Survey on Bias and Fairness in Machine Learning|2019|51997673;2775559;51884035;1782658;143728483|Ninareh Mehrabi;Fred Morstatter;N. Saxena;Kristina Lerman;A. Galstyan|Computer Science;Mathematics|5c5e69387020d7ca7d49487ca841958dc5e08ce6;bb144c04b9eb44579b19d21c3d5954401408440b;877374c2913b787ee9f958f39e31c75d39ebcc15;a86171e13f84fe32212dd7fb6a1c31a34a47155f;48e752c719d33ff55b3b3bec3538727f8ce69399;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da|2340666;51289782;2067208983|False;False;False|desc;desc;desc
398c296d0cc7f9d180f84969f8937e6d3a413796|10.1109/CVPR.2012.6248110|JAMA|16.0|7890-1234_16|3642-3649|Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.||||Multi-column deep neural networks for image classification|2012|1895356;2514691;145341374|D. Ciresan;U. Meier;J. Schmidhuber|Computer Science|abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;0ca26f9a98dda0abb737692f72ffa682df14cb2f;a675fe5a7d99ac6f7ff91fa084462faefe616148;5c7e5248d9eb7f373f10277410bf8506160907ea;8515a302b8f389f8f1008cc2650e5ec0a6913e24|2696727;1772593;35668343|True;True;True|desc;desc;desc
a34e35dbbc6911fa7b94894dffdc0076a261b6f0|10.1162/neco.1992.4.1.1|JAMA|53.0|7890-1234_53|1-58|Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.||||Neural Networks and the Bias/Variance Dilemma|1992|3194361;2246319;2330895|S. Geman;E. Bienenstock;R. Doursat|Computer Science;Mathematics|a7a407968c13ced804a063259d72315a43b84f29;9b539d413393047b28bb7be9b195f142aaf7a80e;53834f0ee8df731cf0e629cd594dce0afaaa3d97;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;b3de1062d8a462dfdc2938558258f8884abe9f4e;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;16c0ef924da1f6b510c9c783ac764156f5a3d631;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;ea58af907495e97c93997119db4a59fab5cd3683;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d|47156522;1471206276;2346777264|True;True;True|desc;desc;desc
f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d|10.1109/SP.2017.41||||3-18|"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rio_de_Janeiro_2016|Rio de Janeiro|Membership Inference Attacks Against Machine Learning Models|2016|2520493;34828439;3469125;1723945|R. Shokri;M. Stronati;Congzheng Song;Vitaly Shmatikov|Computer Science;Mathematics|f9d119346b0773ea83251598fa5305bc75bac8ab;05fd1da7b2e34f86ec7f010bef068717ae964332;d079a2f877f554e00f71a6975435d8325987bdf5;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;9670485f526f2254c0f34e64d9ca06f665a0bd17|31592365;47156522;120791559|True;True;True|desc;desc;desc
819167ace2f0caae7745d2f25a803979be5fbfae|10.1109/EUROSP.2016.36||||372-387|Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_New_York_2015|New York|The Limitations of Deep Learning in Adversarial Settings|2015|1967156;144061974;1680133;2623167;144643812;144231976|Nicolas Papernot;P. Mcdaniel;S. Jha;Matt Fredrikson;Z. B. Celik;A. Swami|Computer Science;Mathematics|2bc3644ce4de7fce5812c1455e056649a47c1bbf;33e46a618fdb22d46951f548d6ceeb384e7f1687|1755694;1746807;145197293|True;False;True|desc;desc;desc
43d2ed5c3c55c1100450cd74dc1031afa24d37b2|10.1201/b17320-16||||399-416|Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.|IROS|IROS_Madrid_2008|Madrid|Collective Classification in Network Data|2008|40655309;1686834;2696727;1746034;153701431;1397398770|Prithviraj Sen;Galileo Namata;M. Bilgic;L. Getoor;Brian Gallagher;Tina Eliassi-Rad|Computer Science|602f31242e577d2d05f918a3080fd50095e7faed;bb144c04b9eb44579b19d21c3d5954401408440b;605402e235bd62437baf3c9ebefe77fb4d92ee95;884895a86fe15cb9601df4a15a1475c07f28da3c;1c00df1cb85fa7886b6666599eab59f2b301dd5d;668b1277fbece28c4841eeab1c97e4ebd0079700;5c7e5248d9eb7f373f10277410bf8506160907ea;e838ba98e198d2dac047736e77c50c0efa49c2dc;61e27dbae190b82639c57f180ecf97e4c46fcad9;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;38f23fe236b152cd4983c8f30d305a568afd0d3e;a9763afda62e960c35c80681f805ddecbef14a92;5a391667242b4a631acdd5917681b16a86523987;e0535dedb8607d83cd2614317c99913378e89e26|144766615;1807250;47454309|False;True;True|desc;desc;desc
86cff4d050beb90fed2e1ceac8940c8221b120aa|10.1109/mex.1987.4307079|The Lancet|7.0|6789-0123_7|110-111|Computers are being used more and more in all aspects of our lives and, programmed correctly, they are more accurate and precise than humans can ever be. Here, however, the myth of the superiority of artificial intelligence is examined and dispelled. The authors, one a philosopher and the other a computer scientist, argue that even highly advanced systems only correspond to the very early stages of human learning and that there are many human skills that computers will never be able to emulate. The mind will always be superior to the machine. To illustrate their point, they set forth a model documenting five distinct levels - novice, advanced beginner, competent, proficient and expert - through which human beings pass in acquiring and mastering a skill. The two final stages require a degree of intuitive intelligence far beyond the most ambitious projects being planned for the future. The authors acknowledge the huge progress made by computers and the massive advantages to be gained from using them, but they stress that their value can only lie in their use as aids, never as substitutes for the human mind.||||Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer|1987|3333658;2547681;2975527|H. Dreyfus;S. Dreyfus;L. Zadeh|Computer Science;Philosophy|1a827052f01ef830cbc849c71e9da99791243a5f;3fa5f45ddbd5184f10bfb92e367493c5a344f207;a675fe5a7d99ac6f7ff91fa084462faefe616148;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;bcce96a2a074448953fc61a29a84afbdfc8db55a;01f29addca4dc6f189f903cb133dea7585813a6f;908cca0abefc35acc38033603714fbb1bcadc49d;d02927d4de4a2a51cced4970da04b812cbee4342;9f387ce140c59a44eaeeea590087351461345164;2077d0f30507d51a0d3bbec4957d55e817d66a59;78947497cbbffc691aac3f590d972130259af9ce;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57|3078154;144049725;32301760|True;True;True|desc;desc;desc
48234756b7cf798bfeb47328f7c5d597fd4838c2|10.1109/IJCNN.2008.4633969||||1322-1328|This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.|NAACL|NAACL_Lisbon_2008|Lisbon|ADASYN: Adaptive synthetic sampling approach for imbalanced learning|2008|2198278;2115297828;2111388466;2116066317|Haibo He;Yang Bai;E. A. Garcia;Shutao Li|Computer Science|1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;e9126a98de0c39dcffe4c4f5158e037460196724;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;6df11b0bb0244d4d36e8955436067cc5d19734fa;36652428740cd30d245d55889f01a7fb04a91c93;771479c18b586eafae21baf262a220aaa7b2eef6;b10e4deadf978d8fd6eec97ff18888629f4261ab;37a67228271527037c9250ae3fd220199275e42e;58a8bead87c8c1e37460dce28285c053c270f6e7;831edc3d67457db83da40d260e93bfd7559347ae|2135990561;1713164;50056360|True;True;False|desc;desc;desc
1fcbefeb0beae4470cf40df74cd116b1d4bdcae4|10.1109/72.914517|Frontiers for Young Minds|27.0|2345-6789_27|"
          181-201
        "|This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.||||An introduction to kernel-based learning algorithms|2001|145034054;2459012;152597562;34628173;1707625|K. Müller;S. Mika;Gunnar Rätsch;K. Tsuda;B. Scholkopf|Computer Science;Medicine;Mathematics|9f387ce140c59a44eaeeea590087351461345164;81a4fd3004df0eb05d6c1cef96ad33d5407820df;01f29addca4dc6f189f903cb133dea7585813a6f;6ec7c724aa1d906e9e9f81c58497adddb22175b8;eed9fa4483cab37eacd59db0fac4b1441431ee85;1dae4d61cd74cc919ecc638bde6b7125728ea97b;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;0165568bcc1a819c18564567f2ec15d859be2519|2233724;1491359904;2107994160|True;True;True|desc;desc;desc
5d150cec2775f9bc863760448f14104cc8f42368|10.1073/pnas.1517384113||||3932 - 3937|Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.|NAACL|NAACL_Mexico_City_2015|Mexico City|Discovering governing equations from data by sparse identification of nonlinear dynamical systems|2015|3083169;2424683;144484982|S. Brunton;J. Proctor;J. Kutz|Environmental Science;Medicine;Computer Science;Physics;Mathematics;Engineering|cedea36fa3692281b3ac767335fe49a16d00957d;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;bf7dcbee272428a2aa3c534200743ff7ab2047f8;df2a7756382540e92895f10703cec32d50c4f316;0ba86604228b555475496e200f31878df3aabd6e;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;b954efe5e46b8952f5a8daf42e7e535119b5408b;d997919c30fa6711bc5c25cf8c8aea34fac27b91;2c47bd8bd699914e3535292b17ba46542800845c;f986968735459e789890f24b6b277b0920a9725d;1a827052f01ef830cbc849c71e9da99791243a5f;cd49acefc8d51e324aa562e5337e1c2aff067053;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;d422df8bff4e677a3077635db116679d25142bfc;2a3842f6070b4554ff21fe62b2a486657d9a304a|92480907;144902513;34789794|True;False;True|desc;desc;desc
e0535dedb8607d83cd2614317c99913378e89e26|10.1162/089976602760407955|Nature|86.0|1234-5678_86|2531-2560|A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.||||Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations|2002|145247053;1792142;1754307|W. Maass;T. Natschläger;H. Markram|Computer Science;Medicine|c2b381b24aabf237394059fed7920cd6fd0e67b8;c43025c429b1fbf6f1379f61801a1b40834d62e7;4f975da00a5b2a2f7236e34edcb7274e5fdab937;df40ce107a71b770c9d0354b78fdd8989da80d2f|2149814967;1718786;31847168|True;True;True|desc;desc;desc
d516daff247f7157fccde6649ace91d969cd1973|10.1145/3233231||||36 - 43|In machine learning, the concept of interpretability is both important and slippery.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Rome_2016|Rome|The mythos of model interpretability|2016|32219137|Zachary Chase Lipton|Computer Science;Philosophy;Mathematics|b8012351bc5ebce4a4b3039bbbba3ce393bc3315;86f0b58404a264a6216e29c78a5c113d900ca461;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;d02927d4de4a2a51cced4970da04b812cbee4342;b3852f0113fcf8a3913c55ae92393ae6ccde347e;602f31242e577d2d05f918a3080fd50095e7faed;61394599ed0aabe04b724c7ca3a778825c7e776f;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a|2256269;1751762;1741702|True;True;False|desc;desc;desc
6aae0dc122102693e8136856ffc8b72df7f78386|10.1145/1007730.1007735|NEJM|39.0|9012-3456_39|20-29|There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.||||A study of the behavior of several methods for balancing machine learning training data|2004|145666101;1793286;1737677|Gustavo E. A. P. A. Batista;R. Prati;M. C. Monard|Computer Science|f3203d0bdefc9670ed508ca776d08aa9f024bafa;395de0bd3837fdf4b4b5e5f04835bcc69c279481;c6a83c4fcc99ba6753109301949c5b7cfa978079;0090023afc66cd2741568599057f4e82b566137c;65b16da51891a6b98140d425804c8a0fd0299219|48419707;144108246;48180545|True;False;False|desc;desc;desc
611544418ca53cdad254df444addc7814abcfddc|10.1080/24754269.2021.1980261|Cell|80.0|5678-9012_80|87 - 87|The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an||||An introduction to statistical learning with applications in R|2021|151004165;2134162313;1840524|Fariha Sohil;Muhammad Umair Sohali;J. Shabbir|Computer Science;Mathematics|8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;48234756b7cf798bfeb47328f7c5d597fd4838c2;8db9df2eadea654f128c1887722c677c708e8a47;872bae24c109f7c30e052ac218b17a8b028d08a0;8e51d68250db5637cd6bc1de98a99396441399b2;d63b884d5ebc739f6e1bdf861fa9276260781404;8c8215b7f8111839f0066010a530a3a9f57ba15e|2146245769;70547150;1695689|True;True;True|desc;desc;desc
44c7d9fe583e3d317a619297e7e949070710799f|10.1093/nar/gkac1052||||D523 - D531|Abstract The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this publication we describe enhancements made to our data processing pipeline and to our website to adapt to an ever-increasing information content. The number of sequences in UniProtKB has risen to over 227 million and we are working towards including a reference proteome for each taxonomic group. We continue to extract detailed annotations from the literature to update or create reviewed entries, while unreviewed entries are supplemented with annotations provided by automated systems using a variety of machine-learning techniques. In addition, the scientific community continues their contributions of publications and annotations to UniProt entries of their interest. Finally, we describe our new website (https://www.uniprot.org/), designed to enhance our users’ experience and make our data easily accessible to the research community. This interface includes access to AlphaFold structures for more than 85% of all entries as well as improved visualisations for subcellular localisation of proteins.|NeurIPS|NeurIPS_New_York_2022|New York|UniProt: the Universal Protein Knowledgebase in 2023|2022|46888415;46502933;143945011;3093838;49967517;2009348;1578100923;35510297;1413794065;2038165074;2191753901;2407842;15309498;2108094514;11852405;2191751245;1413140200;100546775;5059205;2038176754;6289332;2055614715;47457030;2191763301;5219418;34594667;121557847;2116783356;6593936;1694551096;145684421;2038173616;2112890973;15186612;144057990;2595483;39589857;48506828;153529797;2053629650;1766624;2115667193;1914833;144916619;1907575;2061819040;50693907;1865802;153825989;1979856;5240016;1694538098;145718058;2380233;1403651727;38473019;2910143;15660940;3004899;2191754156;48999874;3349310;2649909;2314654;2090895670;1411865594;2044918426;2344478;2509510;40660788;35099951;3375775;1765161;1784123;1873696;3168502;2092438037;2057009;5385807;3304621;1399776494;87711179;2594256;8926744;3177854;143827678;2258061;2375564;11541096;3275738;2649548;2070956511;3322351;37120465;2562543;1876480;40291345;1897062;1491922105;40660854;1744726;1734053;3177811;1763713;3090725;2446311;50401356;3250223;1776336;6199470;153787929;123451953;2108034421;2151810209|A. Bateman;M. Martin;S. Orchard;M. Magrane;Shadab Ahmad;E. Alpi;E. Bowler-Barnett;R. Britto;Hema Bye-A-Jee;Austra Cukura;Paul Denny;Tunca Dogan;Thankgod Ebenezer;Jun Fan;Penelope Garmiri;Leonardo Jose da Costa Gonzales;E. Hatton-Ellis;Abdulrahman Hussein;A. Ignatchenko;Giuseppe Insana;Rizwan Ishtiaq;Vishal Joshi;D. Jyothi;Swaathi Kandasaamy;A. Lock;Aurélien Luciani;Marija Lugarić;Jie Luo;Yvonne Lussi;Alistair MacDougall;F. Madeira;Mahdi Mahmoudy;Alok Mishra;Katie Moulang;Andrew Nightingale;Sangya Pundir;G. Qi;Shriya Raj;P. Raposo;Daniel L Rice;Rabie Saidi;Rafael Santos;Elena Speretta;J. Stephenson;Prabhat Totoo;Edward Turner;N. Tyagi;Preethi Vasudev;Kate Warner;Xavier Watkins;Rossana Zaru;H. Zellner;A. Bridge;L. Aimo;Ghislaine Argoud-Puy;A. Auchincloss;K. Axelsen;Parit Bansal;Delphine Baratin;Teresa M Batista Neto;M. Blatter;Jerven T. Bolleman;E. Boutet;L. Breuza;B. Gil;Cristina Casals-Casas;Kamal Chikh Echioukh;E. Coudert;Béatrice A. Cuche;Edouard de Castro;A. Estreicher;M. Famiglietti;M. Feuermann;E. Gasteiger;P. Gaudet;S. Gehant;V. Gerritsen;A. Gos;N. Gruaz;C. Hulo;Nevila Hyka-Nouspikel;F. Jungo;A. Kerhornou;Philippe le Mercier;D. Lieberherr;P. Masson;A. Morgat;Venkatesh Muthukrishnan;S. Paesano;I. Pedruzzi;S. Pilbout;L. Pourcel;S. Poux;Monica Pozzato;Manuela Pruess;Nicole Redaschi;C. Rivoire;Christian J. A. Sigrist;K. Sonesson;S. Sundaram;Cathy H. Wu;C. Arighi;L. Arminski;Chuming Chen;Yongxing Chen;Hongzhan Huang;K. Laiho;P. McGarvey;D. Natale;K. Ross;C. R. Vinayaka;Qinghua Wang;Yuqi Wang;Jian Zhang|Computer Science;Medicine;Biology|398c296d0cc7f9d180f84969f8937e6d3a413796;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;fee4db01f6f981931dfd87376a8f861353d1e494;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;18bc1d4271abe8dd6e16179cdb06524a4f396e16;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;53834f0ee8df731cf0e629cd594dce0afaaa3d97;184ac0766262312ba76bbdece4e7ffad0aa8180b;a88b3be9b2db0319f8880e60a131b3060dba1eb7;01f29addca4dc6f189f903cb133dea7585813a6f;ea58af907495e97c93997119db4a59fab5cd3683;1904d633fca15140e35d893637232803b6dde6d9;a486e2839291111bb44fa1f07731ada123539f75;cd49acefc8d51e324aa562e5337e1c2aff067053;402f850dff86fb601d34b2841e6083ac0f928edd;2ea6a93199c9227fa0c1c7de13725f918c9be3a4|144335550;47909642;2149106173|True;True;True|desc;desc;desc
0e779fd59353a7f1f5b559b9d65fa4bfe367890c|10.1109/MSP.2017.2693418|PNAS|64.0|4567-8901_64|18-42|Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.||||Geometric Deep Learning: Going beyond Euclidean data|2016|1732570;143627859;1688882;3149531;1697397|M. Bronstein;Joan Bruna;Yann LeCun;Arthur Szlam;P. Vandergheynst|Computer Science;Mathematics|1dae4d61cd74cc919ecc638bde6b7125728ea97b;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;fee4db01f6f981931dfd87376a8f861353d1e494|3294736;2064046776;143761817|True;True;True|desc;desc;desc
2ea6a93199c9227fa0c1c7de13725f918c9be3a4|10.5555/1577069.1755843|Cell|84.0|5678-9012_84|1755-1758|There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.||||Dlib-ml: A Machine Learning Toolkit|2009|2065224236|Davis E. King|Computer Science|63861fbeb7ec41986b85965b9780b428d919919e;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;0ba86604228b555475496e200f31878df3aabd6e;f762cc39a824de1360e8223222739aaa4cd4168c;9b539d413393047b28bb7be9b195f142aaf7a80e;e50f4d3316d13841c287dcdf5479d7820d593571;5794141889d0e994c3103b0aaab08a18222c9c43;34f25a8704614163c4095b3ee2fc969b60de4698|3469125;1979489;49903386|True;True;True|desc;desc;desc
9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746|10.3233/IDA-2002-6504||||429-449|In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Beijing_2002|Beijing|The class imbalance problem: A systematic study|2002|1743642;144980086|N. Japkowicz;Shaju Stephen|Computer Science;Mathematics|d079a2f877f554e00f71a6975435d8325987bdf5;38f23fe236b152cd4983c8f30d305a568afd0d3e;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;6aae0dc122102693e8136856ffc8b72df7f78386;f986968735459e789890f24b6b277b0920a9725d;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;86cff4d050beb90fed2e1ceac8940c8221b120aa;23e44c7c6929bbb1ee5bc111e81e242f4835b712;76f560991d56ad689ec32f9e9d13291e0193f4cf;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;a675fe5a7d99ac6f7ff91fa084462faefe616148;1051280d2b825c04f27d231aba0f8284bb297880|2065370007;40459179;103010565|True;True;True|desc;desc;desc
6adf016e7531c91100d3cf4a74f5d4c87b26b528|10.1109/SP.2016.41|Frontiers for Young Minds|99.0|2345-6789_99|582-597|Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.||||Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks|2015|1967156;144061974;37785191;1680133;144231976|Nicolas Papernot;P. Mcdaniel;Xi Wu;S. Jha;A. Swami|Computer Science;Mathematics|10f919b1a5161b560504c225cfb2d1b3a4768f80;46f74231b9afeb0c290d6d550043c55045284e5f;2521c3d76bc439c961b7003080f4a7a661949547;076af19e50f022ccbe5bf16f413f79b5c6904c05;e0535dedb8607d83cd2614317c99913378e89e26;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;cedea36fa3692281b3ac767335fe49a16d00957d;a9763afda62e960c35c80681f805ddecbef14a92;98c25683fc8d6446448b734b1bcf08e1457f8d85;f9d119346b0773ea83251598fa5305bc75bac8ab;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;23e44c7c6929bbb1ee5bc111e81e242f4835b712;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7|1390903788;40527353;2066145|True;True;True|desc;desc;desc
815c84ab906e43f3e6322f2ca3fd5e1360c64285|10.1126/science.aab3050|Cell|95.0|5678-9012_95|1332 - 1338|Handwritten characters drawn by a model Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.||||Human-level concept learning through probabilistic program induction|2015|2373318;145124475;1763295|B. Lake;R. Salakhutdinov;J. Tenenbaum|Computer Science;Medicine|81a4fd3004df0eb05d6c1cef96ad33d5407820df;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376|40642935;122360608;3151995|True;True;True|desc;desc;desc
a27089efabc5f4abd5ddf2be2a409bff41f31199|10.5220/0006639801080116|The Lancet|3.0|6789-0123_3|108-116|: With exponential growth in the size of computer networks and developed applications, the signiﬁcant in-creasing of the potential damage that can be caused by launching attacks is becoming obvious. Meanwhile, Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are one of the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of adequate dataset, anomaly-based approaches in intrusion detection systems are suffering from accurate deployment, analysis and evaluation. There exist a number of such datasets such as DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their proposed intrusion detection and intrusion prevention approaches. Based on our study over eleven available datasets since 1998, many such datasets are out of date and unreliable to use. Some of these datasets suffer from lack of trafﬁc diversity and volumes, some of them do not cover the variety of attacks, while others anonymized packet information and payload which cannot reﬂect the current trends, or they lack feature set and metadata. This paper produces a reliable dataset that contains benign and seven common attack network ﬂows, which meets real world criteria and is publicly avaliable. Consequently, the paper evaluates the performance of a comprehensive set of network trafﬁc features and machine learning algorithms to indicate the best set of features for detecting the certain attack categories.||||Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization|2018|9787897;2037871;1698268|Iman Sharafaldin;Arash Habibi Lashkari;A. Ghorbani|Computer Science;Engineering|2a3842f6070b4554ff21fe62b2a486657d9a304a;46f74231b9afeb0c290d6d550043c55045284e5f|1716986;2088846;143655174|True;True;True|desc;desc;desc
9008cdacbdcff8a218a6928e94fe7c6dfc237b24|10.1109/CVPR.1997.609310||||130-136|We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Moscow_1997|Moscow|Training support vector machines: an application to face detection|1997|1781874;1771659;1804489|E. Osuna;R. Freund;F. Girosi|Computer Science;Mathematics|72e93aa6767ee683de7f001fa72f1314e40a8f35;5c7e5248d9eb7f373f10277410bf8506160907ea;a40f97770296c7fca2e5361cbceba3f4aae399e0;8c8215b7f8111839f0066010a530a3a9f57ba15e;22fe619996b59c09cb73be40103a123d2e328111;cedea36fa3692281b3ac767335fe49a16d00957d;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;f762cc39a824de1360e8223222739aaa4cd4168c;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;5b66b1c65dcb97d1d0b18014e2e32e8522e66932|49754061;144159726;3000659|True;True;True|desc;desc;desc
2fb23de9524b13a32d9ed7f2441c46c81558a3c8|10.1093/acprof:oso/9780199535255.001.0001||||1-449|Concentration inequalities for functions of independent random variables is an area of probability theory that has witnessed a great revolution in the last few decades, and has applications in a wide variety of areas such as machine learning, statistics, discrete mathematics, and high-dimensional geometry. Roughly speaking, if a function of many independent random variables does not depend too much on any of the variables then it is concentrated in the sense that with high probability, it is close to its expected value. This book offers a host of inequalities to illustrate this rich theory in an accessible way by covering the key developments and applications in the field. The authors describe the interplay between the probabilistic structure (independence) and a variety of tools ranging from functional inequalities to transportation arguments to information theory. Applications to the study of empirical processes, random projections, random matrix theory, and threshold phenomena are also presented. A self-contained introduction to concentration inequalities, it includes a survey of concentration of sums of independent random variables, variance bounds, the entropy method, and the transportation method. Deep connections with isoperimetric problems are revealed whilst special attention is paid to applications to the supremum of empirical processes. Written by leading experts in the field and containing extensive exercise sections this book will be an invaluable resource for researchers and graduate students in mathematics, theoretical computer science, and engineering.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2013|New York|Concentration Inequalities - A Nonasymptotic Theory of Independence|2013|122051864;1755694;3226094|S. Boucheron;G. Lugosi;P. Massart|Computer Science;Mathematics|24e6c5bfe9bb0751e5708b501d04e860011b2953;f94455176857303605ad423599385a2341c568eb;fbf1c51548ffc9b9e538befcd71529365af23d15;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;78989616eeeac55b202e3e4205225e7135054185;aaf9069be5a498179cbd2932d793ea1b9d0092de;7ad66cba3b7e3abae7ef33122588512a146f7f77;d9665992ee36699b8ae4a2e2294552cd4be9003a;7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;220ac48a22547a455d05f416e1fd22bbd0b0788d;5794141889d0e994c3103b0aaab08a18222c9c43|46844846;2266084696;145124475|False;True;True|desc;desc;desc
eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a|10.1198/016214502753479248||||77 - 87|A reliable and precise classification of tumors is essential for successful diagnosis and treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies increasingly used in cancer research. By allowing the monitoring of expression levels in cells for thousands of genes simultaneously, microarray experiments may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. This article compares the performance of different discrimination methods for the classification of tumors based on gene expression data. The methods include nearest-neighbor classifiers, linear discriminant analysis, and classification trees. Recent machine learning approaches, such as bagging and boosting, are also considered. The discrimination methods are applied to datasets from three recently published cancer gene expression studies.|AAMAS|AAMAS_Beijing_2002|Beijing|Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data|2002|2225350;2711125;2673932|S. Dudoit;J. Fridlyand;T. Speed|Biology;Medicine|668b1277fbece28c4841eeab1c97e4ebd0079700;1f87134a630c2dbb9a3645ba658954f00b620a77;7da323e7103245eeaed32367c46abe3f4913df86;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;1dd6c46d868accd5acffd02e4b08b003534b924e|2070970;49967517;2256698086|True;True;True|desc;desc;desc
2c47bd8bd699914e3535292b17ba46542800845c|10.5555/2627435.2697065||||3133-3181|We evaluate 179 classiﬁers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classiﬁers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classiﬁers available today. We use 121 data sets , which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve signiﬁcant conclusions about the classiﬁer behavior, not dependent on the data set collection. The classiﬁers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically signiﬁcant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classiﬁers (3 out of 5 bests classiﬁers are RF), followed by SVM (4 classiﬁers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Toronto_2014|Toronto|Do we need hundreds of classifiers to solve real world classification problems?|2014|1942181;2746944;145676637;2061412126|M. Delgado;E. Cernadas;S. Barro;D. Amorim|Computer Science;Mathematics|5d150cec2775f9bc863760448f14104cc8f42368;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;dd9b99fac67c18be82d7763a8fbf231fc3512423;23e44c7c6929bbb1ee5bc111e81e242f4835b712;e50f4d3316d13841c287dcdf5479d7820d593571;4f975da00a5b2a2f7236e34edcb7274e5fdab937;07abd02f02774d178f26ca99937e5f94001a9ec9;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;8592e46a5435d18bba70557846f47290b34c1aa5;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;78989616eeeac55b202e3e4205225e7135054185;574449170f293dfa868771e9ee0403b56a19b9e9;b10e4deadf978d8fd6eec97ff18888629f4261ab;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;4a554da55fd9ff76c99e25d2ce937b225dc1100c;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70|35238678;145678691;145134886|True;True;True|desc;desc;desc
d12864a8acbab1830be755bfb9cb177e31ca5e20|10.1109/34.824821||||63-84|Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered.|NIPS|NIPS_London_2000|London|On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey|2000|144586498;1696384|R. Plamondon;S. Srihari|Computer Science|3aa1b70fdc97ae96091c5fb39cd911015ac5253e;fbf1c51548ffc9b9e538befcd71529365af23d15;df40ce107a71b770c9d0354b78fdd8989da80d2f;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;fbc913faf39b1e369dfcdcfefb354d846a46573c;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;3def68bd0f856886d34272840a7f81588f2bc082;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;4e6238c8613b5b81f81552939bce33296aedfbfe;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;7e7eb0f93c9550d7336f4bbfad5fe89604295705;a86171e13f84fe32212dd7fb6a1c31a34a47155f;ff7a293e95c0d44582b7625ee2233916f15cb361;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746|35752280;1403428213;144195041|False;True;True|desc;desc;desc
1696cbf7da0ee845c50591843993e6605adec177|10.1145/2347736.2347755|The Lancet|97.0|6789-0123_97|78 - 87|"Tapping into the ""folk knowledge"" needed to advance machine learning applications."||||A few useful things to know about machine learning|2012|1740213|Pedro M. Domingos|Computer Science|85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;5c7e5248d9eb7f373f10277410bf8506160907ea;55f44d39630646f36eac91358f8f27d1bead384c;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;22adb2413901b74128f2a02584dafa77afbd8d60;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92|100546775;3194361;2446311|False;True;True|desc;desc;desc
3a84214cb69ea0b34352285029f368b75718c32b|10.1109/ICENGTECHNOL.2017.8308186||||1-6|The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Sydney_2017|Sydney|Understanding of a convolutional neural network|2017|2338621460;37517325;1410550919|Saad Albawi;T. Mohammed;Saad Al-Zawi|Computer Science|5cbe278b65a81602a864184bbca37de91448a5f5;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;03cb4e2cb669d3f6344a733e622f07909f87ff0a;65b16da51891a6b98140d425804c8a0fd0299219;2077d0f30507d51a0d3bbec4957d55e817d66a59;3cee40494377c0e7d9c7c23a3811b481e55bce39;872bae24c109f7c30e052ac218b17a8b028d08a0;8515a302b8f389f8f1008cc2650e5ec0a6913e24;31a537c48c2bf2d98f2020df5b72c413d0fea1da;b8ebda42e272d3617375118542d4675a0c0e501d|27469806;34954622;1804638|True;False;True|desc;desc;desc
4a554da55fd9ff76c99e25d2ce937b225dc1100c|10.1075/LI.30.1.03NAD||||3-26|This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_2007|Lisbon|A survey of named entity recognition and classification|2007|40421028;1714612|David Nadeau;S. Sekine|Computer Science;Linguistics|01b24de15cf337c55b9866c4b534596ca3d93abe;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;7380e343dd4547e21d5118b16daf03d021d98c4e;8a5d0579590465494c9aba58a857af43b190b6a6;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;0090023afc66cd2741568599057f4e82b566137c;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;bd1f14e7531220c39fad8f86985cce7b283f035d|1796267433;2274681;145124475|True;False;True|desc;desc;desc
dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63|10.1109/CVPR.2001.990517|Frontiers for Young Minds|6.0|2345-6789_6|I-I|"This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ""integral image"" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a ""cascade"" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection."||||Rapid object detection using a boosted cascade of simple features|2001|2247495712;2289341703|Paul A. Viola;Michael J. Jones|Computer Science|a34e35dbbc6911fa7b94894dffdc0076a261b6f0;b3852f0113fcf8a3913c55ae92393ae6ccde347e;a675fe5a7d99ac6f7ff91fa084462faefe616148;427b168f490b56716f22b129ac93aba5425ea08f;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;605402e235bd62437baf3c9ebefe77fb4d92ee95;8515a302b8f389f8f1008cc2650e5ec0a6913e24;4b149a326e38b9237077d794a0d5f5b4865efacf;31a537c48c2bf2d98f2020df5b72c413d0fea1da;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;694bdf6e5906992dad2987a3cc8d1a176de691c9;799f927692a6c08c5e630bea78c087c5051528fc;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;b16408a97170785fb216c9e8b7920d64f478fbc8;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;fbf1c51548ffc9b9e538befcd71529365af23d15;771ca13f78a6cfda9ed99004a386e9e7e187bd34;1696cbf7da0ee845c50591843993e6605adec177;831edc3d67457db83da40d260e93bfd7559347ae|35556316;40648435;1792142|True;True;True|desc;desc;desc
2346d121f38fc19c77e0b062415519843f478163|10.1161/CIRCULATIONAHA.115.001593||||"
          1920-30
        "|Spurred by advances in processing power, memory, storage, and an unprecedented wealth of data, computers are being asked to tackle increasingly complex learning tasks, often with astonishing success. Computers have now mastered a popular variant of poker, learned the laws of physics from experimental data, and become experts in video games - tasks that would have been deemed impossible not too long ago. In parallel, the number of companies centered on applying complex data analysis to varying industries has exploded, and it is thus unsurprising that some analytic companies are turning attention to problems in health care. The purpose of this review is to explore what problems in medicine might benefit from such learning approaches and use examples from the literature to introduce basic concepts in machine learning. It is important to note that seemingly large enough medical data sets and adequate learning algorithms have been available for many decades, and yet, although there are thousands of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Thus, part of my effort will be to identify what obstacles there may be to changing the practice of medicine through statistical learning approaches, and discuss how these might be overcome.|UAI|UAI_Rome_2015|Rome|Machine Learning in Medicine|2015|2077989|Rahul C. Deo;Karsten M. Borgwardt|Computer Science;Medicine|eed9fa4483cab37eacd59db0fac4b1441431ee85;b10e4deadf978d8fd6eec97ff18888629f4261ab;b5887d18420e8ac4f4fa4c83c4952138fd956702;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;402f850dff86fb601d34b2841e6083ac0f928edd;9f387ce140c59a44eaeeea590087351461345164;e0535dedb8607d83cd2614317c99913378e89e26;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;78989616eeeac55b202e3e4205225e7135054185;07abd02f02774d178f26ca99937e5f94001a9ec9;85d727b119304dde458bcd8cf5cb87a906fb41ba;cc1cad12521b5aab43fdda5b4dec67586aef1f87;9b539d413393047b28bb7be9b195f142aaf7a80e;b16408a97170785fb216c9e8b7920d64f478fbc8;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;a3461eaf51016f9d6e85ea47173b27e019e801c4|1738575;1807998;81080659|True;True;False|desc;desc;desc
26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810|10.1609/aaai.v31i1.11164|Science|96.0|3456-7890_96|4444-4451|"
 
 Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.
 
"||||ConceptNet 5.5: An Open Multilingual Graph of General Knowledge|2016|145696762;2060230787;2232845|R. Speer;Joshua Chin;Catherine Havasi|Computer Science;Linguistics|b5887d18420e8ac4f4fa4c83c4952138fd956702|1404459229;2675885;35645263|True;True;True|desc;desc;desc
0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5|10.1109/TKDE.2013.39||||1819-1837|Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.|ICML|ICML_Sydney_2014|Sydney|A Review on Multi-Label Learning Algorithms|2014|3039887;145624000|Min-Ling Zhang;Zhi-Hua Zhou|Computer Science|fbc913faf39b1e369dfcdcfefb354d846a46573c;a85e512d8845bd007b0866b4a97e8341463f8190;cc1cad12521b5aab43fdda5b4dec67586aef1f87;5966d7c7f60898d610812e24c64d4d57855ad86a;f3203d0bdefc9670ed508ca776d08aa9f024bafa;5cbe278b65a81602a864184bbca37de91448a5f5;467568f1777bc51a15a5100516cd4fe8de62b9ab;63861fbeb7ec41986b85965b9780b428d919919e;36d442f59c61ea2912d227c24dee76778c546b0a;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;07abd02f02774d178f26ca99937e5f94001a9ec9;da048cdf883f2ac0551162cb1abd7e6d09e8e86a;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;65d53938a12c77e7920b8eb3a49df249c978ba3f;799f927692a6c08c5e630bea78c087c5051528fc;8592e46a5435d18bba70557846f47290b34c1aa5;df2a7756382540e92895f10703cec32d50c4f316|33221685;47448503;1398620187|True;True;True|desc;desc;desc
864e7db59f2ccfec1ee9f6eba79566ac7b0634df|10.1109/CVPR.2016.511||||4724-4732|Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.|AAMAS|AAMAS_Buenos_Aires_2016|Buenos Aires|Convolutional Pose Machines|2016|2797981;20569810;1733113;1774867|S. Wei;V. Ramakrishna;T. Kanade;Yaser Sheikh|Computer Science|f7d997a640f2b804676cadb8030d8b2c7bd79d85;db68a79e59291b85e10300b79c43843b651aa195;7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;f04df4e20a18358ea2f689b4c129781628ef7fc1;8e51d68250db5637cd6bc1de98a99396441399b2;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;739769f4862753fc80057194456d758d2a148ee3;b3852f0113fcf8a3913c55ae92393ae6ccde347e;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;3df952d4a724655f7520ff95d4b2cef90fff0cae|32786132;48186551;2095762|True;True;True|desc;desc;desc
6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91|10.1109/TPAMI.2018.2798607||||423-443|Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.|UAI|UAI_Los_Angeles_2017|Los Angeles|Multimodal Machine Learning: A Survey and Taxonomy|2017|1756344;118242121;49933077|T. Baltrušaitis;Chaitanya Ahuja;Louis-Philippe Morency|Computer Science;Medicine|09622b0c84bf812814af5b64b0c83dce796899c4;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;b57c54350769ffa59ff57f79ee5aad918844d298;98c25683fc8d6446448b734b1bcf08e1457f8d85;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;88816ae492956f3004daa41357166f1181c0c1bf;afa778ba0ba6333e25671cfb691a4bdda13b2868;64be9999b68e12d260ba7423f6b55ffd41552ad3;e7e25fd534e9e024da329aea546484938df305a5;8592e46a5435d18bba70557846f47290b34c1aa5;398c296d0cc7f9d180f84969f8937e6d3a413796;10f919b1a5161b560504c225cfb2d1b3a4768f80;885af28a751553be48a25b411a5d492767d4cf65;01f702f8b1f9d1314587015f1f038af4d5735e77;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;6ec7c724aa1d906e9e9f81c58497adddb22175b8;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;0090023afc66cd2741568599057f4e82b566137c;819167ace2f0caae7745d2f25a803979be5fbfae|2464550;1410550919;2100612|False;False;True|desc;desc;desc
36652428740cd30d245d55889f01a7fb04a91c93|10.1609/aaai.v32i1.11604||||3538-3545|"
 
 Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.
 
"|Workshop on Machine Learning for Health|Workshop_on_Machine_Learning_for_Health_Rome_2018|Rome|Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning|2018|35692225;40592359;19195265|Qimai Li;Zhichao Han;Xiao-Ming Wu|Computer Science;Mathematics|5c45a5d05ac564adb67811eeb9d41d6460c70135;e24b8a9531573d284647239affc6c855505b0de4;467568f1777bc51a15a5100516cd4fe8de62b9ab;ff7a293e95c0d44582b7625ee2233916f15cb361;831edc3d67457db83da40d260e93bfd7559347ae;91c380406f5a862b5937e70e720802e5c787968d;a206216c3f67605ac6e25b0178c3f156dc0f7ba0;4a554da55fd9ff76c99e25d2ce937b225dc1100c;1592fe924114866c1ac559bae33ea789930daa98;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;35b3233e521f1e9a34837c30be1957858f8f35fe|3258919;2056946837;2110208881|True;True;True|desc;desc;desc
f4156a05a47fdeda30638e10954d3674cc056ab6|10.1093/BIB/6.4.411||||411-412|This book is the first volume of a three-volume series on data mining, which introduces the reader to this rapidly growing field. Data mining, which has gained noticeable popularity in the past decade, is essentially an interdisciplinary field bringing together techniques from machine learning, pattern recognition, statistics, databases, and visualization (Cabena et al., 1998) to address the issue of exploring large and complicated databases to identify “interesting” relationships, e.g., high order interactions, or very non-linear relationships that ordinarily would not be detected by standard statistical analyses (Borok, 1997; Szolvits, 1995). This area has been approached by computer scientists and statisticians from slightly different perspectives. The author of the book is a statistician, but has tried to include a computer science theme throughout the book, in which I think he has been successful. As he mentions in the preface, the book is intended to be used either by analysts, managers, and decision makers in industry or as a textbook for an introductory course in data mining for graduate or advanced undergraduate students (in computer science or statistics). Chapter 1 is a short introductory chapter, in which in addition to a brief description of the Cross-Industry Standard Process for Data Mining (CRISP-DM), several real-world case studies are covered to motivate the topics of subsequent chapters. These case studies are also used to describe the first phase of the CRISPDM process, namely business understanding. Chapters 2 and 3 examine the next two phases of the CRISP-DM process, i.e., data understanding and data preparation. Chapter 2 is on data preprocessing, which is divided into the two major tasks of data cleaning and data transformation. In data cleaning, general methods for handling missing data, identifying misclassified records in the data, and also a graphical method for detecting outliers are described. In the data transformation section, min-max normalization and z-score standardization methods are discussed. A numerical method for detecting outliers based on z-score standardization is also covered. Exploratory data analysis is the topic of Chapter 3, which focuses on data understanding. The chapter begins with making a contrast between hypothesis testing and exploratory data analysis, and is followed by the basic ideas of dealing with correlated variables in the data set. Most of this chapter is dedicated to exploring the variables in a real data set, in which by using several diagrams a number of intuitive approaches for obtaining a high level understanding of the data are proposed.|RSS|RSS_Toronto_2005|Toronto|Discovering Knowledge in Data: An Introduction to Data Mining|2005|144539424|N. Chawla|Computer Science;Mathematics|9d75cc322a4e06d0a3a868cb91b04219a289c12c;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;86cff4d050beb90fed2e1ceac8940c8221b120aa;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;a244c47a1d4a8c2894b22807df8c7eec16cc110a;3fa5f45ddbd5184f10bfb92e367493c5a344f207;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;16c0ef924da1f6b510c9c783ac764156f5a3d631;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;a3461eaf51016f9d6e85ea47173b27e019e801c4;55f44d39630646f36eac91358f8f27d1bead384c;8db9df2eadea654f128c1887722c677c708e8a47;7ad66cba3b7e3abae7ef33122588512a146f7f77;d04d6db5f0df11d0cff57ec7e15134990ac07a4f|49943757;29905643;47448503|True;True;True|desc;desc;desc
9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a|10.1093/nar/gky1016||||D886 - D894|Abstract Combined Annotation-Dependent Depletion (CADD) is a widely used measure of variant deleteriousness that can effectively prioritize causal variants in genetic analyses, particularly highly penetrant contributors to severe Mendelian disorders. CADD is an integrative annotation built from more than 60 genomic features, and can score human single nucleotide variants and short insertion and deletions anywhere in the reference assembly. CADD uses a machine learning model trained on a binary distinction between simulated de novo variants and variants that have arisen and become fixed in human populations since the split between humans and chimpanzees; the former are free of selective pressure and may thus include both neutral and deleterious alleles, while the latter are overwhelmingly neutral (or, at most, weakly deleterious) by virtue of having survived millions of years of purifying selection. Here we review the latest updates to CADD, including the most recent version, 1.4, which supports the human genome build GRCh38. We also present updates to our website that include simplified variant lookup, extended documentation, an Application Program Interface and improved mechanisms for integrating CADD scores into other tools or applications. CADD scores, software and documentation are available at https://cadd.gs.washington.edu.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rome_2018|Rome|CADD: predicting the deleteriousness of variants throughout the human genome|2018|80243122;2113693105;31273465;2431330;6534571|Philipp Rentzsch;D. Witten;G. Cooper;J. Shendure;Martin Kircher|Computer Science;Medicine;Biology|402f850dff86fb601d34b2841e6083ac0f928edd;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;01f29addca4dc6f189f903cb133dea7585813a6f;546785490ac417be1f83ced6a8272e934934f411;5ed59f49c1bb7de06cfa2a9467d5efb535103277;f4a5503783487eba5c5e34b1d02c09016b244b1d;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;4a554da55fd9ff76c99e25d2ce937b225dc1100c;0e90a73f03902cbe915af1aff54ea7f0b3373680|2285968829;2375564;2067208983|True;True;True|desc;desc;desc
ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e|10.5555/1756006.1756008|Cell|6.0|5678-9012_6|19-60|Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.||||Online Learning for Matrix Factorization and Sparse Coding|2009|2599292;144570279;144189388;1699339|J. Mairal;F. Bach;J. Ponce;G. Sapiro|Computer Science;Mathematics|eed9fa4483cab37eacd59db0fac4b1441431ee85;d05d86db86a4ac0d95e6dcd951b42a9651939793;7da323e7103245eeaed32367c46abe3f4913df86;f86f1748d1b6d22870f4347fd5d65314ba800583;f762cc39a824de1360e8223222739aaa4cd4168c|144264986;13919023;1744127|True;True;True|desc;desc;desc
2de0a40e9a5d4f1feb07d61af5a5d87a069653f0|10.1109/69.553155||||866-883|Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cape_Town_1996|Cape Town|Data Mining: An Overview from a Database Perspective|1996|153314745;145325584;144019071|Ming-Syan Chen;Jiawei Han;Philip S. Yu|Computer Science||3090725;1701686;1974678|False;False;True|desc;desc;desc
10f919b1a5161b560504c225cfb2d1b3a4768f80|10.1136/svn-2017-000101||||230 - 243|Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.|NeurIPS|NeurIPS_Berlin_2017|Berlin|Artificial intelligence in healthcare: past, present and future|2017|67092021;2117937034;1976425918;1974599;36156845;119918227;47454309;46829048;2108094216|F. Jiang;Yong Jiang;Hui Zhi;Yi Dong;Hao Li;Sufeng Ma;Yilong Wang;Q. Dong;Haipeng Shen;Yongjun Wang|Computer Science;Medicine|6aae0dc122102693e8136856ffc8b72df7f78386;78989616eeeac55b202e3e4205225e7135054185;b10e4deadf978d8fd6eec97ff18888629f4261ab;d516daff247f7157fccde6649ace91d969cd1973;7ad66cba3b7e3abae7ef33122588512a146f7f77;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0|3177854;6931417;144922861|True;True;True|desc;desc;desc
fee4db01f6f981931dfd87376a8f861353d1e494|10.1037/bul0000084||||187–232|Suicidal thoughts and behaviors (STBs) are major public health problems that have not declined appreciably in several decades. One of the first steps to improving the prevention and treatment of STBs is to establish risk factors (i.e., longitudinal predictors). To provide a summary of current knowledge about risk factors, we conducted a meta-analysis of studies that have attempted to longitudinally predict a specific STB-related outcome. This included 365 studies (3,428 total risk factor effect sizes) from the past 50 years. The present random-effects meta-analysis produced several unexpected findings: across odds ratio, hazard ratio, and diagnostic accuracy analyses, prediction was only slightly better than chance for all outcomes; no broad category or subcategory accurately predicted far above chance levels; predictive ability has not improved across 50 years of research; studies rarely examined the combined effect of multiple risk factors; risk factors have been homogenous over time, with 5 broad categories accounting for nearly 80% of all risk factor tests; and the average study was nearly 10 years long, but longer studies did not produce better prediction. The homogeneity of existing research means that the present meta-analysis could only speak to STB risk factor associations within very narrow methodological limits—limits that have not allowed for tests that approximate most STB theories. The present meta-analysis accordingly highlights several fundamental changes needed in future studies. In particular, these findings suggest the need for a shift in focus from risk factors to machine learning-based risk algorithms.|UAI|UAI_Tokyo_2017|Tokyo|Risk Factors for Suicidal Thoughts and Behaviors: A Meta-Analysis of 50 Years of Research|2017|39865485;144945435;6643595;3527446;5769290;47932906;8256362;2493778;4751081;5027772|J. Franklin;J. Ribeiro;K. Fox;K. Bentley;Evan M. Kleiman;Xieyining Huang;Katherine M. Musacchio;A. Jaroszewski;B. Chang;M. Nock|Medicine;Psychology|799f927692a6c08c5e630bea78c087c5051528fc;f7d997a640f2b804676cadb8030d8b2c7bd79d85;c6bbfb4fcaecc779c899af4bb52083870f4b996a;9d46dc975aeed3f96bddb144079b50238f746ecd;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;b57c54350769ffa59ff57f79ee5aad918844d298;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;9670485f526f2254c0f34e64d9ca06f665a0bd17;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;f4156a05a47fdeda30638e10954d3674cc056ab6;12439a6ff384e95ee2262ee982bc055534e30487|2149106173;1716986;1801719|True;True;True|desc;desc;desc
5966d7c7f60898d610812e24c64d4d57855ad86a|10.1126/science.aal4230||||183 - 186|Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.|ACL|ACL_Sydney_2016|Sydney|Semantics derived automatically from language corpora contain human-like biases|2016|144537437;145315445;47735253|Aylin Caliskan;J. Bryson;Arvind Narayanan|Computer Science;Medicine;Psychology|5c5be36e3111e42247d78a6d529e4b1d7d2ced12;739769f4862753fc80057194456d758d2a148ee3;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;9d46dc975aeed3f96bddb144079b50238f746ecd;1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435|2064160;2125244;3258919|True;True;True|desc;desc;desc
5794141889d0e994c3103b0aaab08a18222c9c43|10.1093/bioinformatics/16.10.906||||"
          906-14
        "|"MOTIVATION
DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.


RESULTS
We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97,802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.


AVAILABILITY
The SVM software is available at http://www.cs. columbia.edu/ approximately bgrundy/svm."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Auckland_2000|Auckland|Support vector machine classification and validation of cancer tissue samples using microarray expression data|2000|1716986;1685083;143857271;34700222;5049147;1733689|T. Furey;N. Cristianini;Nigel P. Duffy;D. Bednarski;M. Schummer;D. Haussler|Computer Science;Medicine;Biology|c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;a86171e13f84fe32212dd7fb6a1c31a34a47155f;38f23fe236b152cd4983c8f30d305a568afd0d3e|145193332;33805504;32786132|True;True;True|desc;desc;desc
a86171e13f84fe32212dd7fb6a1c31a34a47155f|10.1073/PNAS.97.1.262|JACC|100.0|1234-5678_100|"
          262-7
        "|We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their flexibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability to handle large feature spaces, and the ability to identify outliers. We test several SVMs that use different similarity metrics, as well as some other supervised learning methods, and find that the SVMs best identify sets of genes with a common function using expression data. Finally, we use SVMs to predict functional roles for uncharacterized yeast ORFs based on their expression data.||||Knowledge-based analysis of microarray gene expression data by using support vector machines.|2000|143955418;2361327;2116443182;1685083;2070640;1716986;145729499;1733689|M. S. Brown;W. Grundy;D. Lin;N. Cristianini;C. Sugnet;T. Furey;M. Ares;D. Haussler|Computer Science;Medicine;Biology|dd9b99fac67c18be82d7763a8fbf231fc3512423;f4156a05a47fdeda30638e10954d3674cc056ab6;5cbe278b65a81602a864184bbca37de91448a5f5;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;d133cb102ad0f81e3fd17a7db090b28afc124c4a;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;43d2ed5c3c55c1100450cd74dc1031afa24d37b2|1694538098;2076086;143868587|False;True;False|desc;desc;desc
338a891907dce447da9a0fa2f27221bd35164163|10.1145/775152.775226|Cell|73.0|5678-9012_73|519-528|The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.||||Mining the peanut gallery: opinion extraction and semantic classification of product reviews|2003|31744438;145840115;1766638|Kushal Dave;S. Lawrence;David M. Pennock|Computer Science;Business|819167ace2f0caae7745d2f25a803979be5fbfae|37412357;1784123;2127057|True;True;True|desc;desc;desc
62ed272e0e8b7be356c7f7595f5b7a22797a1c3e|10.1039/b918972f||||"
          230-67
        "|The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.|Workshop on AI for Earth|Workshop_on_AI_for_Earth_New_York_2010|New York|Support vector machines for classification and regression.|2010|2793728;31847168|R. Brereton;G. Lloyd|Computer Science;Chemistry;Medicine|86cff4d050beb90fed2e1ceac8940c8221b120aa;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;58a8bead87c8c1e37460dce28285c053c270f6e7;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;872bae24c109f7c30e052ac218b17a8b028d08a0;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;2a3842f6070b4554ff21fe62b2a486657d9a304a;033f25ad905ef2ed32a8331cf38b83953ff15922;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;7e7eb0f93c9550d7336f4bbfad5fe89604295705;9670485f526f2254c0f34e64d9ca06f665a0bd17;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;f04df4e20a18358ea2f689b4c129781628ef7fc1;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;53834f0ee8df731cf0e629cd594dce0afaaa3d97;12439a6ff384e95ee2262ee982bc055534e30487|2314654;1688882;32920239|True;False;True|desc;desc;desc
12439a6ff384e95ee2262ee982bc055534e30487|10.1145/1553374.1553463||||689-696|Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Athens_2009|Athens|Online dictionary learning for sparse coding|2009|2599292;144570279;144189388;1699339|J. Mairal;F. Bach;J. Ponce;G. Sapiro|Computer Science|22adb2413901b74128f2a02584dafa77afbd8d60;dd9b99fac67c18be82d7763a8fbf231fc3512423;877374c2913b787ee9f958f39e31c75d39ebcc15;a244c47a1d4a8c2894b22807df8c7eec16cc110a;48e752c719d33ff55b3b3bec3538727f8ce69399;1c00df1cb85fa7886b6666599eab59f2b301dd5d;1696cbf7da0ee845c50591843993e6605adec177;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;24e6c5bfe9bb0751e5708b501d04e860011b2953;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;a20bfec3c95aad003dcb45a21a220c19cca8bb66;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;b5887d18420e8ac4f4fa4c83c4952138fd956702|2256269;120704132;1702915|True;True;True|desc;desc;desc
0165568bcc1a819c18564567f2ec15d859be2519|10.3115/1613715.1613751|Cell|14.0|5678-9012_14|254-263|Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.||||Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks|2008|144621026;1401020033;1746807;34699434|R. Snow;Brendan T. O'Connor;Dan Jurafsky;A. Ng|Computer Science;Linguistics|825ca26af5a2a510dbc1a7b97587212bc98ae968;de2be42659be5c43c1a992b5d7fe6daf14e571dd;ff7a293e95c0d44582b7625ee2233916f15cb361;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;d517b13f2b152c913b81ce534a149493517dbdad;9b0dd87208a03e78105491e3727213b9b8ac0419;c6a83c4fcc99ba6753109301949c5b7cfa978079;12439a6ff384e95ee2262ee982bc055534e30487;7ad66cba3b7e3abae7ef33122588512a146f7f77;5c45a5d05ac564adb67811eeb9d41d6460c70135;4157ed3db4c656854e69931cb6089b64b08784b9;441c31274f4535a4a50892c1ad6e19eacfd17f8c;5d150cec2775f9bc863760448f14104cc8f42368;872bae24c109f7c30e052ac218b17a8b028d08a0|1399279909;40449634;2066277988|False;True;True|desc;desc;desc
7ab0f0da686cd4094fd96f5a30e0b6072525fd09|10.1146/annurev-bioeng-071516-044442||||"
          221-248
        "|This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.|ICAPS|ICAPS_Tokyo_2017|Tokyo|Deep Learning in Medical Image Analysis.|2017|144986260;46531894;143802908|D. Shen;Guorong Wu;Heung-Il Suk|Computer Science;Medicine|49bdeb07b045dd77f0bfe2b44436608770235a23;afa778ba0ba6333e25671cfb691a4bdda13b2868;55f44d39630646f36eac91358f8f27d1bead384c;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;f3203d0bdefc9670ed508ca776d08aa9f024bafa;a7a407968c13ced804a063259d72315a43b84f29;305d689afb6574ffec7b01e24431d541d0ce6f5d;d05d86db86a4ac0d95e6dcd951b42a9651939793;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4|2066516991;5733445;2939803|False;True;True|desc;desc;desc
049aca6228fb68a263369380eda6d9a4fcbdb382|10.1063/1.4822376|Radiology|89.0|0123-4567_89|906-|"From the Publisher: 
As book review editor of the IEEE Transactions on Neural Networks, Mohamad Hassoun has had the opportunity to assess the multitude of books on artificial neural networks that have appeared in recent years. Now, in Fundamentals of Artificial Neural Networks, he provides the first systematic account of artificial neural network paradigms by identifying clearly the fundamental concepts and major methodologies underlying most of the current theory and practice employed by neural network researchers. 
Such a systematic and unified treatment, although sadly lacking in most recent texts on neural networks, makes the subject more accessible to students and practitioners. Here, important results are integrated in order to more fully explain a wide range of existing empirical observations and commonly used heuristics. There are numerous illustrative examples, over 200 end-of-chapter analytical and computer-based problems that will aid in the development of neural network analysis and design skills, and a bibliography of nearly 700 references. 
Proceeding in a clear and logical fashion, the first two chapters present the basic building blocks and concepts of artificial neural networks and analyze the computational capabilities of the basic network architectures involved. Supervised, reinforcement, and unsupervised learning rules in simple nets are brought together in a common framework in chapter three. The convergence and solution properties of these learning rules are then treated mathematically in chapter four, using the ""average learning equation"" analysis approach. This organization of material makes it natural to switch into learning multilayer nets using backpropand its variants, described in chapter five. Chapter six covers most of the major neural network paradigms, while associative memories and energy minimizing nets are given detailed coverage in the next chapter. The final chapter takes up Boltzmann machines and Boltzmann learning along with other global search/optimization algorithms such as stochastic gradient search, simulated annealing, and genetic algorithms."||||Fundamentals of Artificial Neural Networks|1996|2679783;8909922;2096380087;2228894764|M. Hassoun;N. Intrator;S. Mckay;Wolfgang Christian|Computer Science|61394599ed0aabe04b724c7ca3a778825c7e776f;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;36652428740cd30d245d55889f01a7fb04a91c93;6981ea66000e2c98f8a81f4bef05802234d986a4;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70|3090725;2490652;1491359904|True;False;True|desc;desc;desc
602f31242e577d2d05f918a3080fd50095e7faed|10.1109/ASPAA.2003.1285840|JAMA|63.0|7890-1234_63|143-146|Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.||||Factors in automatic musical genre classification of audio signals|2003|2286187694;1693065|Tao Li;G. Tzanetakis|Computer Science|273dfbcb68080251f5e9ff38b4413d7bd84b10a1;86cff4d050beb90fed2e1ceac8940c8221b120aa;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;bcce96a2a074448953fc61a29a84afbdfc8db55a;65b16da51891a6b98140d425804c8a0fd0299219;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;3def68bd0f856886d34272840a7f81588f2bc082;61e27dbae190b82639c57f180ecf97e4c46fcad9;55f44d39630646f36eac91358f8f27d1bead384c;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;9eb715fe0347445a2d63518cbb476d345ba86233;5794141889d0e994c3103b0aaab08a18222c9c43;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;6df11b0bb0244d4d36e8955436067cc5d19734fa;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;10f919b1a5161b560504c225cfb2d1b3a4768f80|2285465435;2312900424;1716788|False;True;True|desc;desc;desc
fd8ce955dc0c570b66305dfbc65e4ed5f37658d0|10.1109/mex.1987.4307100|BMJ|51.0|8901-2345_51|92-93|Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.||||Induction: Processes of Inference, Learning, and Discovery|1987|94653581;2009767;2518186;1756123;1743808|J. H. Holland;K. Holyoak;R. Nisbett;Paul Thagard;S. Smoliar|Computer Science;Philosophy|4e6238c8613b5b81f81552939bce33296aedfbfe;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;23e44c7c6929bbb1ee5bc111e81e242f4835b712;ff7a293e95c0d44582b7625ee2233916f15cb361;0ca26f9a98dda0abb737692f72ffa682df14cb2f;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;b8ebda42e272d3617375118542d4675a0c0e501d;222d3a63d4f81d39ea324530b57328c58f298888;5aefde4203ce46ea900a96835a7c59a5f50800e7;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;a34e35dbbc6911fa7b94894dffdc0076a261b6f0|1679209;120704132;69539592|True;True;True|desc;desc;desc
9691f67f5075bde2fd70da0135a4a70f25ef042b|10.1145/1273496.1273598||||3-30|We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\epsilon}$$ is $${\tilde{O}(1 / \epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\Omega(1 / \epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\tilde{O}(d/(\lambda \epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.|NeurIPS|NeurIPS_Auckland_2007|Auckland|Pegasos: primal estimated sub-gradient solver for SVM|2007|1389955537;1740765;1706280;145658292|Shai Shalev-Shwartz;Y. Singer;N. Srebro;Andrew Cotter|Computer Science;Mathematics|339a8e4cb0eba77675711ac255ac2a5d7ede1d53;92ace17730c2173e642934d64f96d359697b7a93;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;fee4db01f6f981931dfd87376a8f861353d1e494;5c5e69387020d7ca7d49487ca841958dc5e08ce6;1592fe924114866c1ac559bae33ea789930daa98;9b539d413393047b28bb7be9b195f142aaf7a80e;23e44c7c6929bbb1ee5bc111e81e242f4835b712;8de174ab5419b9d3127695405efd079808e956e8;2346d121f38fc19c77e0b062415519843f478163;22fe619996b59c09cb73be40103a123d2e328111;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;9071775ebcfebddd54d879fe7e6c627673e4d305|1433007765;2547681;3322351|True;True;True|desc;desc;desc
72e93aa6767ee683de7f001fa72f1314e40a8f35|10.1109/ICASSP.2013.6639343||||8595-8598|We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Buenos_Aires_2011|Buenos Aires|Building high-level features using large scale unsupervised learning|2011|2827616;1706809;3089272;145139947;32131713;2118440152;49959210;34699434|Quoc V. Le;Marc'Aurelio Ranzato;R. Monga;M. Devin;G. Corrado;Kai Chen;J. Dean;A. Ng|Computer Science;Mathematics|b954efe5e46b8952f5a8daf42e7e535119b5408b;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c|145272844;1680638;2338621460|True;False;True|desc;desc;desc
48e752c719d33ff55b3b3bec3538727f8ce69399|10.1109/5254.920602|PNAS|76.0|4567-8901_76|72-79|The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.||||Ontology Learning for the Semantic Web|2002|1806905;1752093|A. Maedche;Steffen Staab|Computer Science;Engineering|1c00df1cb85fa7886b6666599eab59f2b301dd5d;06645d735b59b14479ae1d0392136bbf44227d0f;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;85d727b119304dde458bcd8cf5cb87a906fb41ba;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;91c380406f5a862b5937e70e720802e5c787968d|2229081;2157480;2418520|True;True;True|desc;desc;desc
a6f835ca6e12245a835ab6074bc6ec2c3c60b85a|10.1109/TEVC.2019.2890858||||828-841|Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.|ICCV|ICCV_Auckland_2017|Auckland|One Pixel Attack for Fooling Deep Neural Networks|2017|1730754;145197293;145106127|Jiawei Su;Danilo Vasconcellos Vargas;K. Sakurai|Computer Science;Mathematics|6adf016e7531c91100d3cf4a74f5d4c87b26b528;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;5e095981ebf4d389e9356bd56e59e0ade1b42e88;2c47bd8bd699914e3535292b17ba46542800845c;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;dd971c07879e1ce12b06991319528c06280eeb9b;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;825ca26af5a2a510dbc1a7b97587212bc98ae968;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce|47156522;144322708;1709589|True;False;False|desc;desc;desc
bcce96a2a074448953fc61a29a84afbdfc8db55a|10.1561/2200000018||||107-194|Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Madrid_2012|Madrid|Online Learning and Online Convex Optimization|2012|1389955537|Shai Shalev-Shwartz|Computer Science;Mathematics|f8b7a3434f887ce4570b7e98c7f1b91c008042d4;1dae4d61cd74cc919ecc638bde6b7125728ea97b;7e7eb0f93c9550d7336f4bbfad5fe89604295705;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;5aefde4203ce46ea900a96835a7c59a5f50800e7;86f0b58404a264a6216e29c78a5c113d900ca461;6aae0dc122102693e8136856ffc8b72df7f78386;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;bb144c04b9eb44579b19d21c3d5954401408440b;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;e0535dedb8607d83cd2614317c99913378e89e26;799f927692a6c08c5e630bea78c087c5051528fc;222d3a63d4f81d39ea324530b57328c58f298888;1dd6c46d868accd5acffd02e4b08b003534b924e;e4a85af3f5dc41e13dc2cae9ee851953709b764e;a1874aafa8730bdd4b28f29d025141c13ee28b58;bf7dcbee272428a2aa3c534200743ff7ab2047f8|5875612;145511365;145729499|True;False;True|desc;desc;desc
62ccd99a65bfc7c735ae1f33b75b107665de95df|10.1145/3298981|The Lancet|11.0|6789-0123_11|1 - 19|Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.||||Federated Machine Learning|2019|153096457;1614034792;11573257;8230559|Qiang Yang;Yang Liu;Tianjian Chen;Yongxin Tong|Computer Science|f4a5503783487eba5c5e34b1d02c09016b244b1d;222d3a63d4f81d39ea324530b57328c58f298888;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;78947497cbbffc691aac3f590d972130259af9ce;d517b13f2b152c913b81ce534a149493517dbdad;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;8592e46a5435d18bba70557846f47290b34c1aa5;605402e235bd62437baf3c9ebefe77fb4d92ee95;d422df8bff4e677a3077635db116679d25142bfc;d0c882bcae6531fa13e75bcc5c297b9985f207f7|40459179;1401020033;39717886|True;True;True|desc;desc;desc
4d1fdd81f033cd58f3723bfc61e7d12079647a7a|10.1056/NEJMp1606181|Radiology|36.0|0123-4567_36|"
          1216-9
        "|The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.||||Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.|2016|3797258;39714312|Z. Obermeyer;E. Emanuel|Computer Science;Medicine||2112109655;2746944;2347034162|True;True;False|desc;desc;desc
427b168f490b56716f22b129ac93aba5425ea08f|10.1145/1150402.1150429||||217-226|Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.|NeurIPS|NeurIPS_Sydney_2006|Sydney|Training linear SVMs in linear time|2006|1680188|T. Joachims|Computer Science;Mathematics|09622b0c84bf812814af5b64b0c83dce796899c4;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;dd971c07879e1ce12b06991319528c06280eeb9b;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;f9d119346b0773ea83251598fa5305bc75bac8ab;cbac8b0d82ea8e9251d5530695841d816cb196b9;16c0ef924da1f6b510c9c783ac764156f5a3d631;a1874aafa8730bdd4b28f29d025141c13ee28b58;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;e24b8a9531573d284647239affc6c855505b0de4|2601522;143711421;2285968829|True;True;True|desc;desc;desc
afcc28d71be4ea6a48a339f9e4e5557d1b2b25be|10.1109/TSMCC.2011.2161285|Science|63.0|3456-7890_63|463-484|Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.||||A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches|2012|1924061;143741583;1695535;2256695032;2256697310|M. Galar;Alberto Fernández;E. Tartas;H. Bustince;F. Herrera|Computer Science|402f850dff86fb601d34b2841e6083ac0f928edd|1773836;2300131692;34700222|True;True;True|desc;desc;desc
78989616eeeac55b202e3e4205225e7135054185|10.1109/TCCN.2017.2758370||||563-575|We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Toronto_2017|Toronto|An Introduction to Deep Learning for the Physical Layer|2017|1388350203;1749686|Tim O'Shea;J. Hoydis|Computer Science;Mathematics;Engineering|5e095981ebf4d389e9356bd56e59e0ade1b42e88;c43025c429b1fbf6f1379f61801a1b40834d62e7;1696cbf7da0ee845c50591843993e6605adec177;48e752c719d33ff55b3b3bec3538727f8ce69399;1592fe924114866c1ac559bae33ea789930daa98;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;5ed59f49c1bb7de06cfa2a9467d5efb535103277;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;3fa5f45ddbd5184f10bfb92e367493c5a344f207;bf7dcbee272428a2aa3c534200743ff7ab2047f8;908cca0abefc35acc38033603714fbb1bcadc49d;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;dd971c07879e1ce12b06991319528c06280eeb9b|39455775;2070970;2493778|False;True;True|desc;desc;desc
5ed59f49c1bb7de06cfa2a9467d5efb535103277|10.1145/203330.203343||||58-68|Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Moscow_1995|Moscow|Temporal difference learning and TD-Gammon|1995|1699108|G. Tesauro|Computer Science|8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;b16408a97170785fb216c9e8b7920d64f478fbc8;0ba86604228b555475496e200f31878df3aabd6e;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;815c84ab906e43f3e6322f2ca3fd5e1360c64285;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;36652428740cd30d245d55889f01a7fb04a91c93|40291345;1660855299;19225295|False;False;False|desc;desc;desc
a486e2839291111bb44fa1f07731ada123539f75|10.1162/tacl_a_00065||||339-351|We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.|RSS|RSS_Sydney_2016|Sydney|Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation|2016|2109675545;144927151;2827616;2048712;48607963;2545358;144203200;1765169;145233583;2227182886;48342565;2056946837|Melvin Johnson;M. Schuster;Quoc V. Le;M. Krikun;Yonghui Wu;Z. Chen;Nikhil Thorat;F. Viégas;M. Wattenberg;Gregory S. Corrado;Macduff Hughes;Jeffrey Dean|Computer Science;Linguistics|2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;5794141889d0e994c3103b0aaab08a18222c9c43;3df952d4a724655f7520ff95d4b2cef90fff0cae;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;a27089efabc5f4abd5ddf2be2a409bff41f31199;b57c54350769ffa59ff57f79ee5aad918844d298;bb144c04b9eb44579b19d21c3d5954401408440b;1592fe924114866c1ac559bae33ea789930daa98;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;88816ae492956f3004daa41357166f1181c0c1bf|37257989;1792298;2315504|False;True;True|desc;desc;desc
7ad66cba3b7e3abae7ef33122588512a146f7f77|10.1109/TKDE.2021.3070203||||5586-5609|Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Los_Angeles_2017|Los Angeles|A Survey on Multi-Task Learning|2017|46867608;152290618|Yu Zhang;Qiang Yang|Computer Science;Mathematics|81a4fd3004df0eb05d6c1cef96ad33d5407820df;427b168f490b56716f22b129ac93aba5425ea08f;58a8bead87c8c1e37460dce28285c053c270f6e7;a7a407968c13ced804a063259d72315a43b84f29;1592fe924114866c1ac559bae33ea789930daa98;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;2521c3d76bc439c961b7003080f4a7a661949547|39286677;144095199;34361437|True;True;True|desc;desc;desc
8e51d68250db5637cd6bc1de98a99396441399b2|10.1561/2200000073|BMJ|19.0|8901-2345_19|355-607|"Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a ""global"" cost to every such transport, using the ""local"" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications."||||Computational Optimal Transport|2018|145481009;1711979|G. Peyré;Marco Cuturi|Computer Science;Mathematics|1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;48e752c719d33ff55b3b3bec3538727f8ce69399;d516daff247f7157fccde6649ace91d969cd1973;c43025c429b1fbf6f1379f61801a1b40834d62e7;b57c54350769ffa59ff57f79ee5aad918844d298;f04df4e20a18358ea2f689b4c129781628ef7fc1;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;58a8bead87c8c1e37460dce28285c053c270f6e7;7ad66cba3b7e3abae7ef33122588512a146f7f77;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;e7e25fd534e9e024da329aea546484938df305a5;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;bb144c04b9eb44579b19d21c3d5954401408440b;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;8db9df2eadea654f128c1887722c677c708e8a47;265644f1b6740ca34bfbe9762b90b33021adde62;ea58af907495e97c93997119db4a59fab5cd3683|1763713;1788277;2064096900|True;True;True|desc;desc;desc
1626c940a64ad96a7ed53d7d6c0df63c6696956b|10.1145/1273496.1273596||||791-798|Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Sydney_2007|Sydney|Restricted Boltzmann machines for collaborative filtering|2007|145124475;1714004;1695689|R. Salakhutdinov;A. Mnih;Geoffrey E. Hinton|Computer Science|5ed59f49c1bb7de06cfa2a9467d5efb535103277;6ec7c724aa1d906e9e9f81c58497adddb22175b8;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;38f23fe236b152cd4983c8f30d305a568afd0d3e;21dfbc88b21b27fe8a245ab1df98edd45f655ae7|1390903788;2057173;1959810281|False;True;False|desc;desc;desc
d37fc9e9c4fedc32865b08661e7fb950df1f8fbe|10.1214/009053607000000677||||1171-1220|We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rio_de_Janeiro_2007|Rio de Janeiro|Kernel methods in machine learning|2007|143936663;1707625;46234526|Thomas Hofmann;B. Scholkopf;Alex Smola|Computer Science;Mathematics|cc1cad12521b5aab43fdda5b4dec67586aef1f87;36652428740cd30d245d55889f01a7fb04a91c93;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;d422df8bff4e677a3077635db116679d25142bfc;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;5aefde4203ce46ea900a96835a7c59a5f50800e7;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;c43025c429b1fbf6f1379f61801a1b40834d62e7;f3203d0bdefc9670ed508ca776d08aa9f024bafa;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;9670485f526f2254c0f34e64d9ca06f665a0bd17;46f74231b9afeb0c290d6d550043c55045284e5f;94549a171a61039ed1f9b5954ce42181c574ccc3;5794141889d0e994c3103b0aaab08a18222c9c43;222d3a63d4f81d39ea324530b57328c58f298888;8592e46a5435d18bba70557846f47290b34c1aa5;441c31274f4535a4a50892c1ad6e19eacfd17f8c;f9d119346b0773ea83251598fa5305bc75bac8ab|72419159;1403820685;152290618|True;False;True|desc;desc;desc
3def68bd0f856886d34272840a7f81588f2bc082|10.1145/3571730||||1 - 38|Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Rio_de_Janeiro_2022|Rio de Janeiro|Survey of Hallucination in Natural Language Generation|2022|3391272;40221187;1397300094;1660855299;49678544;98271906;38524906;23672613;94210578;47653392;2111680936;40539650|Ziwei Ji;Nayeon Lee;Rita Frieske;Tiezheng Yu;D. Su;Yan Xu;Etsuko Ishii;Yejin Bang;Delong Chen;Wenliang Dai;Andrea Madotto;Pascale Fung|Computer Science;Linguistics||2077989;2256698086;2373952|True;True;True|desc;desc;desc
c43025c429b1fbf6f1379f61801a1b40834d62e7|10.1109/ISCAS.2010.5537907||||253-256|"Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ""features"")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described."|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_Auckland_2010|Auckland|Convolutional networks and applications in vision|2010|1688882;2645384;2256269|Yann LeCun;K. Kavukcuoglu;C. Farabet|Computer Science|ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;34f25a8704614163c4095b3ee2fc969b60de4698;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;8592e46a5435d18bba70557846f47290b34c1aa5;7e7eb0f93c9550d7336f4bbfad5fe89604295705;9071775ebcfebddd54d879fe7e6c627673e4d305|40867019;2249973698;88256002|True;True;True|desc;desc;desc
0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376|10.1109/COMST.2019.2902862|Cell|20.0|5678-9012_20|2334-2360|The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. On the other hand, UAVs can operate as flying mobile terminals within a cellular network. Such cellular-connected UAVs can enable several applications ranging from real-time video streaming to item delivery. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as 3D deployment, performance analysis, channel modeling, and energy efficiency are explored along with representative results. Then, open problems and potential research directions pertaining to UAV communications are introduced. Finally, various analytical frameworks and mathematical tools, such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems.||||A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and Open Problems|2018|2250918;145412074;1702172;3223942;145118318|Mohammad Mozaffari;W. Saad;M. Bennis;Young-Han Nam;M. Debbah|Computer Science;Mathematics;Engineering|ac3ccaf58ba543fd8d7b787cf939e55345b1659f;7e7eb0f93c9550d7336f4bbfad5fe89604295705;d422df8bff4e677a3077635db116679d25142bfc;6981ea66000e2c98f8a81f4bef05802234d986a4;6df11b0bb0244d4d36e8955436067cc5d19734fa;398c296d0cc7f9d180f84969f8937e6d3a413796;8515a302b8f389f8f1008cc2650e5ec0a6913e24|71352570;1730232;1706809|False;True;False|desc;desc;desc
c62043a7d2537bbf40a84b9913957452a47fdb83|10.7551/MITPRESS/9780262170055.001.0001||||29-38|Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series|IROS|IROS_Auckland_2009|Auckland|Dataset Shift in Machine Learning|2009|1409068337;67154907;2071649;145306271|Joaquin Quionero-Candela;Masashi Sugiyama;A. Schwaighofer;Neil D. Lawrence|Computer Science|f4156a05a47fdeda30638e10954d3674cc056ab6;0023582fde36430c7e3ae81611a14e558c8f4bae;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;5794141889d0e994c3103b0aaab08a18222c9c43;6981ea66000e2c98f8a81f4bef05802234d986a4;06645d735b59b14479ae1d0392136bbf44227d0f;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;5aefde4203ce46ea900a96835a7c59a5f50800e7;bf7dcbee272428a2aa3c534200743ff7ab2047f8;fbc913faf39b1e369dfcdcfefb354d846a46573c|3507117;2056204271;34812292|True;True;True|desc;desc;desc
21cea8f56a0d067d640f923b2d69e18ed5542f6d|10.1109/34.868684||||831-843|"We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system is particularly concerned with detecting when interactions between people occur, and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely HMMs and CHMMs, for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. 
 
Finally, to deal with the problem of limited training data, a synthetic 'Alife-style' training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training."|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Los_Angeles_1999|Los Angeles|A Bayesian Computer Vision System for Modeling Human Interactions|1999|145709776;3199842;144994682|Nuria Oliver;Barbara Rosario;A. Pentland|Computer Science|f354310098e09c1e1dc88758fca36767fd9d084d;4c75b748911ddcd888c5122f7672f69caa5d661f;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;24e6c5bfe9bb0751e5708b501d04e860011b2953;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;694bdf6e5906992dad2987a3cc8d1a176de691c9;df40ce107a71b770c9d0354b78fdd8989da80d2f;398c296d0cc7f9d180f84969f8937e6d3a413796;5aefde4203ce46ea900a96835a7c59a5f50800e7;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;5e095981ebf4d389e9356bd56e59e0ade1b42e88;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;c6a83c4fcc99ba6753109301949c5b7cfa978079;a20bfec3c95aad003dcb45a21a220c19cca8bb66;5a4631d5d75e3610037f87839628a4d166581e01;d05d86db86a4ac0d95e6dcd951b42a9651939793;6aae0dc122102693e8136856ffc8b72df7f78386;175e37bca3762b3a52c6a0e153060b98a251d061;ec6200bdcc23b79a71555962cde50306c4029f1a|1967156;1787610;2536315|False;True;True|desc;desc;desc
b8ebda42e272d3617375118542d4675a0c0e501d|10.1109/CVPR.2017.572||||5385-5394|In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Los_Angeles_2017|Los Angeles|Deep Hashing Network for Unsupervised Domain Adaptation|2017|3151995;2072459779;2471253;1743991|Hemanth Venkateswara;José Eusébio;Shayok Chakraborty;S. Panchanathan|Computer Science|30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;9e475a514f54665478aac6038c262e5a6bac5e64;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;8db9df2eadea654f128c1887722c677c708e8a47;1f87134a630c2dbb9a3645ba658954f00b620a77|2406435;8270717;1755208|True;True;True|desc;desc;desc
8515a302b8f389f8f1008cc2650e5ec0a6913e24|10.1109/MSP.2008.4408441||||41-56|Due to the volume conduction multichannel electroencephalogram (EEG) recordings give a rather blurred image of brain activity. Therefore spatial filters are extremely useful in single-trial analysis in order to improve the signal-to-noise ratio. There are powerful methods from machine learning and signal processing that permit the optimization of spatio-temporal filters for each subject in a data dependent fashion beyond the fixed filters based on the sensor geometry, e.g., Laplacians. Here we elucidate the theoretical background of the common spatial pattern (CSP) algorithm, a popular method in brain-computer interface (BCD research. Apart from reviewing several variants of the basic algorithm, we reveal tricks of the trade for achieving a powerful CSP performance, briefly elaborate on theoretical aspects of CSP, and demonstrate the application of CSP-type preprocessing in our studies of the Berlin BCI (BBCI) project.|COLT|COLT_Rio_de_Janeiro_2008|Rio de Janeiro|Optimizing Spatial filters for Robust EEG Single-Trial Analysis|2008|3156886;2870603;3294736;1716788;72571095|B. Blankertz;Ryota Tomioka;S. Lemm;M. Kawanabe;K.-R. Muller|Computer Science;Engineering|0c3751db5a24c636c1aa8abfd9d63321b38cfce5;d516daff247f7157fccde6649ace91d969cd1973;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;546785490ac417be1f83ced6a8272e934934f411;09622b0c84bf812814af5b64b0c83dce796899c4;98c25683fc8d6446448b734b1bcf08e1457f8d85;dd971c07879e1ce12b06991319528c06280eeb9b;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;bd1f14e7531220c39fad8f86985cce7b283f035d;877374c2913b787ee9f958f39e31c75d39ebcc15|145371438;68982679;34207023|True;True;False|desc;desc;desc
f7d997a640f2b804676cadb8030d8b2c7bd79d85|10.5555/1756006.1859921||||2079-2107|Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rio_de_Janeiro_2010|Rio de Janeiro|On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation|2010|8974886;28280741|G. Cawley;N. L. C. Talbot|Computer Science|bd1f14e7531220c39fad8f86985cce7b283f035d;36652428740cd30d245d55889f01a7fb04a91c93;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;e4a85af3f5dc41e13dc2cae9ee851953709b764e;d997919c30fa6711bc5c25cf8c8aea34fac27b91;db68a79e59291b85e10300b79c43843b651aa195;8a5d0579590465494c9aba58a857af43b190b6a6;01f29addca4dc6f189f903cb133dea7585813a6f;33e46a618fdb22d46951f548d6ceeb384e7f1687;72e93aa6767ee683de7f001fa72f1314e40a8f35;546785490ac417be1f83ced6a8272e934934f411;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;c2b381b24aabf237394059fed7920cd6fd0e67b8|1716986;1398980400;9802604|True;True;True|desc;desc;desc
d0c882bcae6531fa13e75bcc5c297b9985f207f7|10.1145/360402.360406|The Lancet|34.0|6789-0123_34|1-15|With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.||||Web mining research: a survey|2000|2937911;1755851|R. Kosala;H. Blockeel|Computer Science|9e475a514f54665478aac6038c262e5a6bac5e64;222d3a63d4f81d39ea324530b57328c58f298888;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;5a391667242b4a631acdd5917681b16a86523987;033f25ad905ef2ed32a8331cf38b83953ff15922|38524906;2125517703;144126414|True;True;False|desc;desc;desc
a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e|10.1109/TPAMI.2008.275|Cell|25.0|5678-9012_25|105-119|The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.||||Faster and Better: A Machine Learning Approach to Corner Detection|2008|1721991;145881323;144418842|E. Rosten;R. Porter;T. Drummond|Computer Science;Medicine|01f29addca4dc6f189f903cb133dea7585813a6f;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;7da323e7103245eeaed32367c46abe3f4913df86;5e095981ebf4d389e9356bd56e59e0ade1b42e88;adc61e21eafecfbf6ebecc570f9f913659a2bfb2|143750713;2274883;46718993|True;True;True|desc;desc;desc
467568f1777bc51a15a5100516cd4fe8de62b9ab|10.5555/1577069.1755839||||1633-1685|The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.|ECCV|ECCV_Sydney_2009|Sydney|Transfer Learning for Reinforcement Learning Domains: A Survey|2009|39286677;144848112|Matthew E. Taylor;P. Stone|Computer Science|3aa1b70fdc97ae96091c5fb39cd911015ac5253e;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;0b544dfe355a5070b60986319a3f51fb45d1348e;f3203d0bdefc9670ed508ca776d08aa9f024bafa;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;574449170f293dfa868771e9ee0403b56a19b9e9;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;61e27dbae190b82639c57f180ecf97e4c46fcad9;98c25683fc8d6446448b734b1bcf08e1457f8d85;78989616eeeac55b202e3e4205225e7135054185;92ace17730c2173e642934d64f96d359697b7a93;4f71ab367eb37cfd145d41327f7bb14077e5e7c5;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;2346d121f38fc19c77e0b062415519843f478163;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;e838ba98e198d2dac047736e77c50c0efa49c2dc|143747945;32837403;2109916949|False;False;True|desc;desc;desc
2521c3d76bc439c961b7003080f4a7a661949547|10.1126/SCIENCE.1105809||||523 - 529|Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.|NAACL|NAACL_New_York_2005|New York|Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data|2005|46599630;3289622;1397424343;1761370;1857137|K. Sachs;O. Perez;D. Pe’er;D. Lauffenburger;G. Nolan|Computer Science;Biology;Medicine|31a537c48c2bf2d98f2020df5b72c413d0fea1da;da048cdf883f2ac0551162cb1abd7e6d09e8e86a;03cb4e2cb669d3f6344a733e622f07909f87ff0a;a486e2839291111bb44fa1f07731ada123539f75;605402e235bd62437baf3c9ebefe77fb4d92ee95;799f927692a6c08c5e630bea78c087c5051528fc;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;e838ba98e198d2dac047736e77c50c0efa49c2dc;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;b5887d18420e8ac4f4fa4c83c4952138fd956702;467568f1777bc51a15a5100516cd4fe8de62b9ab;9257779eed46107bcdce9f4dc86298572ff466ce;dd971c07879e1ce12b06991319528c06280eeb9b;f354310098e09c1e1dc88758fca36767fd9d084d;cd49acefc8d51e324aa562e5337e1c2aff067053;7380e343dd4547e21d5118b16daf03d021d98c4e;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7|145534175;39189579;1753652328|True;False;False|desc;desc;desc
5b66b1c65dcb97d1d0b18014e2e32e8522e66932|10.1145/502512.502529|Radiology|36.0|0123-4567_36|97-106|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.||||Mining time-changing data streams|2001|1740077;145029414;1740213|Geoff Hulten;Laurie Spencer;Pedro M. Domingos|Computer Science;Mathematics|872352b0a53ab6cbb4420f81df64d215d86c7d9b;e9126a98de0c39dcffe4c4f5158e037460196724;877374c2913b787ee9f958f39e31c75d39ebcc15;305d689afb6574ffec7b01e24431d541d0ce6f5d;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;36652428740cd30d245d55889f01a7fb04a91c93;175e37bca3762b3a52c6a0e153060b98a251d061;5d150cec2775f9bc863760448f14104cc8f42368;a675fe5a7d99ac6f7ff91fa084462faefe616148;62ccd99a65bfc7c735ae1f33b75b107665de95df;bf7dcbee272428a2aa3c534200743ff7ab2047f8;819167ace2f0caae7745d2f25a803979be5fbfae;398c296d0cc7f9d180f84969f8937e6d3a413796;a486e2839291111bb44fa1f07731ada123539f75;92ace17730c2173e642934d64f96d359697b7a93;81a4fd3004df0eb05d6c1cef96ad33d5407820df|3172077;1688882;1744800|True;True;True|desc;desc;desc
94549a171a61039ed1f9b5954ce42181c574ccc3|10.1093/nar/gkp356||||W652 - W660|Metabolomics is a newly emerging field of ‘omics’ research that is concerned with characterizing large numbers of metabolites using NMR, chromatography and mass spectrometry. It is frequently used in biomarker identification and the metabolic profiling of cells, tissues or organisms. The data processing challenges in metabolomics are quite unique and often require specialized (or expensive) data analysis software and a detailed knowledge of cheminformatics, bioinformatics and statistics. In an effort to simplify metabolomic data analysis while at the same time improving user accessibility, we have developed a freely accessible, easy-to-use web server for metabolomic data analysis called MetaboAnalyst. Fundamentally, MetaboAnalyst is a web-based metabolomic data processing tool not unlike many of today's web-based microarray analysis packages. It accepts a variety of input data (NMR peak lists, binned spectra, MS peak lists, compound/concentration data) in a wide variety of formats. It also offers a number of options for metabolomic data processing, data normalization, multivariate statistical analysis, graphing, metabolite identification and pathway mapping. In particular, MetaboAnalyst supports such techniques as: fold change analysis, t-tests, PCA, PLS-DA, hierarchical clustering and a number of more sophisticated statistical or machine learning methods. It also employs a large library of reference spectra to facilitate compound identification from most kinds of input spectra. MetaboAnalyst guides users through a step-by-step analysis pipeline using a variety of menus, information hyperlinks and check boxes. Upon completion, the server generates a detailed report describing each method used, embedded with graphical and tabular outputs. MetaboAnalyst is capable of handling most kinds of metabolomic data and was designed to perform most of the common kinds of metabolomic data analyses. MetaboAnalyst is accessible at http://www.metaboanalyst.ca|ICML|ICML_Madrid_2009|Madrid|MetaboAnalyst: a web server for metabolomic data analysis and interpretation|2009|144545434;2722839;36093977;2066145|J. Xia;N. Psychogios;N. Young;D. Wishart|Chemistry;Computer Science;Medicine;Biology|c62043a7d2537bbf40a84b9913957452a47fdb83;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;b954efe5e46b8952f5a8daf42e7e535119b5408b;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;65b16da51891a6b98140d425804c8a0fd0299219|40995886;144467554;1745644|False;True;False|desc;desc;desc
65b16da51891a6b98140d425804c8a0fd0299219|10.1109/CVPR.2017.35|BMJ|17.0|8901-2345_17|257-265|Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.||||Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring|2016|40648435;2111251935;2135837|Seungjun Nah;Tae Hyun Kim;Kyoung Mu Lee|Computer Science;Engineering|9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;12d1d070a53d4084d88a77b8b143bad51c40c38f;01f702f8b1f9d1314587015f1f038af4d5735e77;91c380406f5a862b5937e70e720802e5c787968d;24e6c5bfe9bb0751e5708b501d04e860011b2953;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;a244c47a1d4a8c2894b22807df8c7eec16cc110a;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;864e7db59f2ccfec1ee9f6eba79566ac7b0634df|40867019;1754307;32837403|True;False;False|desc;desc;desc
ade03d0c772c35dc8e865bdb41d7bc54d5b782d1|10.18637/JSS.V011.I09|NEJM|61.0|9012-3456_61|1-20|kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.||||kernlab - An S4 Package for Kernel Methods in R|2004|1713164;116865041;1764952;2144516|Alexandros Karatzoglou;A. Smola;K. Hornik;A. Zeileis|Computer Science;Mathematics|e4a85af3f5dc41e13dc2cae9ee851953709b764e;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;f4156a05a47fdeda30638e10954d3674cc056ab6;01f29addca4dc6f189f903cb133dea7585813a6f;602f31242e577d2d05f918a3080fd50095e7faed;ea58af907495e97c93997119db4a59fab5cd3683|46867608;145617808;144563108|True;True;True|desc;desc;desc
e4a85af3f5dc41e13dc2cae9ee851953709b764e|10.1126/science.aag2302||||602 - 606|Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.|ICML|ICML_Beijing_2016|Beijing|Solving the quantum many-body problem with artificial neural networks|2016|50666189;1752096|Giuseppe Carleo;M. Troyer|Computer Science;Medicine;Physics|0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;1051280d2b825c04f27d231aba0f8284bb297880;01f29addca4dc6f189f903cb133dea7585813a6f;0165568bcc1a819c18564567f2ec15d859be2519;6ec7c724aa1d906e9e9f81c58497adddb22175b8;45557cc70cd6989ab6b03e5aeb787e34299099f7;693914b7f38c19585e35668fd626aecf62d4c5e7;23e44c7c6929bbb1ee5bc111e81e242f4835b712;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;605402e235bd62437baf3c9ebefe77fb4d92ee95;771ca13f78a6cfda9ed99004a386e9e7e187bd34|31779043;1397398770;5385807|False;True;True|desc;desc;desc
85d727b119304dde458bcd8cf5cb87a906fb41ba|10.1109/TKDE.2005.50||||299-310|The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_London_2005|London|Using AUC and accuracy in evaluating learning algorithms|2005|2110151928;1688204|Jin Huang;C. Ling|Computer Science|5b66b1c65dcb97d1d0b18014e2e32e8522e66932;07abd02f02774d178f26ca99937e5f94001a9ec9;d079a2f877f554e00f71a6975435d8325987bdf5;2521c3d76bc439c961b7003080f4a7a661949547;d05d86db86a4ac0d95e6dcd951b42a9651939793;d63b884d5ebc739f6e1bdf861fa9276260781404;5a391667242b4a631acdd5917681b16a86523987;63861fbeb7ec41986b85965b9780b428d919919e|2897313;144751024;2276155|True;True;True|desc;desc;desc
1c00df1cb85fa7886b6666599eab59f2b301dd5d|10.1109/TNN.2006.880583||||1411-1423|In this paper, we develop an online sequential learning algorithm for single hidden layer feedforward networks (SLFNs) with additive or radial basis function (RBF) hidden nodes in a unified framework. The algorithm is referred to as online sequential extreme learning machine (OS-ELM) and can learn data one-by-one or chunk-by-chunk (a block of data) with fixed or varying chunk size. The activation functions for additive nodes in OS-ELM can be any bounded nonconstant piecewise continuous functions and the activation functions for RBF nodes can be any integrable piecewise continuous functions. In OS-ELM, the parameters of hidden nodes (the input weights and biases of additive nodes or the centers and impact factors of RBF nodes) are randomly selected and the output weights are analytically determined based on the sequentially arriving data. The algorithm uses the ideas of ELM of Huang developed for batch learning which has been shown to be extremely fast with generalization performance better than other batch training methods. Apart from selecting the number of hidden nodes, no other control parameters have to be manually chosen. Detailed performance comparison of OS-ELM is done with other popular sequential learning algorithms on benchmark problems drawn from the regression, classification and time series prediction areas. The results show that the OS-ELM is faster than the other sequential algorithms and produces better generalization performance|IROS|IROS_Los_Angeles_2006|Los Angeles|A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks|2006|2972859;145678691;1800678;145411276|Nan-Ying Liang;G. Huang;P. Saratchandran;N. Sundararajan|Computer Science;Medicine|1e41ed1ac234cba0138329047e16a8a424389e77;72e93aa6767ee683de7f001fa72f1314e40a8f35;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;220ac48a22547a455d05f416e1fd22bbd0b0788d;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;9b539d413393047b28bb7be9b195f142aaf7a80e;a85e512d8845bd007b0866b4a97e8341463f8190;fee4db01f6f981931dfd87376a8f861353d1e494;a86171e13f84fe32212dd7fb6a1c31a34a47155f;f986968735459e789890f24b6b277b0920a9725d;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3|2428490;1787610;50666189|True;True;False|desc;desc;desc
b24972552161cd9eda729e748762a73430983e3a|10.1142/9789812776655|JAMA|47.0|7890-1234_47|1-308|Support Vector Machines Basic Methods of Least Squares Support Vector Machines Bayesian Inference for LS-SVM Models Robustness Large Scale Problems LS-SVM for Unsupervised Learning LS-SVM for Recurrent Networks and Control.||||Least Squares Support Vector Machines|2002|1744439;1738575;145355963;143750713;1704135|J. Suykens;T. V. Gestel;J. Brabanter;B. Moor;J. Vandewalle|Computer Science;Mathematics|6ec7c724aa1d906e9e9f81c58497adddb22175b8;53834f0ee8df731cf0e629cd594dce0afaaa3d97;9670485f526f2254c0f34e64d9ca06f665a0bd17;dd971c07879e1ce12b06991319528c06280eeb9b;885af28a751553be48a25b411a5d492767d4cf65;05fd1da7b2e34f86ec7f010bef068717ae964332;45557cc70cd6989ab6b03e5aeb787e34299099f7;03cb4e2cb669d3f6344a733e622f07909f87ff0a;1904d633fca15140e35d893637232803b6dde6d9;872352b0a53ab6cbb4420f81df64d215d86c7d9b;ec6200bdcc23b79a71555962cde50306c4029f1a|143953836;3216372;2225350|True;True;True|desc;desc;desc
cc5afe344cc7ed7acd68a28b9774ea8023a162dc|10.1609/aimag.v38i3.2741||||50-57|We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rome_2016|Rome|"European Union Regulations on Algorithmic Decision-Making and a ""Right to Explanation"""|2016|11025540;2127497|B. Goodman;S. Flaxman|Computer Science;Law;Mathematics;Engineering|12439a6ff384e95ee2262ee982bc055534e30487;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;d516daff247f7157fccde6649ace91d969cd1973;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;739769f4862753fc80057194456d758d2a148ee3;5a391667242b4a631acdd5917681b16a86523987;38f23fe236b152cd4983c8f30d305a568afd0d3e;cc1cad12521b5aab43fdda5b4dec67586aef1f87;0e90a73f03902cbe915af1aff54ea7f0b3373680;4a554da55fd9ff76c99e25d2ce937b225dc1100c;44c7d9fe583e3d317a619297e7e949070710799f;9e475a514f54665478aac6038c262e5a6bac5e64;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;bd898f483476e3dcacf83cd85efc64e6319da0e1;a1874aafa8730bdd4b28f29d025141c13ee28b58|8181864;49370597;3149531|True;True;True|desc;desc;desc
f94455176857303605ad423599385a2341c568eb|10.1093/nar/gky318||||W257 - W263|Abstract Advancement in the field of computational research has made it possible for the in silico methods to offer significant benefits to both regulatory needs and requirements for risk assessments, and pharmaceutical industry to assess the safety profile of a chemical. Here, we present ProTox-II that incorporates molecular similarity, pharmacophores, fragment propensities and machine-learning models for the prediction of various toxicity endpoints; such as acute toxicity, hepatotoxicity, cytotoxicity, carcinogenicity, mutagenicity, immunotoxicity, adverse outcomes pathways (Tox21) and toxicity targets. The predictive models are built on data from both in vitro assays (e.g. Tox21 assays, Ames bacterial mutation assays, hepG2 cytotoxicity assays, Immunotoxicity assays) and in vivo cases (e.g. carcinogenicity, hepatotoxicity). The models have been validated on independent external sets and have shown strong performance. ProTox-II provides a freely available webserver for in silico toxicity prediction for toxicologists, regulatory agencies, computational and medicinal chemists, and all users without login at http://tox.charite.de/protox_II. The webserver takes a two-dimensional chemical structure as an input and reports the possible toxicity profile of the chemical for 33 models with confidence scores, and an overall toxicity radar chart along with three most similar compounds with known acute toxicity.|IJCAI|IJCAI_Mexico_City_2018|Mexico City|ProTox-II: a webserver for the prediction of toxicity of chemicals|2018|48180545;2069534421;32721322;2785761|Priyanka Banerjee;Andreas Eckert;Anna K. Schrey;R. Preissner|Biology;Environmental Science;Chemistry;Medicine;Computer Science|18d026ec5d0eebd17ee2c762da89540c0b3d7bde;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;12d1d070a53d4084d88a77b8b143bad51c40c38f;f86f1748d1b6d22870f4347fd5d65314ba800583;fee4db01f6f981931dfd87376a8f861353d1e494;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;e7e25fd534e9e024da329aea546484938df305a5;2a3842f6070b4554ff21fe62b2a486657d9a304a;8e51d68250db5637cd6bc1de98a99396441399b2;9e475a514f54665478aac6038c262e5a6bac5e64;9d7c04de906823a60d3ccb5f510fd0029af5c8b0|1740538;2064096900;1702283|True;True;True|desc;desc;desc
8c8215b7f8111839f0066010a530a3a9f57ba15e|10.1613/jair.530||||317-365|This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.|ICAPS|ICAPS_New_York_1998|New York|AntNet: Distributed Stigmergetic Control for Communications Networks|1998|1744127;153570946|G. D. Caro;M. Dorigo|Computer Science|b9518627db25f05930e931f56497602363a75491;1f87134a630c2dbb9a3645ba658954f00b620a77;c62043a7d2537bbf40a84b9913957452a47fdb83;18bc1d4271abe8dd6e16179cdb06524a4f396e16;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;d133cb102ad0f81e3fd17a7db090b28afc124c4a;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4|145306271;145532827;1743642|True;True;True|desc;desc;desc
23e44c7c6929bbb1ee5bc111e81e242f4835b712|10.1093/bioinformatics/btq134||||"
          1340-7
        "|"MOTIVATION
In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inflexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred.


RESULTS
In this work, we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias. The method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non-informative setting. The P-value of the observed importance provides a corrected measure of feature importance. We apply our method to simulated data and demonstrate that (i) non-informative predictors do not receive significant P-values, (ii) informative variables can successfully be recovered among non-informative variables and (iii) P-values computed with permutation importance (PIMP) are very helpful for deciding the significance of variables, and therefore improve model interpretability. Furthermore, PIMP was used to correct RF-based importance measures for two real-world case studies. We propose an improved RF model that uses the significant variables with respect to the PIMP measure and show that its prediction accuracy is superior to that of other existing models.


AVAILABILITY
R code for the method presented in this article is available at http://www.mpi-inf.mpg.de/ approximately altmann/download/PIMP.R CONTACT: altmann@mpi-inf.mpg.de, laura.tolosi@mpi-inf.mpg.de


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Tokyo_2010|Tokyo|Permutation importance: a corrected feature importance measure|2010|1712994;1798960;2097232470;49370597|A. Altmann;Laura Tolosi;Oliver Sander;Thomas Lengauer|Computer Science;Medicine;Mathematics|e24b8a9531573d284647239affc6c855505b0de4;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;64be9999b68e12d260ba7423f6b55ffd41552ad3;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;f7d997a640f2b804676cadb8030d8b2c7bd79d85;ea58af907495e97c93997119db4a59fab5cd3683;0b544dfe355a5070b60986319a3f51fb45d1348e;bf7dcbee272428a2aa3c534200743ff7ab2047f8;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;7da323e7103245eeaed32367c46abe3f4913df86|1711460;48385057;145124475|False;True;True|desc;desc;desc
bb144c04b9eb44579b19d21c3d5954401408440b|10.5555/2567709.2567736|Frontiers for Young Minds|33.0|2345-6789_33|2349-2353|Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.||||Orange: data mining toolbox in python|2013|2701105;1772593;48828579;88256002;2820151;40337361;145703442;2448692;3110092;1851562;2789798;2256143;2761087;3105120;2095762;1775384|J. Demšar;Tomaž Curk;Ales Erjavec;C. Gorup;Tomaz Hocevar;Mitar Milutinovic;M. Mozina;M. Polajnar;Marko Toplak;A. Staric;Miha Stajdohar;Lan Umek;Lan Zagar;Jure Zbontar;M. Zitnik;B. Zupan|Computer Science|4e6238c8613b5b81f81552939bce33296aedfbfe;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;d517b13f2b152c913b81ce534a149493517dbdad;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;5aefde4203ce46ea900a96835a7c59a5f50800e7;872bae24c109f7c30e052ac218b17a8b028d08a0;467568f1777bc51a15a5100516cd4fe8de62b9ab;53834f0ee8df731cf0e629cd594dce0afaaa3d97;0165568bcc1a819c18564567f2ec15d859be2519;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;4f975da00a5b2a2f7236e34edcb7274e5fdab937;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;8e51d68250db5637cd6bc1de98a99396441399b2|34660073;2152471960;31519649|True;False;True|desc;desc;desc
d079a2f877f554e00f71a6975435d8325987bdf5|10.1609/aaai.v30i1.10306||||2058-2065|"
 
 Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.
 
"|RSS|RSS_Cape_Town_2015|Cape Town|Return of Frustratingly Easy Domain Adaptation|2015|2636783;33221685;2903226|Baochen Sun;Jiashi Feng;Kate Saenko|Computer Science|8515a302b8f389f8f1008cc2650e5ec0a6913e24;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;049aca6228fb68a263369380eda6d9a4fcbdb382;0ba86604228b555475496e200f31878df3aabd6e;395de0bd3837fdf4b4b5e5f04835bcc69c279481;9071775ebcfebddd54d879fe7e6c627673e4d305;f86f1748d1b6d22870f4347fd5d65314ba800583;bb144c04b9eb44579b19d21c3d5954401408440b;546785490ac417be1f83ced6a8272e934934f411;9b0dd87208a03e78105491e3727213b9b8ac0419;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;605402e235bd62437baf3c9ebefe77fb4d92ee95;d997919c30fa6711bc5c25cf8c8aea34fac27b91|2084564300;1746034;3207228|False;False;True|desc;desc;desc
884895a86fe15cb9601df4a15a1475c07f28da3c|10.1145/1273496.1273521||||193-200|Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.|Workshop on AI for Earth|Workshop_on_AI_for_Earth_Moscow_2007|Moscow|Boosting for transfer learning|2007|1752769;152290618;1701421;1811427|Wenyuan Dai;Qiang Yang;Gui-Rong Xue;Yong Yu|Computer Science|b3852f0113fcf8a3913c55ae92393ae6ccde347e;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;df40ce107a71b770c9d0354b78fdd8989da80d2f;2a3842f6070b4554ff21fe62b2a486657d9a304a;a40f97770296c7fca2e5361cbceba3f4aae399e0;1c00df1cb85fa7886b6666599eab59f2b301dd5d;31a537c48c2bf2d98f2020df5b72c413d0fea1da;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;1626c940a64ad96a7ed53d7d6c0df63c6696956b;f986968735459e789890f24b6b277b0920a9725d|2223688580;1404268156;5059205|False;True;True|desc;desc;desc
075f328ef87a076151feb4d5b1f97b66aa597a90|10.1561/2200000050||||231-357|This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cairo_2014|Cairo|Convex Optimization: Algorithms and Complexity|2014|1815542|Sébastien Bubeck|Computer Science;Mathematics|6aae0dc122102693e8136856ffc8b72df7f78386|3698245;35244773;1865802|True;True;True|desc;desc;desc
b3852f0113fcf8a3913c55ae92393ae6ccde347e|10.1145/1273496.1273592|BMJ|10.0|8901-2345_10|759-766|"We present a new machine learning framework called ""self-taught learning"" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation."||||Self-taught learning: transfer learning from unlabeled data|2007|2979876;2078284037;1697141;1409971380;34699434|Rajat Raina;Alexis Battle;Honglak Lee;Ben Packer;A. Ng|Computer Science|3aa1b70fdc97ae96091c5fb39cd911015ac5253e;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;55f44d39630646f36eac91358f8f27d1bead384c;36d442f59c61ea2912d227c24dee76778c546b0a;d422df8bff4e677a3077635db116679d25142bfc;a244c47a1d4a8c2894b22807df8c7eec16cc110a;611544418ca53cdad254df444addc7814abcfddc;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;6ec7c724aa1d906e9e9f81c58497adddb22175b8;e838ba98e198d2dac047736e77c50c0efa49c2dc;1626c940a64ad96a7ed53d7d6c0df63c6696956b;c43025c429b1fbf6f1379f61801a1b40834d62e7;0ca26f9a98dda0abb737692f72ffa682df14cb2f;5a391667242b4a631acdd5917681b16a86523987;4a554da55fd9ff76c99e25d2ce937b225dc1100c|7446832;2110208881;1679873|True;False;True|desc;desc;desc
d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f|10.1109/JPROC.2009.2035722|Cell|79.0|5678-9012_79|925-936|On the heels of compressed sensing, a new field has very recently emerged. This field addresses a broad range of problems of significant practical interest, namely, the recovery of a data matrix from what appears to be incomplete, and perhaps even corrupted, information. In its simplest form, the problem is to recover a matrix from a small sample of its entries. It comes up in many areas of science and engineering, including collaborative filtering, machine learning, control, remote sensing, and computer vision, to name a few. This paper surveys the novel literature on matrix completion, which shows that under some suitable conditions, one can recover an unknown low-rank matrix from a nearly minimal set of entries by solving a simple convex optimization problem, namely, nuclear-norm minimization subject to data constraints. Further, this paper introduces novel results showing that matrix completion is provably accurate even when the few observed entries are corrupted with a small amount of noise. A typical result is that one can recover an unknown matrix of low rank from just about log noisy samples with an error that is proportional to the noise level. We present numerical results that complement our quantitative analysis and show that, in practice, nuclear-norm minimization accurately fills in the many missing entries of large low-rank matrices from just a few noisy samples. Some analogies between matrix completion and compressed sensing are discussed throughout.||||Matrix Completion With Noise|2009|2006869;1800179|E. Candès;Y. Plan|Computer Science;Mathematics;Engineering|21cea8f56a0d067d640f923b2d69e18ed5542f6d;5794141889d0e994c3103b0aaab08a18222c9c43;1e41ed1ac234cba0138329047e16a8a424389e77;8a5d0579590465494c9aba58a857af43b190b6a6|31292557;3035541;1699645|False;True;True|desc;desc;desc
bd898f483476e3dcacf83cd85efc64e6319da0e1|10.1109/RBME.2009.2034865||||147-171|Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Beijing_2009|Beijing|Histopathological Image Analysis: A Review|2009|5733445;2034769;145643855;1705442;2229652;7834060|M. Gurcan;L. Boucheron;A. Can;A. Madabhushi;N. Rajpoot;B. Yener|Computer Science;Medicine|d7701e78e0bfc92b03a89582e80cfb751ac03f26;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;4f975da00a5b2a2f7236e34edcb7274e5fdab937;08b43d84e6747e370ef307e2ada50675b414514a;8de174ab5419b9d3127695405efd079808e956e8;0ba86604228b555475496e200f31878df3aabd6e;65b16da51891a6b98140d425804c8a0fd0299219;d12864a8acbab1830be755bfb9cb177e31ca5e20;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;e0535dedb8607d83cd2614317c99913378e89e26;61e27dbae190b82639c57f180ecf97e4c46fcad9;f354310098e09c1e1dc88758fca36767fd9d084d;cd49acefc8d51e324aa562e5337e1c2aff067053;872352b0a53ab6cbb4420f81df64d215d86c7d9b;2077d0f30507d51a0d3bbec4957d55e817d66a59;98c25683fc8d6446448b734b1bcf08e1457f8d85;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;06645d735b59b14479ae1d0392136bbf44227d0f|1766638;2594256;145057514|True;True;True|desc;desc;desc
d7701e78e0bfc92b03a89582e80cfb751ac03f26|10.1109/DSAA.2018.00018||||80-89|There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Moscow_2018|Moscow|Explaining Explanations: An Overview of Interpretability of Machine Learning|2018|145019478;144159726;144002190;50397921;144417360;1735243|Leilani H. Gilpin;David Bau;Ben Z. Yuan;Ayesha Bajwa;Michael A. Specter;Lalana Kagal|Computer Science;Mathematics|1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;01f702f8b1f9d1314587015f1f038af4d5735e77;1f87134a630c2dbb9a3645ba658954f00b620a77;7380e343dd4547e21d5118b16daf03d021d98c4e;222d3a63d4f81d39ea324530b57328c58f298888;076af19e50f022ccbe5bf16f413f79b5c6904c05;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;9eb715fe0347445a2d63518cbb476d345ba86233;b24972552161cd9eda729e748762a73430983e3a;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;971766088dfaf63fb55e6f0190b14f28f2c98ad0|2239580655;1878835;143809344|True;True;False|desc;desc;desc
574449170f293dfa868771e9ee0403b56a19b9e9|10.1080/00404969.2019.1653638||||212 - 217|. The article presents artistic projects in which the issue of gender is actualized. The above was carried out from the position of post-humanist ideas through the study of the impact of the development of technologies on modern socio-cultural processes. Thus, the projects created in different genres were analyzed, including post-cinema (“Night Walk for Edinburgh”, “A Total Jizzfest”) aimed at chang-ing gender perceptions in society, Ellen Pearlman’s opera “Emotionally intelligent” Artificially Intelligent Brainwave Opera” (AIBO), in which, thanks to the involvement of artificial intelligence as an actor, gender opposition is reinforced. The Female Laptop Orchestra (FLO) project is dedicated to the support of female creativity, which, thanks to the involvement of information and communication tools, com-bines music creation with research in its activity. An important aspect of the considered projects is defined as interactivity, which is im-plemented by involving the viewer in the process of their creation. It was found that, on the one hand, the digital virtual space made it possible to reduce the importance of social roles or gender affili-ation, thereby leveling the stereotype of the opposition between the author’s masculinity and femininity. On the other hand, with the development of machine learning technologies, a new opposition between the human author and the artificial intelligence in the same ca-pacity is gradually emerging.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cape_Town_2019|Cape Town|Gender|2019|144126414|A. Hood|Art|f354310098e09c1e1dc88758fca36767fd9d084d;220ac48a22547a455d05f416e1fd22bbd0b0788d;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;10f919b1a5161b560504c225cfb2d1b3a4768f80;6adf016e7531c91100d3cf4a74f5d4c87b26b528;4f975da00a5b2a2f7236e34edcb7274e5fdab937;f7d997a640f2b804676cadb8030d8b2c7bd79d85;1a827052f01ef830cbc849c71e9da99791243a5f;a3461eaf51016f9d6e85ea47173b27e019e801c4;1626c940a64ad96a7ed53d7d6c0df63c6696956b|3151995;1711979;1397181875|True;True;True|desc;desc;desc
2b7f9117eb6608a58be4c078ca3d69c0e5ccb875|10.1109/SP.2017.12||||19-38|Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Sydney_2017|Sydney|SecureML: A System for Scalable Privacy-Preserving Machine Learning|2017|1773836;2108473999|Payman Mohassel;Yupeng Zhang|Computer Science|86f0b58404a264a6216e29c78a5c113d900ca461;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;d517b13f2b152c913b81ce534a149493517dbdad;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;b3de1062d8a462dfdc2938558258f8884abe9f4e;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;dd9b99fac67c18be82d7763a8fbf231fc3512423;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b|145676637;2108434617;1792616|True;True;True|desc;desc;desc
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3|10.1145/3386252|Radiology|3.0|0123-4567_3|1 - 34|Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1||||Generalizing from a Few Examples|2019|2115793087;3259992;145193332;1726587|Yaqing Wang;Quanming Yao;J. Kwok;L. Ni|Computer Science|adc61e21eafecfbf6ebecc570f9f913659a2bfb2;5c45a5d05ac564adb67811eeb9d41d6460c70135;86f0b58404a264a6216e29c78a5c113d900ca461|2407842;46502933;34699434|True;True;True|desc;desc;desc
01f702f8b1f9d1314587015f1f038af4d5735e77|10.1109/CIMCA.2005.1631345||||695-701|Opposition-based learning as a new scheme for machine intelligence is introduced. Estimates and counter-estimates, weights and opposite weights, and actions versus counter-actions are the foundation of this new approach. Examples are provided. Possibilities for extensions of existing learning algorithms are discussed. Preliminary results are provided|AAAI|AAAI_New_York_2005|New York|Opposition-Based Learning: A New Scheme for Machine Intelligence|2005|9315255|H. Tizhoosh|Computer Science|1e41ed1ac234cba0138329047e16a8a424389e77;2a3842f6070b4554ff21fe62b2a486657d9a304a;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;07abd02f02774d178f26ca99937e5f94001a9ec9;85d727b119304dde458bcd8cf5cb87a906fb41ba;fee4db01f6f981931dfd87376a8f861353d1e494;62ccd99a65bfc7c735ae1f33b75b107665de95df;65b16da51891a6b98140d425804c8a0fd0299219;d517b13f2b152c913b81ce534a149493517dbdad;b24972552161cd9eda729e748762a73430983e3a;3fa5f45ddbd5184f10bfb92e367493c5a344f207|144766615;2152472162;2031130914|True;True;True|desc;desc;desc
b16408a97170785fb216c9e8b7920d64f478fbc8|10.1109/MGRS.2016.2540798||||22-40|Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input?output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding.In this technical tutorial, a general framework of DL for RS data is provided, and the state-of-the-art DL methods in RS are regarded as special cases of input-output data combined with various deep networks and tuning tricks. Although extensive experimental results confirm the excellent performance of the DL-based algorithms in RS big data analysis, even more exciting prospects can be expected for DL in RS. Key bottlenecks and potential directions are also indicated in this article, guiding further research into DL for RS data.|IJCAI|IJCAI_New_York_2016|New York|Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art|2016|9802604;2107901992;145728792|Liangpei Zhang;Lefei Zhang;Bo Du|Computer Science;Environmental Science|4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;2c47bd8bd699914e3535292b17ba46542800845c;e0535dedb8607d83cd2614317c99913378e89e26;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;fbf1c51548ffc9b9e538befcd71529365af23d15;2346d121f38fc19c77e0b062415519843f478163;e9126a98de0c39dcffe4c4f5158e037460196724;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;fbc913faf39b1e369dfcdcfefb354d846a46573c;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;10f919b1a5161b560504c225cfb2d1b3a4768f80;1f87134a630c2dbb9a3645ba658954f00b620a77;b3852f0113fcf8a3913c55ae92393ae6ccde347e|6486577;1711979;32131713|True;True;True|desc;desc;desc
61394599ed0aabe04b724c7ca3a778825c7e776f|10.1109/TPAMI.2015.2509974||||2096-2109|Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2016|New York|Struck: Structured Output Tracking with Kernels|2016|1837057;143777501;1741702;143729959;37535930;2445538;143635540|Sam Hare;S. Golodetz;Amir Saffari;Vibhav Vineet;Ming-Ming Cheng;S. Hicks;Philip H. S. Torr|Computer Science;Medicine|efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;78989616eeeac55b202e3e4205225e7135054185;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;9eb715fe0347445a2d63518cbb476d345ba86233;93884d89dfc8c3886f642018227a43fb7b58044f;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;8515a302b8f389f8f1008cc2650e5ec0a6913e24;1dd6c46d868accd5acffd02e4b08b003534b924e;2a3842f6070b4554ff21fe62b2a486657d9a304a;338a891907dce447da9a0fa2f27221bd35164163;e50f4d3316d13841c287dcdf5479d7820d593571;693914b7f38c19585e35668fd626aecf62d4c5e7;df40ce107a71b770c9d0354b78fdd8989da80d2f|1716301;1682008;1763713|True;True;True|desc;desc;desc
76f560991d56ad689ec32f9e9d13291e0193f4cf|10.1109/ICCV.1998.710772|NEJM|68.0|9012-3456_68|555-562|This paper presents a general trainable framework for object detection in static images of cluttered scenes. The detection technique we develop is based on a wavelet representation of an object class derived from a statistical analysis of the class instances. By learning an object class in terms of a subset of an overcomplete dictionary of wavelet basis functions, we derive a compact representation of an object class which is used as an input to a support vector machine classifier. This representation overcomes both the problem of in-class variability and provides a low false detection rate in unconstrained environments. We demonstrate the capabilities of the technique in two domains whose inherent information content differs significantly. The first system is face detection and the second is the domain of people which, in contrast to faces, vary greatly in color, texture, and patterns. Unlike previous approaches, this system learns from examples and does not rely on any a priori (hand-crafted) models or motion-based segmentation. The paper also presents a motion-based extension to enhance the performance of the detection algorithm over video sequences. The results presented here suggest that this architecture may well be quite general.||||A general framework for object detection|1998|145030811;145328018;1685292|C. Papageorgiou;Michael Oren;T. Poggio|Computer Science|4b149a326e38b9237077d794a0d5f5b4865efacf;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;21cea8f56a0d067d640f923b2d69e18ed5542f6d;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;46f74231b9afeb0c290d6d550043c55045284e5f;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;0e779fd59353a7f1f5b559b9d65fa4bfe367890c;86f0b58404a264a6216e29c78a5c113d900ca461|2743486;2291135512;2428490|True;True;True|desc;desc;desc
f3203d0bdefc9670ed508ca776d08aa9f024bafa|10.1109/tnn.1997.623228|Frontiers for Young Minds|76.0|2345-6789_76|1219-1219|Included in Prentice Hall's MATLAB Curriculum Series, this text provides a comprehensive treatment of the methodologies underlying neuro-fuzzy and soft computing. The book places equal emphasis on theoretical aspects of covered methodologies, empirical observations, and verifications of various applications in practice.||||Neuro-fuzzy And Soft Computing: A Computational Approach To Learning And Machine Intelligence [Books in Brief]|1997|144293175;2216207827|J. Jang;Chuen-Tsai Sun|Computer Science|dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63|153314745;2358911;5840101|False;True;True|desc;desc;desc
adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1|10.5555/2627435.2697059||||2231-2239|Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.|AAMAS|AAMAS_Rome_2012|Rome|Multimodal learning with deep Boltzmann machines|2012|2897313;145124475|Nitish Srivastava;R. Salakhutdinov|Computer Science;Mathematics|799f927692a6c08c5e630bea78c087c5051528fc;5a4631d5d75e3610037f87839628a4d166581e01;b57c54350769ffa59ff57f79ee5aad918844d298;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;f762cc39a824de1360e8223222739aaa4cd4168c;222d3a63d4f81d39ea324530b57328c58f298888;9f387ce140c59a44eaeeea590087351461345164;cbac8b0d82ea8e9251d5530695841d816cb196b9;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;5c45a5d05ac564adb67811eeb9d41d6460c70135|152912047;1974678;1699645|True;True;True|desc;desc;desc
01b24de15cf337c55b9866c4b534596ca3d93abe|10.1073/PNAS.0500334102||||"
          7426-31
        "|We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis.|UAI|UAI_Toronto_2005|Toronto|Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps.|2005|1780112;37805393;1832448;34207023;1786884;49818480;1698824|R. Coifman;Stéphane Lafon;Ann B. Lee;M. Maggioni;B. Nadler;F. Warner;S. Zucker|Computer Science;Medicine;Physics;Mathematics|076af19e50f022ccbe5bf16f413f79b5c6904c05;e0535dedb8607d83cd2614317c99913378e89e26;668b1277fbece28c4841eeab1c97e4ebd0079700;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;6ec7c724aa1d906e9e9f81c58497adddb22175b8;10f919b1a5161b560504c225cfb2d1b3a4768f80;01f702f8b1f9d1314587015f1f038af4d5735e77;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;b9518627db25f05930e931f56497602363a75491;1dae4d61cd74cc919ecc638bde6b7125728ea97b;24e6c5bfe9bb0751e5708b501d04e860011b2953;fbc913faf39b1e369dfcdcfefb354d846a46573c;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;88816ae492956f3004daa41357166f1181c0c1bf;f04df4e20a18358ea2f689b4c129781628ef7fc1;885af28a751553be48a25b411a5d492767d4cf65;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;d997919c30fa6711bc5c25cf8c8aea34fac27b91;5cbe278b65a81602a864184bbca37de91448a5f5|2112890973;48539382;6199470|True;True;True|desc;desc;desc
aaf9069be5a498179cbd2932d793ea1b9d0092de|10.1109/COMST.2020.2986024|Science|58.0|3456-7890_58|2031-2063|In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.||||Federated Learning in Mobile Edge Networks: A Comprehensive Survey|2019|1753717562;3046954;2233724;2878072;144791622;153096457;2266084696;1679209|Wei Yang Bryan Lim;Nguyen Cong Luong;D. Hoang;Yutao Jiao;Ying-Chang Liang;Qiang Yang;Dusist Niyato;C. Miao|Computer Science;Engineering|3b7d120c0e801ef318bc9c607a0789f175637c7f;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4|9802604;1697141;2711164|True;True;True|desc;desc;desc
8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8|10.1093/bioinformatics/btx180|JACC|59.0|1234-5678_59|2424–2426|Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.||||Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification|2017|1398461214;2129065;1680265;2504157;3032174;145844120;144924970|Ignacio Arganda-Carreras;V. Kaynig;C. Rueden;K. Eliceiri;J. Schindelin;Albert Cardona;H. Seung|Computer Science;Medicine;Biology|18d026ec5d0eebd17ee2c762da89540c0b3d7bde;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;441c31274f4535a4a50892c1ad6e19eacfd17f8c;a85e512d8845bd007b0866b4a97e8341463f8190;f04df4e20a18358ea2f689b4c129781628ef7fc1;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;05fd1da7b2e34f86ec7f010bef068717ae964332;62ccd99a65bfc7c735ae1f33b75b107665de95df;402f850dff86fb601d34b2841e6083ac0f928edd;d422df8bff4e677a3077635db116679d25142bfc;872bae24c109f7c30e052ac218b17a8b028d08a0;9071775ebcfebddd54d879fe7e6c627673e4d305;222d3a63d4f81d39ea324530b57328c58f298888;8592e46a5435d18bba70557846f47290b34c1aa5;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;d0c882bcae6531fa13e75bcc5c297b9985f207f7|1968806;2112890973;35188630|True;False;False|desc;desc;desc
2a3842f6070b4554ff21fe62b2a486657d9a304a|10.1126/science.1192788|JACC|45.0|1234-5678_45|1279 - 1285|In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?||||How to Grow a Mind: Statistics, Structure, and Abstraction|2011|2238317928;2249973698;2247786963;2250210463|J. B. Tenenbaum;Charles Kemp;Thomas L. Griffiths;Noah D. Goodman|Computer Science;Medicine;Philosophy;Psychology|2077d0f30507d51a0d3bbec4957d55e817d66a59;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;a206216c3f67605ac6e25b0178c3f156dc0f7ba0;cedea36fa3692281b3ac767335fe49a16d00957d;5c5e69387020d7ca7d49487ca841958dc5e08ce6;a86171e13f84fe32212dd7fb6a1c31a34a47155f;01b24de15cf337c55b9866c4b534596ca3d93abe;5966d7c7f60898d610812e24c64d4d57855ad86a;d02927d4de4a2a51cced4970da04b812cbee4342;d7701e78e0bfc92b03a89582e80cfb751ac03f26;398c296d0cc7f9d180f84969f8937e6d3a413796|2645384;1766638;6289332|True;False;False|desc;desc;desc
d0ab11de3077490c80a08abd0fb8827bac84c454|10.1039/c7sc02664a|Cell|42.0|5678-9012_42|513 - 530|A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.||||MoleculeNet: a benchmark for molecular machine learning|2017|9957625;2378027;5932099;145986494;2347660128;5929246;40867019;1806271|Zhenqin Wu;Bharath Ramsundar;Evan N. Feinberg;Joseph Gomes;Caleb Geniesse;Aneesh S. Pappu;K. Leswing;V. Pande|Chemistry;Medicine;Computer Science;Physics;Mathematics|43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402|1706280;146218865;2344478|True;False;True|desc;desc;desc
c2b381b24aabf237394059fed7920cd6fd0e67b8|10.1109/69.250074||||914-925|The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >|ACL|ACL_Moscow_1993|Moscow|Database Mining: A Performance Perspective|1993|144947410;1733797;31536502|R. Agrawal;T. Imielinski;A. Swami|Computer Science|e24b8a9531573d284647239affc6c855505b0de4;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;86cff4d050beb90fed2e1ceac8940c8221b120aa;e838ba98e198d2dac047736e77c50c0efa49c2dc|97874426;2110208881;49345823|True;True;True|desc;desc;desc
9f387ce140c59a44eaeeea590087351461345164|10.19026/RJASET.5.4644||||4168-4182|Recommender Systems are software tools and techniques for suggesting items to users by considering their preferences in an automated fashion. The suggestions provided are aimed at support users in various decision- making processes. Technically, recommender system has their origins in different fields such as Information Retrieval (IR), text classification, machine learning and Decision Support Systems (DSS). Recommender systems are used to address the Information Overload (IO) problem by recommending potentially interesting or useful items to users. They have proven to be worthy tools for online users to deal with the IO and have become one of the most popular and powerful tools in E-commerce. Many existing recommender systems rely on the Collaborative Filtering (CF) and have been extensively used in E-commerce .They have proven to be very effective with powerful techniques in many famous E-commerce companies. This study presents an overview of the field of recommender systems with current generation of recommendation methods and examines comprehensively CF systems with its algorithms.|NIPS|NIPS_Lisbon_2013|Lisbon|Collaborative filtering recommender systems|2013|3095698;19381012;66083398;40397208;70627370;70547150|M. Nilashi;Karamollah Bagherifard;O. Ibrahim;H. Alizadeh;L. Nojeem;Nazanin Roozegar|Computer Science|d422df8bff4e677a3077635db116679d25142bfc;5c7e5248d9eb7f373f10277410bf8506160907ea;81a4fd3004df0eb05d6c1cef96ad33d5407820df;86f0b58404a264a6216e29c78a5c113d900ca461|3289622;1780112;46266495|True;False;True|desc;desc;desc
6a775ba9287f33ba62a7a4f353dd1e0a77aa236a|10.1145/1066116.1189041|Frontiers for Young Minds|63.0|2345-6789_63|113-128|Achieving crisp interactive response in resource-intensive applications such as augmented reality, language translation, and speech recognition is a major challenge on resource-poor wearable hardware. In this paper we describe a solution based on multi-fidelity computation supported by predictive resource management. We show that such an approach can substantially reduce both the mean and the variance of response time. On a benchmark representative of augmented reality, we demonstrate a 60% reduction in mean latency and a 30% reduction in the coefficient of variation. We also show that a history-based approach to demand prediction is the key to this performance improvement: by applying simple machine learning techniques to logs of measured resource demand, we are able to accurately model resource demand as a function of fidelity.||||Predictive Resource Management for Wearable Computing|2003|2285416852;2231981905|Dushyanth Narayanan;Mahadev Satyanarayanan|Computer Science;Engineering|40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;48234756b7cf798bfeb47328f7c5d597fd4838c2;8c8215b7f8111839f0066010a530a3a9f57ba15e;884895a86fe15cb9601df4a15a1475c07f28da3c;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;12d1d070a53d4084d88a77b8b143bad51c40c38f;e0535dedb8607d83cd2614317c99913378e89e26;44c7d9fe583e3d317a619297e7e949070710799f;aaf9069be5a498179cbd2932d793ea1b9d0092de|4429495;2967405;28280741|True;True;True|desc;desc;desc
9071775ebcfebddd54d879fe7e6c627673e4d305|10.1145/3007787.3001139||||14-26|A number of recent efforts have attempted to design accelerators for popular machine learning algorithms, such as those involving convolutional and deep neural networks (CNNs and DNNs). These algorithms typically involve a large number of multiply-accumulate (dot-product) operations. A recent project, DaDianNao, adopts a near data processing approach, where a specialized neural functional unit performs all the digital arithmetic operations and receives input weights from adjacent eDRAM banks. This work explores an in-situ processing approach, where memristor crossbar arrays not only store input weights, but are also used to perform dot-product operations in an analog manner. While the use of crossbar memory as an analog dot-product engine is well known, no prior work has designed or characterized a full-fledged accelerator based on crossbars. In particular, our work makes the following contributions: (i) We design a pipelined architecture, with some crossbars dedicated for each neural network layer, and eDRAM buffers that aggregate data between pipeline stages. (ii) We define new data encoding techniques that are amenable to analog computations and that can reduce the high overheads of analog-to-digital conversion (ADC). (iii) We define the many supporting digital components required in an analog CNN accelerator and carry out a design space exploration to identify the best balance of memristor storage/compute, ADCs, and eDRAM storage on a chip. On a suite of CNN and DNN workloads, the proposed ISAAC architecture yields improvements of 14.8×, 5.5×, and 7.5× in throughput, energy, and computational density (respectively), relative to the state-of-the-art DaDianNao architecture.|IJCAI|IJCAI_Paris_2016|Paris|ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars|2016|1433007765;1783032;2358911;1777422;33102961;143707095;1761983;3052879|Ali Shafiee;Anirban Nag;N. Muralimanohar;R. Balasubramonian;J. Strachan;Miao Hu;R. S. Williams;Vivek Srikumar|Computer Science;Engineering|2ea6a93199c9227fa0c1c7de13725f918c9be3a4;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;64be9999b68e12d260ba7423f6b55ffd41552ad3;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;30b2a3422332a76663110beae4bfc4d74763f4a0;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;908cca0abefc35acc38033603714fbb1bcadc49d;72e93aa6767ee683de7f001fa72f1314e40a8f35;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;184ac0766262312ba76bbdece4e7ffad0aa8180b;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be|144189431;1422494434;144869088|True;True;False|desc;desc;desc
7b2dd79083a74699e4e0509ac3f0a8a302b4eabe|10.1090/S0273-0979-01-00923-5||||1-49|(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cairo_2001|Cairo|On the mathematical foundations of learning|2001|1755208;34911188|F. Cucker;S. Smale|Computer Science;Education;Mathematics|db68a79e59291b85e10300b79c43843b651aa195;884895a86fe15cb9601df4a15a1475c07f28da3c;d133cb102ad0f81e3fd17a7db090b28afc124c4a;a85e512d8845bd007b0866b4a97e8341463f8190;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;01f29addca4dc6f189f903cb133dea7585813a6f;ff7a293e95c0d44582b7625ee2233916f15cb361;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;a486e2839291111bb44fa1f07731ada123539f75;8515a302b8f389f8f1008cc2650e5ec0a6913e24|145967056;2053884612;1403820685|True;True;True|desc;desc;desc
09622b0c84bf812814af5b64b0c83dce796899c4|10.1145/336597.336662||||195-204|Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast,content-based methods use information about an item itself to make suggestions.This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.|ICLR|ICLR_Cairo_1999|Cairo|Content-based book recommending using learning for text categorization|1999|1797655;143747945|R. Mooney;Loriene Roy|Computer Science|6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;cc1cad12521b5aab43fdda5b4dec67586aef1f87;bf7dcbee272428a2aa3c534200743ff7ab2047f8;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2|2967405;145776090;1761983|True;True;True|desc;desc;desc
b954efe5e46b8952f5a8daf42e7e535119b5408b|10.1001/jama.2017.18152||||2211–2223|Importance A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases. Objective To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes. Design, Setting, and Participants Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes. Exposures Use of a deep learning system. Main Outcomes and Measures Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard. Results In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6% men), the prevalence of referable diabetic retinopathy was 3.0%; vision-threatening diabetic retinopathy, 0.6%; possible glaucoma, 0.1%; and AMD, 2.5%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95% CI, 0.925-0.943), sensitivity was 90.5% (95% CI, 87.3%-93.0%), and specificity was 91.6% (95% CI, 91.0%-92.2%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95% CI, 0.956-0.961), sensitivity was 100% (95% CI, 94.1%-100.0%), and specificity was 91.1% (95% CI, 90.7%-91.4%). For possible glaucoma, AUC was 0.942 (95% CI, 0.929-0.954), sensitivity was 96.4% (95% CI, 81.7%-99.9%), and specificity was 87.2% (95% CI, 86.8%-87.5%). For AMD, AUC was 0.931 (95% CI, 0.928-0.935), sensitivity was 93.2% (95% CI, 91.1%-99.8%), and specificity was 88.7% (95% CI, 88.3%-89.0%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images). Conclusions and Relevance In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.|AAMAS|AAMAS_Sydney_2017|Sydney|Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes|2017|39189579;10182207;40062372;2918013;49816694;38174487;7933218;1398980400;1398980406;2279523130;47506317;6626836;3308866;2054745187;144840717;3211619;2942112;143816239;1992820;1869515;144025987;144751024;1723044;39167827;145036533;145520804;144793401;1390489982;2279559513|D. Ting;C. Cheung;Gilbert Lim;G. Tan;N. Quang;A. Gan;Haslina Hamzah;R. García-Franco;Ian Yew San Yeo;S. Lee;E. Wong;C. Sabanayagam;M. Baskaran;Farah Ibrahim;N. Tan;E. Finkelstein;E. Lamoureux;I. Wong;N. Bressler;S. Sivaprasad;R. Varma;J. Jonas;M. He;Ching-Yu Cheng;G. Cheung;T. Aung;W. Hsu;M. Lee;T. Wong|Computer Science;Medicine|07abd02f02774d178f26ca99937e5f94001a9ec9;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7;b3de1062d8a462dfdc2938558258f8884abe9f4e;98c25683fc8d6446448b734b1bcf08e1457f8d85;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;d05d86db86a4ac0d95e6dcd951b42a9651939793;0b544dfe355a5070b60986319a3f51fb45d1348e;38f23fe236b152cd4983c8f30d305a568afd0d3e;427b168f490b56716f22b129ac93aba5425ea08f;de2be42659be5c43c1a992b5d7fe6daf14e571dd;55f44d39630646f36eac91358f8f27d1bead384c;4f975da00a5b2a2f7236e34edcb7274e5fdab937;265644f1b6740ca34bfbe9762b90b33021adde62|40975594;145905360;39682944|True;True;True|desc;desc;desc
7da323e7103245eeaed32367c46abe3f4913df86|10.1109/SURV.2008.080406||||56-76|The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Mexico_City_2008|Mexico City|A survey of techniques for internet traffic classification using machine learning|2008|1716282;145027304|Thuy T. T. Nguyen;G. Armitage|Computer Science|a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;8515a302b8f389f8f1008cc2650e5ec0a6913e24;5aefde4203ce46ea900a96835a7c59a5f50800e7;8592e46a5435d18bba70557846f47290b34c1aa5;24e6c5bfe9bb0751e5708b501d04e860011b2953;908cca0abefc35acc38033603714fbb1bcadc49d;9691f67f5075bde2fd70da0135a4a70f25ef042b;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;9e475a514f54665478aac6038c262e5a6bac5e64;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;1e41ed1ac234cba0138329047e16a8a424389e77;884895a86fe15cb9601df4a15a1475c07f28da3c;12d1d070a53d4084d88a77b8b143bad51c40c38f;2521c3d76bc439c961b7003080f4a7a661949547;d0ab11de3077490c80a08abd0fb8827bac84c454;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;49bdeb07b045dd77f0bfe2b44436608770235a23;a9763afda62e960c35c80681f805ddecbef14a92|2967405;2108094514;52216909|True;True;True|desc;desc;desc
e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da|10.1109/JSAC.2019.2904348||||1205-1221|Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.|CVPR|CVPR_Los_Angeles_2018|Los Angeles|Adaptive Federated Learning in Resource Constrained Edge Computing Systems|2018|50695457;40917131;2522394;145353889;1702283;145299837;46998035|Shiqiang Wang;Tiffany Tuor;Theodoros Salonidis;K. Leung;C. Makaya;T. He;K. Chan|Computer Science;Mathematics;Engineering|1626c940a64ad96a7ed53d7d6c0df63c6696956b;5ed59f49c1bb7de06cfa2a9467d5efb535103277;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;9257779eed46107bcdce9f4dc86298572ff466ce;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;049aca6228fb68a263369380eda6d9a4fcbdb382;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;44c7d9fe583e3d317a619297e7e949070710799f;8de174ab5419b9d3127695405efd079808e956e8;09622b0c84bf812814af5b64b0c83dce796899c4;b8ebda42e272d3617375118542d4675a0c0e501d;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;7380e343dd4547e21d5118b16daf03d021d98c4e;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746|1775384;3172075;2107901992|True;True;True|desc;desc;desc
e9126a98de0c39dcffe4c4f5158e037460196724|10.1063/1.5019779||||"
          241722
        "|Deep learning has led to a paradigm shift in artificial intelligence, including web, text, and image search, speech recognition, as well as bioinformatics, with growing impact in chemical physics. Machine learning, in general, and deep learning, in particular, are ideally suitable for representing quantum-mechanical interactions, enabling us to model nonlinear potential-energy surfaces or enhancing the exploration of chemical compound space. Here we present the deep learning architecture SchNet that is specifically designed to model atomistic systems by making use of continuous-filter convolutional layers. We demonstrate the capabilities of SchNet by accurately predicting a range of properties across chemical space for molecules and materials, where our model learns chemically plausible embeddings of atom types across the periodic table. Finally, we employ SchNet to predict potential-energy surfaces and energy-conserving force fields for molecular dynamics simulations of small molecules and perform an exemplary study on the quantum-mechanical properties of C20-fullerene that would have been infeasible with regular ab initio molecular dynamics.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Los_Angeles_2017|Los Angeles|SchNet - A deep learning architecture for molecules and materials.|2017|33075217;10667063;51005232;2462983;145034054|Kristof T. Schütt;H. Sauceda;P. Kindermans;A. Tkatchenko;K. Müller|Chemistry;Medicine;Computer Science;Materials Science;Physics|8e51d68250db5637cd6bc1de98a99396441399b2;eed9fa4483cab37eacd59db0fac4b1441431ee85;395de0bd3837fdf4b4b5e5f04835bcc69c279481;a675fe5a7d99ac6f7ff91fa084462faefe616148;8c8215b7f8111839f0066010a530a3a9f57ba15e;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;88816ae492956f3004daa41357166f1181c0c1bf;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;5794141889d0e994c3103b0aaab08a18222c9c43|1740765;2346939080;3194485|True;True;True|desc;desc;desc
de2be42659be5c43c1a992b5d7fe6daf14e571dd|10.1109/TGRS.2017.2685945|PNAS|100.0|4567-8901_100|3965-3981|Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.||||AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification|2016|39943835;49268477;144322708;2276155;145905113;2798207;9802604;7828998|Gui-Song Xia;Jingwen Hu;Fan Hu;Baoguang Shi;X. Bai;Yanfei Zhong;Liangpei Zhang;Xiaoqiang Lu|Computer Science;Environmental Science|a3461eaf51016f9d6e85ea47173b27e019e801c4;5794141889d0e994c3103b0aaab08a18222c9c43;2a3842f6070b4554ff21fe62b2a486657d9a304a;dd971c07879e1ce12b06991319528c06280eeb9b;44c7d9fe583e3d317a619297e7e949070710799f;8de174ab5419b9d3127695405efd079808e956e8;885af28a751553be48a25b411a5d492767d4cf65;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;e4a85af3f5dc41e13dc2cae9ee851953709b764e;9b539d413393047b28bb7be9b195f142aaf7a80e;c62043a7d2537bbf40a84b9913957452a47fdb83;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;53834f0ee8df731cf0e629cd594dce0afaaa3d97;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;33e46a618fdb22d46951f548d6ceeb384e7f1687;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;46f74231b9afeb0c290d6d550043c55045284e5f;88816ae492956f3004daa41357166f1181c0c1bf;23e44c7c6929bbb1ee5bc111e81e242f4835b712|2044655623;145557251;103010565|False;True;True|desc;desc;desc
0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c|10.1145/1380584.1380586|The Lancet|48.0|6789-0123_48|8:1-8:49|Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.||||Statistical machine translation|2008|2244001304;143758717|Hany Hassan;Kareem Darwish|Computer Science;Linguistics|07abd02f02774d178f26ca99937e5f94001a9ec9;f04df4e20a18358ea2f689b4c129781628ef7fc1;a486e2839291111bb44fa1f07731ada123539f75;d0c882bcae6531fa13e75bcc5c297b9985f207f7;a20bfec3c95aad003dcb45a21a220c19cca8bb66;5c5e69387020d7ca7d49487ca841958dc5e08ce6;c6bbfb4fcaecc779c899af4bb52083870f4b996a;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;65d53938a12c77e7920b8eb3a49df249c978ba3f;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;bd898f483476e3dcacf83cd85efc64e6319da0e1;877374c2913b787ee9f958f39e31c75d39ebcc15;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;5b66b1c65dcb97d1d0b18014e2e32e8522e66932|144484982;2597809;3032174|True;False;True|desc;desc;desc
36bca41eba5a7cea8d69a89ee7bc24923bc380ba|10.1145/312129.312220|Frontiers for Young Minds|94.0|2345-6789_94|155-164|Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD problems. Individually making each classification learner costsensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classifier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost, is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.||||MetaCost: a general method for making classifiers cost-sensitive|1999|1740213|Pedro M. Domingos|Computer Science|7380e343dd4547e21d5118b16daf03d021d98c4e;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;d7701e78e0bfc92b03a89582e80cfb751ac03f26;de2be42659be5c43c1a992b5d7fe6daf14e571dd;049aca6228fb68a263369380eda6d9a4fcbdb382;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;799f927692a6c08c5e630bea78c087c5051528fc;f986968735459e789890f24b6b277b0920a9725d;7bb6bdf4ed609e5e72d4206d1b308323e73dceec|2827616;1837057;37212795|True;True;True|desc;desc;desc
dd971c07879e1ce12b06991319528c06280eeb9b|10.5167/UZH-185139||||154-180|Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Berlin_2019|Berlin|Event-Based Vision: A Survey|2019|144036711;1694635;33780923;1897771;1736425;1860631;2864731;2052135690;3302681;1751586;2075371|Guillermo Gallego;T. Delbrück;G. Orchard;C. Bartolozzi;B. Taba;A. Censi;Stefan Leutenegger;A. Davison;J. Conradt;Kostas Daniilidis;D. Scaramuzza|Computer Science;Medicine;Physics;Engineering|e7e25fd534e9e024da329aea546484938df305a5;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;34f25a8704614163c4095b3ee2fc969b60de4698;2bc3644ce4de7fce5812c1455e056649a47c1bbf;22fe619996b59c09cb73be40103a123d2e328111;4f975da00a5b2a2f7236e34edcb7274e5fdab937;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;48234756b7cf798bfeb47328f7c5d597fd4838c2;c43025c429b1fbf6f1379f61801a1b40834d62e7;09622b0c84bf812814af5b64b0c83dce796899c4;d133cb102ad0f81e3fd17a7db090b28afc124c4a;d517b13f2b152c913b81ce534a149493517dbdad;467568f1777bc51a15a5100516cd4fe8de62b9ab;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;04fd278c01df1564e741b4c6e052fc1c5924ab8d|153171583;46409715;144195041|False;False;True|desc;desc;desc
cd49acefc8d51e324aa562e5337e1c2aff067053|10.1093/NSR/NWX105|BMJ|11.0|8901-2345_11|30-43|As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.||||An Overview of Multi-task Learning|2018|46867608;153096457|Yu Zhang;Qiang Yang|Computer Science|d04d6db5f0df11d0cff57ec7e15134990ac07a4f;98c25683fc8d6446448b734b1bcf08e1457f8d85;2077d0f30507d51a0d3bbec4957d55e817d66a59;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;8d1c588d202f150e1797ed113fba7e67bfa43ecb;220ac48a22547a455d05f416e1fd22bbd0b0788d;9d75cc322a4e06d0a3a868cb91b04219a289c12c;0b544dfe355a5070b60986319a3f51fb45d1348e;799f927692a6c08c5e630bea78c087c5051528fc;5a391667242b4a631acdd5917681b16a86523987;9691f67f5075bde2fd70da0135a4a70f25ef042b;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;6981ea66000e2c98f8a81f4bef05802234d986a4;9e475a514f54665478aac6038c262e5a6bac5e64;d079a2f877f554e00f71a6975435d8325987bdf5;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c|1996134;2064241030;145624000|True;False;True|desc;desc;desc
2c8ac3e1f0edeed1fbd76813e61efdc384c319c7|10.3115/1072228.1072378||||1-7|In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.|ICAPS|ICAPS_Toronto_2002|Toronto|Learning Question Classifiers|2002|2153901204;144590225|Xin Li;D. Roth|Computer Science|42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;9eb715fe0347445a2d63518cbb476d345ba86233;fbf1c51548ffc9b9e538befcd71529365af23d15;01f702f8b1f9d1314587015f1f038af4d5735e77;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;467568f1777bc51a15a5100516cd4fe8de62b9ab;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;398c296d0cc7f9d180f84969f8937e6d3a413796;075f328ef87a076151feb4d5b1f97b66aa597a90;10f919b1a5161b560504c225cfb2d1b3a4768f80|2114950020;3172075;2346691157|False;True;False|desc;desc;desc
3adcfd254b271bcc2fb7e2a62d750db17e6c2c08|10.1093/NSR/NWX106|BMJ|75.0|8901-2345_75|44-53|Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.||||A brief introduction to weakly supervised learning|2018|145624000|Zhi-Hua Zhou|Computer Science|8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;bd1f14e7531220c39fad8f86985cce7b283f035d;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;fbc913faf39b1e369dfcdcfefb354d846a46573c;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;6adf016e7531c91100d3cf4a74f5d4c87b26b528;a85e512d8845bd007b0866b4a97e8341463f8190;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;36d442f59c61ea2912d227c24dee76778c546b0a;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;d63b884d5ebc739f6e1bdf861fa9276260781404|12565475;2343019;33498709|True;True;False|desc;desc;desc
f86f1748d1b6d22870f4347fd5d65314ba800583|10.1073/pnas.1903070116||||15849 - 15854|Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Cape_Town_2018|Cape Town|Reconciling modern machine-learning practice and the classical bias–variance trade-off|2018|145520115;143724861;143791100;151213231|M. Belkin;Daniel J. Hsu;Siyuan Ma;Soumik Mandal|Computer Science;Medicine;Mathematics|cc5afe344cc7ed7acd68a28b9774ea8023a162dc;d517b13f2b152c913b81ce534a149493517dbdad;49bdeb07b045dd77f0bfe2b44436608770235a23;f986968735459e789890f24b6b277b0920a9725d;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;f7d997a640f2b804676cadb8030d8b2c7bd79d85;53834f0ee8df731cf0e629cd594dce0afaaa3d97;c62043a7d2537bbf40a84b9913957452a47fdb83;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;4f975da00a5b2a2f7236e34edcb7274e5fdab937;63861fbeb7ec41986b85965b9780b428d919919e;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;b57c54350769ffa59ff57f79ee5aad918844d298;0e779fd59353a7f1f5b559b9d65fa4bfe367890c;a7a407968c13ced804a063259d72315a43b84f29;fbf1c51548ffc9b9e538befcd71529365af23d15;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;e838ba98e198d2dac047736e77c50c0efa49c2dc;36bca41eba5a7cea8d69a89ee7bc24923bc380ba|2334455;3374545;1680265|True;True;True|desc;desc;desc
92ace17730c2173e642934d64f96d359697b7a93|10.1017/CBO9780511804779.017||||I-XXIV, 1-697|Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_Cape_Town_2012|Cape Town|Bayesian reasoning and machine learning|2012|145617808|D. Barber|Computer Science;Mathematics|4b149a326e38b9237077d794a0d5f5b4865efacf;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8;61e27dbae190b82639c57f180ecf97e4c46fcad9;98c25683fc8d6446448b734b1bcf08e1457f8d85;9257779eed46107bcdce9f4dc86298572ff466ce;7e7eb0f93c9550d7336f4bbfad5fe89604295705;d0c882bcae6531fa13e75bcc5c297b9985f207f7;34f25a8704614163c4095b3ee2fc969b60de4698;467568f1777bc51a15a5100516cd4fe8de62b9ab;31a537c48c2bf2d98f2020df5b72c413d0fea1da|35363891;1694621;1686834|True;True;True|desc;desc;desc
df2a7756382540e92895f10703cec32d50c4f316|10.1103/PhysRevLett.108.058301||||"
          058301
        "|We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.|AAAI|AAAI_New_York_2011|New York|Fast and accurate modeling of molecular atomization energies with machine learning.|2011|48041657;2462983;145034054;7847508|M. Rupp;A. Tkatchenko;K. Müller;O. A. von Lilienfeld|Chemistry;Medicine;Computer Science;Physics;Mathematics|fbc913faf39b1e369dfcdcfefb354d846a46573c;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;c6a83c4fcc99ba6753109301949c5b7cfa978079;a85e512d8845bd007b0866b4a97e8341463f8190;03cb4e2cb669d3f6344a733e622f07909f87ff0a;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;37a67228271527037c9250ae3fd220199275e42e;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;9670485f526f2254c0f34e64d9ca06f665a0bd17;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;fee4db01f6f981931dfd87376a8f861353d1e494|1718786;32163276;3472959|True;True;True|desc;desc;desc
63861fbeb7ec41986b85965b9780b428d919919e|10.1145/500141.500159|JAMA|76.0|7890-1234_76|107-118|Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.||||Support vector machine active learning for image retrieval|2001|2058177533;33794424|Simon Tong;E. Chang|Computer Science|78947497cbbffc691aac3f590d972130259af9ce;16c0ef924da1f6b510c9c783ac764156f5a3d631;d9665992ee36699b8ae4a2e2294552cd4be9003a;36652428740cd30d245d55889f01a7fb04a91c93;6ec7c724aa1d906e9e9f81c58497adddb22175b8;22adb2413901b74128f2a02584dafa77afbd8d60|153701431;2262660352;1806905|True;True;True|desc;desc;desc
033f25ad905ef2ed32a8331cf38b83953ff15922|10.1109/JPROC.2015.2483592||||11-33|Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Los_Angeles_2015|Los Angeles|A Review of Relational Machine Learning for Knowledge Graphs|2015|1729762;1702318;1700754;1718798|Maximilian Nickel;K. Murphy;Volker Tresp;E. Gabrilovich|Computer Science;Mathematics|2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;fbc913faf39b1e369dfcdcfefb354d846a46573c;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;c6bbfb4fcaecc779c899af4bb52083870f4b996a;f9d119346b0773ea83251598fa5305bc75bac8ab;825ca26af5a2a510dbc1a7b97587212bc98ae968;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;5c5e69387020d7ca7d49487ca841958dc5e08ce6;f86f1748d1b6d22870f4347fd5d65314ba800583|50483051;1685083;1707625|True;True;False|desc;desc;desc
5c7e5248d9eb7f373f10277410bf8506160907ea|10.1126/science.aat8084||||1004 - 1008|All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.|ICML|ICML_Tokyo_2018|Tokyo|All-optical machine learning using diffractive deep neural networks|2018|143746845;48156295;3913651;4901769;5602194;2146359;2660014|Xing Lin;Y. Rivenson;N. Yardimci;Muhammed Veli;Yilin Luo;M. Jarrahi;A. Ozcan|Computer Science;Medicine;Physics;Engineering|bcce96a2a074448953fc61a29a84afbdfc8db55a;4f975da00a5b2a2f7236e34edcb7274e5fdab937;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;9257779eed46107bcdce9f4dc86298572ff466ce;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;395de0bd3837fdf4b4b5e5f04835bcc69c279481;f9d119346b0773ea83251598fa5305bc75bac8ab;427b168f490b56716f22b129ac93aba5425ea08f;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;a244c47a1d4a8c2894b22807df8c7eec16cc110a;03cb4e2cb669d3f6344a733e622f07909f87ff0a;b3852f0113fcf8a3913c55ae92393ae6ccde347e;ea58af907495e97c93997119db4a59fab5cd3683;971766088dfaf63fb55e6f0190b14f28f2c98ad0|37517325;2111266186;1660855299|True;True;True|desc;desc;desc
53834f0ee8df731cf0e629cd594dce0afaaa3d97|10.1001/jama.2013.393||||"
          1351-2
        "|THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_New_York_2013|New York|The inevitable application of big data to health care.|2013|3507117;4648711|T. Murdoch;A. Detsky|Computer Science;Medicine|759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;46f74231b9afeb0c290d6d550043c55045284e5f;f4156a05a47fdeda30638e10954d3674cc056ab6;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;611544418ca53cdad254df444addc7814abcfddc;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;b10e4deadf978d8fd6eec97ff18888629f4261ab;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;18bc1d4271abe8dd6e16179cdb06524a4f396e16;f762cc39a824de1360e8223222739aaa4cd4168c|2152471960;1899753;2939803|True;True;True|desc;desc;desc
f8b7a3434f887ce4570b7e98c7f1b91c008042d4|10.1093/nar/gkx1067|Frontiers for Young Minds|40.0|2345-6789_40|D296 - D302|Abstract MicroRNAs (miRNAs) are small non-coding RNAs of ∼ 22 nucleotides that are involved in negative regulation of mRNA at the post-transcriptional level. Previously, we developed miRTarBase which provides information about experimentally validated miRNA-target interactions (MTIs). Here, we describe an updated database containing 422 517 curated MTIs from 4076 miRNAs and 23 054 target genes collected from over 8500 articles. The number of MTIs curated by strong evidence has increased ∼1.4-fold since the last update in 2016. In this updated version, target sites validated by reporter assay that are available in the literature can be downloaded. The target site sequence can extract new features for analysis via a machine learning approach which can help to evaluate the performance of miRNA-target prediction tools. Furthermore, different ways of browsing enhance user browsing specific MTIs. With these improvements, miRTarBase serves as more comprehensively annotated, experimentally validated miRNA-target interactions databases in the field of miRNA related research. miRTarBase is available at http://miRTarBase.mbc.nctu.edu.tw/.||||miRTarBase update 2018: a resource for experimentally validated microRNA-target interactions|2017|2115670;4334655;2087279;2065682157;1742112;144580498;2112109655;1393662331;37257989;2111210;1393935479;46385221;2068205528;2536315;151487590;2118105521;3207228;2451356;2184327;40995886;49180504;2107949987;47482843;2145670006;2207015236;2152662291;89103582;8243291;7889032;152912047;2110603432;2274883;35556316;143869250;144505734;2146046695|Chih-Hung Chou;S. Shrestha;Chi-Dung Yang;Nai-Wen Chang;Yu-Ling Lin;K. Liao;Wei-Chih Huang;Ting-Hsuan Sun;Siang-Jyun Tu;Wei-Hsiang Lee;Men-Yee Chiew;Chun-San Tai;Ting-Yen Wei;Tzi-Ren Tsai;Hsin-Tzu Huang;Chung-Yu Wang;Hsin‐Yi Wu;S. Ho;Pin-Rong Chen;Cheng-Hsun Chuang;Pei-Jung Hsieh;Yi-Shin Wu;Wen-Liang Chen;Mengge Li;Yu-chun Wu;Xin-Yi Huang;Fung-Ling Ng;W. Buddhakosai;P. Huang;K. Lan;Chia-Yen Huang;Shun-Long Weng;Yeong-Nan Cheng;Chao Liang;W. Hsu;Hsien-Da Huang|Computer Science;Biology;Medicine|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0|2609123;51175233;2601522|True;True;True|desc;desc;desc
1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d|10.1093/PROTEIN/GZH062||||"
          527-36
        "|We present a thorough analysis of nuclear export signals and a prediction server, which we have made publicly available. The machine learning prediction method is a significant improvement over the generally used consensus patterns. Nuclear export signals (NESs) are extremely important regulators of the subcellular location of proteins. This regulation has an impact on transcription and other nuclear processes, which are fundamental to the viability of the cell. NESs are studied in relation to cancer, the cell cycle, cell differentiation and other important aspects of molecular biology. Our conclusion from this analysis is that the most important properties of NESs are accessibility and flexibility allowing relevant proteins to interact with the signal. Furthermore, we show that not only the known hydrophobic residues are important in defining a nuclear export signals. We employ both neural networks and hidden Markov models in the prediction algorithm and verify the method on the most recently discovered NESs. The NES predictor (NetNES) is made available for general use at http://www.cbs.dtu.dk/.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Beijing_2004|Beijing|Analysis and prediction of leucine-rich nuclear export signals.|2004|2348958870;1930075;2348963575;2256698086;2348958558;2246856610|Tanja la Cour;Lars Kiemer;Anne Mølgaard;Ramneek Gupta;Karen Skriver;Søren Brunak|Computer Science;Medicine;Biology|22adb2413901b74128f2a02584dafa77afbd8d60;98c25683fc8d6446448b734b1bcf08e1457f8d85;aaf9069be5a498179cbd2932d793ea1b9d0092de;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;8de174ab5419b9d3127695405efd079808e956e8;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;78989616eeeac55b202e3e4205225e7135054185;86cff4d050beb90fed2e1ceac8940c8221b120aa;5aefde4203ce46ea900a96835a7c59a5f50800e7|1398620187;1390903788;47479574|True;True;True|desc;desc;desc
f354310098e09c1e1dc88758fca36767fd9d084d|10.1109/CVPR.2004.144|The Lancet|41.0|6789-0123_41|II-104 Vol.2|We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.||||Learning methods for generic object recognition with invariance to pose and lighting|2004|1688882;13919023;52184096|Yann LeCun;Fu Jie Huang;L. Bottou|Computer Science|21dfbc88b21b27fe8a245ab1df98edd45f655ae7;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;d422df8bff4e677a3077635db116679d25142bfc;4b149a326e38b9237077d794a0d5f5b4865efacf;184ac0766262312ba76bbdece4e7ffad0aa8180b;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63|1842532;1755208;49180504|True;True;False|desc;desc;desc
4b149a326e38b9237077d794a0d5f5b4865efacf|10.1109/TAFFC.2017.2740923||||18-31|Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1,250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.|ICLR|ICLR_London_2017|London|AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild|2017|2314025;9706655;145531712|A. Mollahosseini;Behzad Hasani;M. Mahoor|Computer Science|05fd1da7b2e34f86ec7f010bef068717ae964332;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8|1701766;1716301;1409068337|True;True;False|desc;desc;desc
8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92|10.1109/SP.2010.25||||305-316|"In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational ""real world"" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_London_2010|London|Outside the Closed World: On Using Machine Learning for Network Intrusion Detection|2010|1690799;1744800|Robin Sommer;V. Paxson|Computer Science|7da323e7103245eeaed32367c46abe3f4913df86;72e93aa6767ee683de7f001fa72f1314e40a8f35;53834f0ee8df731cf0e629cd594dce0afaaa3d97;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;df2a7756382540e92895f10703cec32d50c4f316;e838ba98e198d2dac047736e77c50c0efa49c2dc|32084575;48804181;2493778|True;True;True|desc;desc;desc
220ac48a22547a455d05f416e1fd22bbd0b0788d|10.1109/CVPR.2017.18||||95-104|Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.|ICCV|ICCV_Paris_2016|Paris|Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks|2016|2732737;2286640;35363891;1761978;1707347|Konstantinos Bousmalis;N. Silberman;David Dohan;D. Erhan;Dilip Krishnan|Computer Science|07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;e838ba98e198d2dac047736e77c50c0efa49c2dc;8de174ab5419b9d3127695405efd079808e956e8;7e7eb0f93c9550d7336f4bbfad5fe89604295705;bf7dcbee272428a2aa3c534200743ff7ab2047f8;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;f3203d0bdefc9670ed508ca776d08aa9f024bafa;a3461eaf51016f9d6e85ea47173b27e019e801c4;86cff4d050beb90fed2e1ceac8940c8221b120aa;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;06645d735b59b14479ae1d0392136bbf44227d0f;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;8db9df2eadea654f128c1887722c677c708e8a47;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;de2be42659be5c43c1a992b5d7fe6daf14e571dd;5966d7c7f60898d610812e24c64d4d57855ad86a|19225295;2075675;3349310|True;True;True|desc;desc;desc
43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402|10.2307/2008816|JAMA|47.0|7890-1234_47|I-XII, 1-272|SIMULATED ANNEALING. Combinatorial Optimization. Simulated Annealing. Asymptotic Convergence. Finite-Time Approximation. Simulated Annealing in Practice. Parallel Simulated Annealing Algorithms. BOLTZMANN MACHINES. Neural Computing. Boltzmann Machines. Combinatorial Optimization and Boltzmann Machines. Classification and Boltzmann Machines. Learning and Boltzmann Machines. Appendix. Bibliography. Indices.||||Simulated annealing and Boltzmann machines - a stochastic approach to combinatorial optimization and neural computing|1990|1796770;8952312|E. Aarts;J. Korst|Computer Science;Mathematics|adc61e21eafecfbf6ebecc570f9f913659a2bfb2;4a554da55fd9ff76c99e25d2ce937b225dc1100c;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;78989616eeeac55b202e3e4205225e7135054185;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;6aae0dc122102693e8136856ffc8b72df7f78386;5c7e5248d9eb7f373f10277410bf8506160907ea;184ac0766262312ba76bbdece4e7ffad0aa8180b;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;825ca26af5a2a510dbc1a7b97587212bc98ae968;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;58a8bead87c8c1e37460dce28285c053c270f6e7;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;049aca6228fb68a263369380eda6d9a4fcbdb382|145034054;143777501;1686834|False;True;True|desc;desc;desc
fbf1c51548ffc9b9e538befcd71529365af23d15|10.1109/TCSVT.2008.2005594||||1473-1488|"The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) ""actions"" and 2) ""activities."" ""Actions"" are characterized by simple motion patterns typically executed by a single human. ""Activities"" are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities."|IJCAI|IJCAI_New_York_2008|New York|Machine Recognition of Human Activities: A Survey|2008|143655174;9215658;1728462;2493008|P. Turaga;R. Chellappa;V. S. Subrahmanian;O. Udrea|Computer Science|e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;a9763afda62e960c35c80681f805ddecbef14a92;49bdeb07b045dd77f0bfe2b44436608770235a23;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;de2be42659be5c43c1a992b5d7fe6daf14e571dd;23e44c7c6929bbb1ee5bc111e81e242f4835b712;d133cb102ad0f81e3fd17a7db090b28afc124c4a;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;01f702f8b1f9d1314587015f1f038af4d5735e77;b954efe5e46b8952f5a8daf42e7e535119b5408b;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c|2490652;2348958870;2154435638|True;True;True|desc;desc;desc
b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b|10.1109/JRPROC.1961.287775|JACC|37.0|1234-5678_37|8-30|The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.||||Steps toward Artificial Intelligence|1995|1847175|M. Minsky|Computer Science||88740363;1712994;1706276|True;True;True|desc;desc;desc
2369db9921078c4bb76072ef7d6426e9f1dbfdb5|10.1155/2015/258619|Nature|44.0|1234-5678_44|258619:1-258619:12|Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.||||Deep Convolutional Neural Networks for Hyperspectral Image Classification|2015|145066193;7524887;2157336032;31292557;51330074|Wei Hu;Yangyu Huang;Wei Li;Fan Zhang;Hengchao Li|Computer Science;Environmental Science|a7a407968c13ced804a063259d72315a43b84f29;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;a27089efabc5f4abd5ddf2be2a409bff41f31199;1dd6c46d868accd5acffd02e4b08b003534b924e;e4a85af3f5dc41e13dc2cae9ee851953709b764e;18bc1d4271abe8dd6e16179cdb06524a4f396e16;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;45c9f19b1eb46095e61f3c1a9970a6161c13a861;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;885af28a751553be48a25b411a5d492767d4cf65;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;e24b8a9531573d284647239affc6c855505b0de4;63861fbeb7ec41986b85965b9780b428d919919e;4e6238c8613b5b81f81552939bce33296aedfbfe;f762cc39a824de1360e8223222739aaa4cd4168c;d997919c30fa6711bc5c25cf8c8aea34fac27b91;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;3def68bd0f856886d34272840a7f81588f2bc082|143857271;143774737;2562282|True;True;True|desc;desc;desc
ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc|10.1109/MVT.2019.2921208||||28-41|A key enabler for the intelligent information society of 2030, 6G networks are expected to provide performance superior to 5G and satisfy emerging services and applications. In this article, we present our vision of what 6G will be and describe usage scenarios and requirements for multi-terabyte per second (Tb/s) and intelligent 6G networks. We present a large-dimensional and autonomous network architecture that integrates space, air, ground, and underwater networks to provide ubiquitous and unlimited wireless connectivity. We also discuss artificial intelligence (AI) and machine learning [1], [2] for autonomous networks and innovative air-interface design. Finally, we identify several promising technologies for the 6G ecosystem, including terahertz (THz) communications, very-large-scale antenna arrays [i.e., supermassive (SM) multiple-input, multiple-output (MIMO)], large intelligent surfaces (LISs) and holographic beamforming (HBF), orbital angular momentum (OAM) multiplexing, laser and visible-light communications (VLC), blockchain-based spectrum sharing, quantum communications and computing, molecular communications, and the Internet of Nano-Things.|ICLR|ICLR_Toronto_2019|Toronto|6G Wireless Networks: Vision, Requirements, Architecture, and Key Technologies|2019|103010565;1749827;1730232;2263629384;144095199;2301538;144015029;145563833|Zhengquan Zhang;Yue Xiao;Zheng Ma;Ming Xiao;Z. Ding;Xianfu Lei;G. Karagiannidis;P. Fan|Computer Science;Environmental Science;Engineering|19e8869f4c29353de0d9b52542c1fe9def4cbc7d;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;305d689afb6574ffec7b01e24431d541d0ce6f5d;c62043a7d2537bbf40a84b9913957452a47fdb83;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;3df952d4a724655f7520ff95d4b2cef90fff0cae|49678544;2285968829;32920239|True;True;True|desc;desc;desc
222d3a63d4f81d39ea324530b57328c58f298888|10.1109/ICDM.2001.989541||||369-376|Previous studies propose that associative classification has high classification accuracy and strong flexibility at handling unstructured data. However, it still suffers from the huge set of mined rules and sometimes biased classification or overfitting since the classification is based on only a single high-confidence rule. The authors propose a new associative classification method, CMAR, i.e., Classification based on Multiple Association Rules. The method extends an efficient frequent pattern mining method, FP-growth, constructs a class distribution-associated FP-tree, and mines large databases efficiently. Moreover, it applies a CR-tree structure to store and retrieve mined association rules efficiently, and prunes rules effectively based on confidence, correlation and database coverage. The classification is performed based on a weighted /spl chi//sup 2/ analysis using multiple strong association rules. Our extensive experiments on 26 databases from the UCI machine learning database repository show that CMAR is consistent, highly effective at classification of various kinds of databases and has better average classification accuracy in comparison with CBA and C4.5. Moreover, our performance study shows that the method is highly efficient and scalable in comparison with other reported associative classification methods.|ICCV|ICCV_Paris_2001|Paris|CMAR: accurate and efficient classification based on multiple class-association rules|2001|2108611657;145325584;145525190|Wenmin Li;Jiawei Han;J. Pei|Computer Science|9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;35b3233e521f1e9a34837c30be1957858f8f35fe;b9518627db25f05930e931f56497602363a75491;f94455176857303605ad423599385a2341c568eb;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;908cca0abefc35acc38033603714fbb1bcadc49d;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;a20bfec3c95aad003dcb45a21a220c19cca8bb66;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;cbac8b0d82ea8e9251d5530695841d816cb196b9;12439a6ff384e95ee2262ee982bc055534e30487;e7e25fd534e9e024da329aea546484938df305a5;8de174ab5419b9d3127695405efd079808e956e8;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;e24b8a9531573d284647239affc6c855505b0de4|35099951;1714612;2191754156|False;True;True|desc;desc;desc
7380e343dd4547e21d5118b16daf03d021d98c4e|10.1109/ICCV.2017.371||||3449-3457|As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.|IROS|IROS_Berlin_2017|Berlin|Interpretable Explanations of Black Boxes by Meaningful Perturbation|2017|25576460;1687524|Ruth C. Fong;A. Vedaldi|Computer Science;Mathematics|5b66b1c65dcb97d1d0b18014e2e32e8522e66932;2bc3644ce4de7fce5812c1455e056649a47c1bbf;1626c940a64ad96a7ed53d7d6c0df63c6696956b;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;872bae24c109f7c30e052ac218b17a8b028d08a0;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;0090023afc66cd2741568599057f4e82b566137c;1051280d2b825c04f27d231aba0f8284bb297880;5c45a5d05ac564adb67811eeb9d41d6460c70135;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd|2250210463;1733797;1751586|False;True;True|desc;desc;desc
afa778ba0ba6333e25671cfb691a4bdda13b2868|10.1109/TIFS.2020.2988575|Nature|8.0|1234-5678_8|3454-3469|"Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients’ private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients’ side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula>-client random scheduling strategy, where <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula> (<inline-formula> <tex-math notation=""LaTeX"">$1\leq K< N$ </tex-math></inline-formula>) clients are randomly selected from the <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula>-client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula> that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels."||||Federated Learning With Differential Privacy: Algorithms and Performance Analysis|2019|145158805;145409858;145573463;144496518;8896870;2083526329;145824256;1718541;145967056|Kang Wei;Jun Li;Ming Ding;Chuan Ma;H. Yang;Farokhi Farhad;Shi Jin;Tony Q. S. Quek;H. Poor|Computer Science;Mathematics|44c7d9fe583e3d317a619297e7e949070710799f;a85e512d8845bd007b0866b4a97e8341463f8190;b3de1062d8a462dfdc2938558258f8884abe9f4e;dd9b99fac67c18be82d7763a8fbf231fc3512423;22fe619996b59c09cb73be40103a123d2e328111;c6bbfb4fcaecc779c899af4bb52083870f4b996a;b57c54350769ffa59ff57f79ee5aad918844d298;e7e25fd534e9e024da329aea546484938df305a5|1996134;2373318;152290618|False;True;False|desc;desc;desc
36d442f59c61ea2912d227c24dee76778c546b0a|10.1109/JPROC.2018.2820126|The Lancet|5.0|6789-0123_5|808-828|Research in graph signal processing (GSP) aims to develop tools for processing data defined on irregular graph domains. In this paper, we first provide an overview of core ideas in GSP and their connection to conventional digital signal processing, along with a brief historical perspective to highlight how concepts recently developed in GSP build on top of prior research in other areas. We then summarize recent advances in developing basic GSP tools, including methods for sampling, filtering, or graph learning. Next, we review progress in several application areas using GSP, including processing and analysis of sensor network data, biological data, and applications to image processing and machine learning.||||Graph Signal Processing: Overview, Challenges, and Applications|2017|145029825;1703189;2442915;51283515;1697397|Antonio Ortega;P. Frossard;J. Kovacevic;J. Moura;P. Vandergheynst|Computer Science;Environmental Science;Engineering|305d689afb6574ffec7b01e24431d541d0ce6f5d;9eb715fe0347445a2d63518cbb476d345ba86233;1c00df1cb85fa7886b6666599eab59f2b301dd5d;09622b0c84bf812814af5b64b0c83dce796899c4|145295514;1908796;46691607|True;True;True|desc;desc;desc
872bae24c109f7c30e052ac218b17a8b028d08a0|10.1162/NECO_a_00142||||1661-1674|Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_London_2011|London|A Connection Between Score Matching and Denoising Autoencoders|2011|145467703|Pascal Vincent|Computer Science;Medicine;Mathematics|0165568bcc1a819c18564567f2ec15d859be2519;6adf016e7531c91100d3cf4a74f5d4c87b26b528;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;dd971c07879e1ce12b06991319528c06280eeb9b;63861fbeb7ec41986b85965b9780b428d919919e;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;c43025c429b1fbf6f1379f61801a1b40834d62e7;49bdeb07b045dd77f0bfe2b44436608770235a23;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9|50736254;50055322;1687524|True;True;True|desc;desc;desc
04fd278c01df1564e741b4c6e052fc1c5924ab8d|10.1109/TIE.2017.2774777||||5990-5998|Fault diagnosis is vital in manufacturing system, since early detections on the emerging problem can save invaluable time and cost. With the development of smart manufacturing, the data-driven fault diagnosis becomes a hot topic. However, the traditional data-driven fault diagnosis methods rely on the features extracted by experts. The feature extraction process is an exhausted work and greatly impacts the final result. Deep learning (DL) provides an effective way to extract the features of raw data automatically. Convolutional neural network (CNN) is an effective DL method. In this study, a new CNN based on LeNet-5 is proposed for fault diagnosis. Through a conversion method converting signals into two-dimensional (2-D) images, the proposed method can extract the features of the converted 2-D images and eliminate the effect of handcrafted features. The proposed method which is tested on three famous datasets, including motor bearing dataset, self-priming centrifugal pump dataset, and axial piston hydraulic pump dataset, has achieved prediction accuracy of 99.79%, 99.481%, and 100%, respectively. The results have been compared with other DL and traditional methods, including adaptive deep CNN, sparse filter, deep belief network, and support vector machine. The comparisons show that the proposed CNN-based data-driven fault diagnosis method has achieved significant improvements.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Toronto_2018|Toronto|A New Convolutional Neural Network-Based Data-Driven Fault Diagnosis Method|2018|7564853;1574064065;145276267;2108307007|Long Wen;Xinyu Li;Liang Gao;Yuyan Zhang|Computer Science;Engineering|18d026ec5d0eebd17ee2c762da89540c0b3d7bde;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;a86171e13f84fe32212dd7fb6a1c31a34a47155f;2077d0f30507d51a0d3bbec4957d55e817d66a59|1728462;2238317928;3156886|True;False;True|desc;desc;desc
ec76f55da5c6df30f6e4c9e4945bd3304d508ef7|10.1109/72.991432|Radiology|8.0|0123-4567_8|"
          464-71
        "|A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).||||Fuzzy support vector machines|2002|2146245769;9437199|Chun-fu Lin;Sheng-de Wang|Computer Science;Medicine;Mathematics|574449170f293dfa868771e9ee0403b56a19b9e9;a40f97770296c7fca2e5361cbceba3f4aae399e0;9b0dd87208a03e78105491e3727213b9b8ac0419;bf7dcbee272428a2aa3c534200743ff7ab2047f8|1895356;49943757;2034769|True;True;True|desc;desc;desc
b57c54350769ffa59ff57f79ee5aad918844d298|10.5555/1953048.2021036|Nature|19.0|1234-5678_19|"
          1069-1109
        "|Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.||||Differentially Private Empirical Risk Minimization|2009|38120884;1806678;9208982|Kamalika Chaudhuri;C. Monteleoni;A. Sarwate|Computer Science;Medicine;Mathematics|1dae4d61cd74cc919ecc638bde6b7125728ea97b;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;cc1cad12521b5aab43fdda5b4dec67586aef1f87|2309291535;34207023;1874286|True;True;True|desc;desc;desc
9d3e0fce253a4ae4a4456b2f24c03329a2b74621|10.1109/JBHI.2016.2636665||||4-21|With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.|AAAI|AAAI_Paris_2017|Paris|Deep Learning for Health Informatics|2017|2347855602;1905807;2775904;3163767;1443783456;1745644;144574968|Daniele Ravì;Charence Wong;F. Deligianni;M. Berthelot;Javier Andreu-Perez;Benny P. L. Lo;Guang-Zhong Yang|Computer Science;Medicine|6df11b0bb0244d4d36e8955436067cc5d19734fa;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;afa778ba0ba6333e25671cfb691a4bdda13b2868;e838ba98e198d2dac047736e77c50c0efa49c2dc;d7701e78e0bfc92b03a89582e80cfb751ac03f26;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;1dd6c46d868accd5acffd02e4b08b003534b924e;2521c3d76bc439c961b7003080f4a7a661949547|1838644;145603848;46215055|True;True;True|desc;desc;desc
1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435|10.1198/016214505000000907|NEJM|77.0|9012-3456_77|138 - 156|Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0–1 loss function. The convexity makes these algorithms computationally efficient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0–1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function—that it satisfies a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise, and show that in this case, strictly convex loss functions lead to faster rates of convergence of the risk than would be implied by standard uniform convergence arguments. Finally, we present applications of our results to the estimation of convergence rates in function classes that are scaled convex hulls of a finite-dimensional base class, with a variety of commonly used loss functions.||||Convexity, Classification, and Risk Bounds|2006|1745169;1694621;40411909|P. Bartlett;Michael I. Jordan;Jon D. McAuliffe|Computer Science;Mathematics|e838ba98e198d2dac047736e77c50c0efa49c2dc;668b1277fbece28c4841eeab1c97e4ebd0079700;92ace17730c2173e642934d64f96d359697b7a93;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;d12864a8acbab1830be755bfb9cb177e31ca5e20;6adf016e7531c91100d3cf4a74f5d4c87b26b528;d0ab11de3077490c80a08abd0fb8827bac84c454;e50f4d3316d13841c287dcdf5479d7820d593571;220ac48a22547a455d05f416e1fd22bbd0b0788d;91c380406f5a862b5937e70e720802e5c787968d;9e475a514f54665478aac6038c262e5a6bac5e64;48e752c719d33ff55b3b3bec3538727f8ce69399;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;0e90a73f03902cbe915af1aff54ea7f0b3373680;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7|1806678;8243291;2115667193|True;True;True|desc;desc;desc
3b7d120c0e801ef318bc9c607a0789f175637c7f|10.1109/FG.2018.00019||||59-66|Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes.|ICRA|ICRA_Tokyo_2018|Tokyo|OpenFace 2.0: Facial Behavior Analysis Toolkit|2018|1756344;144802290;144529448;49933077|T. Baltrušaitis;Amir Zadeh;Y. Lim;Louis-Philippe Morency|Computer Science|546785490ac417be1f83ced6a8272e934934f411;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b|1908796;37805393;2983295|True;True;False|desc;desc;desc
30b2a3422332a76663110beae4bfc4d74763f4a0|10.7551/mitpress/1120.003.0092||||681-687|This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.|ACL|ACL_Rio_de_Janeiro_2001|Rio de Janeiro|A kernel method for multi-labelled classification|2001|1766703;145183709|A. Elisseeff;J. Weston|Computer Science;Biology||1685187;35965227;2109675545|True;True;True|desc;desc;desc
771479c18b586eafae21baf262a220aaa7b2eef6|10.1109/TGRS.2016.2601622||||7405-7415|Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.|AAAI|AAAI_Sydney_2016|Sydney|Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images|2016|2152127024;2895041;7181955|Gong Cheng;Peicheng Zhou;Junwei Han|Computer Science;Environmental Science;Engineering|6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;fbf1c51548ffc9b9e538befcd71529365af23d15;48e752c719d33ff55b3b3bec3538727f8ce69399;872bae24c109f7c30e052ac218b17a8b028d08a0;2077d0f30507d51a0d3bbec4957d55e817d66a59;9d75cc322a4e06d0a3a868cb91b04219a289c12c;dd971c07879e1ce12b06991319528c06280eeb9b;049aca6228fb68a263369380eda6d9a4fcbdb382;a86171e13f84fe32212dd7fb6a1c31a34a47155f;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;9071775ebcfebddd54d879fe7e6c627673e4d305;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;45557cc70cd6989ab6b03e5aeb787e34299099f7;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;61e27dbae190b82639c57f180ecf97e4c46fcad9;b9518627db25f05930e931f56497602363a75491|1768624;2116783356;3026399|True;False;False|desc;desc;desc
8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f|10.1126/science.aaw4741|JACC|1.0|1234-5678_1|1026 - 1030|Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.||||Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations|2020|145401977;37412357;1720124|M. Raissi;A. Yazdani;G. Karniadakis|Computer Science;Medicine;Physics;Engineering|a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;d422df8bff4e677a3077635db116679d25142bfc;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;65d53938a12c77e7920b8eb3a49df249c978ba3f;075f328ef87a076151feb4d5b1f97b66aa597a90;9d75cc322a4e06d0a3a868cb91b04219a289c12c;36652428740cd30d245d55889f01a7fb04a91c93;908cca0abefc35acc38033603714fbb1bcadc49d;d516daff247f7157fccde6649ace91d969cd1973;09622b0c84bf812814af5b64b0c83dce796899c4;1696cbf7da0ee845c50591843993e6605adec177;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;a675fe5a7d99ac6f7ff91fa084462faefe616148;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;4b149a326e38b9237077d794a0d5f5b4865efacf;f354310098e09c1e1dc88758fca36767fd9d084d;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;98c25683fc8d6446448b734b1bcf08e1457f8d85;c43025c429b1fbf6f1379f61801a1b40834d62e7|1715863;3052879;90583296|False;False;True|desc;desc;desc
fbc913faf39b1e369dfcdcfefb354d846a46573c|10.1198/jasa.2003.s269||||489 - 489|"From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Buenos_Aires_2001|Buenos Aires|Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond|2001|2285968829|Christopher K. I Williams|Computer Science;Mathematics|45c9f19b1eb46095e61f3c1a9970a6161c13a861;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;44c7d9fe583e3d317a619297e7e949070710799f;df2a7756382540e92895f10703cec32d50c4f316;a244c47a1d4a8c2894b22807df8c7eec16cc110a;d079a2f877f554e00f71a6975435d8325987bdf5|39743720;34911188;2464550|True;True;True|desc;desc;desc
e24b8a9531573d284647239affc6c855505b0de4|10.1145/2046684.2046692||||43-58|In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2019|New York|Adversarial machine learning|2019|50055322;1687701;39743720;2064046776;1787610|Ling Huang;A. Joseph;B. Nelson;Benjamin I. P. Rubinstein;J. D. Tygar|Computer Science|a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;e9126a98de0c39dcffe4c4f5158e037460196724;427b168f490b56716f22b129ac93aba5425ea08f;f4156a05a47fdeda30638e10954d3674cc056ab6;4e6238c8613b5b81f81552939bce33296aedfbfe;bb144c04b9eb44579b19d21c3d5954401408440b;a9763afda62e960c35c80681f805ddecbef14a92;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;f762cc39a824de1360e8223222739aaa4cd4168c;1592fe924114866c1ac559bae33ea789930daa98;1e41ed1ac234cba0138329047e16a8a424389e77|39714312;49811299;2055574299|True;True;True|desc;desc;desc
cedea36fa3692281b3ac767335fe49a16d00957d|10.1111/AJPS.12103||||1064-1082|Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Buenos_Aires_2014|Buenos Aires|Structural Topic Models for Open‐Ended Survey Responses|2014|2464550;28924497;3252940;2061550766;1403360325;13901928;50483051;2157480|Margaret E. Roberts;Brandon M Stewart;D. Tingley;Christopher Lucas;Jetson Leder-Luis;S. Gadarian;B. Albertson;David G. Rand|Computer Science|8de174ab5419b9d3127695405efd079808e956e8|47457030;38676513;145836900|True;True;True|desc;desc;desc
1a827052f01ef830cbc849c71e9da99791243a5f|10.1002/cpbi.11||||14.10.1 - 14.10.91|MetaboAnalyst (http://www.metaboanalyst.ca) is a comprehensive Web application for metabolomic data analysis and interpretation. MetaboAnalyst handles most of the common metabolomic data types from most kinds of metabolomics platforms (MS and NMR) for most kinds of metabolomics experiments (targeted, untargeted, quantitative). In addition to providing a variety of data processing and normalization procedures, MetaboAnalyst also supports a number of data analysis and data visualization tasks using a range of univariate, multivariate methods such as PCA (principal component analysis), PLS‐DA (partial least squares discriminant analysis), heatmap clustering and machine learning methods. MetaboAnalyst also offers a variety of tools for metabolomic data interpretation including MSEA (metabolite set enrichment analysis), MetPA (metabolite pathway analysis), and biomarker selection via ROC (receiver operating characteristic) curve analysis, as well as time series and power analysis. This unit provides an overview of the main functional modules and the general workflow of the latest version of MetaboAnalyst (MetaboAnalyst 3.0), followed by eight detailed protocols. © 2016 by John Wiley & Sons, Inc.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Athens_2016|Athens|Using MetaboAnalyst 3.0 for Comprehensive Metabolomics Data Analysis|2016|144545434;2066145|J. Xia;D. Wishart|Chemistry;Computer Science;Medicine|441c31274f4535a4a50892c1ad6e19eacfd17f8c;3cee40494377c0e7d9c7c23a3811b481e55bce39;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;fbf1c51548ffc9b9e538befcd71529365af23d15;6aae0dc122102693e8136856ffc8b72df7f78386;872bae24c109f7c30e052ac218b17a8b028d08a0;65b16da51891a6b98140d425804c8a0fd0299219;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;93884d89dfc8c3886f642018227a43fb7b58044f;ea58af907495e97c93997119db4a59fab5cd3683;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;a85e512d8845bd007b0866b4a97e8341463f8190;1f87134a630c2dbb9a3645ba658954f00b620a77;61394599ed0aabe04b724c7ca3a778825c7e776f;d02927d4de4a2a51cced4970da04b812cbee4342;467568f1777bc51a15a5100516cd4fe8de62b9ab|32516743;47909642;1998820|True;False;True|desc;desc;desc
da048cdf883f2ac0551162cb1abd7e6d09e8e86a|10.1109/TPAMI.2009.187||||569-575|In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the k-fold cross-validation classification error estimator (k-cv). Our main contribution is a novel theoretical decomposition of the variance of the k-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of k. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of k-fold cross validation.|AAMAS|AAMAS_Tokyo_2010|Tokyo|Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation|2010|2116899260;2110404467;144762651|J. D. Rodríguez;Aritz Pérez Martínez;J. A. Lozano|Computer Science;Medicine;Mathematics|a486e2839291111bb44fa1f07731ada123539f75;1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435;bd1f14e7531220c39fad8f86985cce7b283f035d;872bae24c109f7c30e052ac218b17a8b028d08a0;222d3a63d4f81d39ea324530b57328c58f298888;9b0dd87208a03e78105491e3727213b9b8ac0419;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;fbc913faf39b1e369dfcdcfefb354d846a46573c;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;d517b13f2b152c913b81ce534a149493517dbdad;dd9b99fac67c18be82d7763a8fbf231fc3512423;a675fe5a7d99ac6f7ff91fa084462faefe616148;971766088dfaf63fb55e6f0190b14f28f2c98ad0;4f71ab367eb37cfd145d41327f7bb14077e5e7c5;36d442f59c61ea2912d227c24dee76778c546b0a|2300131692;4990733;144195041|True;True;True|desc;desc;desc
30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70|10.1109/SP.2019.00029|PNAS|27.0|4567-8901_27|691-706|Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.||||Exploiting Unintended Feature Leakage in Collaborative Learning|2018|145557680;3469125;1728207;1723945|Luca Melis;Congzheng Song;Emiliano De Cristofaro;Vitaly Shmatikov|Computer Science|e7e25fd534e9e024da329aea546484938df305a5;f3203d0bdefc9670ed508ca776d08aa9f024bafa;a20bfec3c95aad003dcb45a21a220c19cca8bb66;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;5794141889d0e994c3103b0aaab08a18222c9c43;0ba86604228b555475496e200f31878df3aabd6e;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;9d7c04de906823a60d3ccb5f510fd0029af5c8b0|144535526;6248858;49785210|True;False;False|desc;desc;desc
8592e46a5435d18bba70557846f47290b34c1aa5|10.7551/mitpress/3349.003.0005||||282-317|This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_London_1986|London|Learning and relearning in Boltzmann machines|1986|1695689;1714528|Geoffrey E. Hinton;T. Sejnowski|Computer Science;Physics;Mathematics|4b149a326e38b9237077d794a0d5f5b4865efacf;cbac8b0d82ea8e9251d5530695841d816cb196b9|1686834;1404459229;2309030|False;True;True|desc;desc;desc
0ca26f9a98dda0abb737692f72ffa682df14cb2f|10.1109/TSP.2004.831016|PNAS|90.0|4567-8901_90|2153-2164|Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.||||Sparse Bayesian learning for basis selection|2004|2242717;144876925|D. Wipf;B. Rao|Computer Science;Mathematics|799f927692a6c08c5e630bea78c087c5051528fc;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;a675fe5a7d99ac6f7ff91fa084462faefe616148;ac12c9b9e35e58b55d85a97c47886a7371c14afa;a486e2839291111bb44fa1f07731ada123539f75;2346d121f38fc19c77e0b062415519843f478163;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c|31536502;1409971380;47909642|True;True;True|desc;desc;desc
07abd02f02774d178f26ca99937e5f94001a9ec9|10.1145/1458082.1458150|Radiology|61.0|0123-4567_61|509-518|"This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75%. This performance is constant whether the system is evaluated on Wikipedia articles or ""real world"" documents.
 This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics."||||Learning to link with wikipedia|2008|1972431;9419406|David N. Milne;I. Witten|Computer Science|dd9b99fac67c18be82d7763a8fbf231fc3512423;cd49acefc8d51e324aa562e5337e1c2aff067053;5e095981ebf4d389e9356bd56e59e0ade1b42e88;5a391667242b4a631acdd5917681b16a86523987;075f328ef87a076151feb4d5b1f97b66aa597a90;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;65b16da51891a6b98140d425804c8a0fd0299219;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;d133cb102ad0f81e3fd17a7db090b28afc124c4a;611544418ca53cdad254df444addc7814abcfddc;4a554da55fd9ff76c99e25d2ce937b225dc1100c|1760871;145920814;1685083|True;True;True|desc;desc;desc
9d75cc322a4e06d0a3a868cb91b04219a289c12c|10.1257/JEP.31.2.87|Radiology|91.0|0123-4567_91|87-106|Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.||||Machine Learning: An Applied Econometric Approach|2017|2062143;47281276|S. Mullainathan;Jann Spiess|Computer Science;Economics|f86f1748d1b6d22870f4347fd5d65314ba800583;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;a9763afda62e960c35c80681f805ddecbef14a92;222d3a63d4f81d39ea324530b57328c58f298888|7175017;2855690;2743486|False;True;True|desc;desc;desc
3cee40494377c0e7d9c7c23a3811b481e55bce39|10.1093/nar/gkq275||||e132 - e132|We describe an algorithm for gene identification in DNA sequences derived from shotgun sequencing of microbial communities. Accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters. While several machine learning approaches could be proposed to bypass this difficulty, one effective method is to estimate parameters from dependencies, formed in evolution, between frequencies of oligonucleotides in protein-coding regions and genome nucleotide composition. Original version of the method was proposed in 1999 and has been used since for (i) reconstructing codon frequency vector needed for gene finding in viral genomes and (ii) initializing parameters of self-training gene finding algorithms. With advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies, as well as by separating models for bacteria and archaea. These advances have increased the accuracy of model reconstruction and, subsequently, gene prediction. We describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences. Also, we show that as a result of application of the new method, several thousands of new genes could be added to existing annotations of several human and mouse gut metagenomes.|ICML|ICML_Beijing_2010|Beijing|Ab initio gene identification in metagenomic sequences|2010|2113953;3194485;2715148|Wenhan Zhu;A. Lomsadze;M. Borodovsky|Computer Science;Biology;Medicine;Environmental Science|2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;de2be42659be5c43c1a992b5d7fe6daf14e571dd;9b539d413393047b28bb7be9b195f142aaf7a80e;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016|46888415;2924473;8735139|True;True;True|desc;desc;desc
4f975da00a5b2a2f7236e34edcb7274e5fdab937|10.1126/science.aaf7894||||790 - 794|Measuring consumption and wealth remotely Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary. Science, this issue p. 790; see also p. 753 Satellites collect data that can be used to measure income and wealth. Reliable data on economic livelihoods remain scarce in the developing world, hampering efforts to study these outcomes and to design policies that improve them. Here we demonstrate an accurate, inexpensive, and scalable method for estimating consumption expenditure and asset wealth from high-resolution satellite imagery. Using survey and satellite data from five African countries—Nigeria, Tanzania, Uganda, Malawi, and Rwanda—we show how a convolutional neural network can be trained to identify image features that can explain up to 75% of the variation in local-level economic outcomes. Our method, which requires only publicly available data, could transform efforts to track and target poverty in developing countries. It also demonstrates how powerful machine learning techniques can be applied in a setting with limited training data, suggesting broad potential application across many scientific domains.|CVPR|CVPR_Cairo_2016|Cairo|Combining satellite imagery and machine learning to predict poverty|2016|2752609;49240687;46215055;120334004;2465182;2490652|Neal Jean;M. Burke;Sang Michael Xie;W. Davis;D. Lobell;Stefano Ermon|Computer Science;Medicine;Environmental Science;Economics|825ca26af5a2a510dbc1a7b97587212bc98ae968;1592fe924114866c1ac559bae33ea789930daa98;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;693914b7f38c19585e35668fd626aecf62d4c5e7;771ca13f78a6cfda9ed99004a386e9e7e187bd34;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;f94455176857303605ad423599385a2341c568eb;4b149a326e38b9237077d794a0d5f5b4865efacf;5a4631d5d75e3610037f87839628a4d166581e01;222d3a63d4f81d39ea324530b57328c58f298888;24e6c5bfe9bb0751e5708b501d04e860011b2953;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;1904d633fca15140e35d893637232803b6dde6d9;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;72e93aa6767ee683de7f001fa72f1314e40a8f35;22fe619996b59c09cb73be40103a123d2e328111;d12864a8acbab1830be755bfb9cb177e31ca5e20;9b539d413393047b28bb7be9b195f142aaf7a80e;0b544dfe355a5070b60986319a3f51fb45d1348e;d133cb102ad0f81e3fd17a7db090b28afc124c4a|1899753;2117912702;2878072|True;True;True|desc;desc;desc
d997919c30fa6711bc5c25cf8c8aea34fac27b91|10.1109/WACV.2016.7477553|Science|92.0|3456-7890_92|1-10|Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.||||OpenFace: An open source facial behavior analysis toolkit|2016|1756344;2149814967;49933077|T. Baltrušaitis;P. Robinson;Louis-Philippe Morency|Computer Science|4f975da00a5b2a2f7236e34edcb7274e5fdab937;8de174ab5419b9d3127695405efd079808e956e8;36bca41eba5a7cea8d69a89ee7bc24923bc380ba|2334455;2152127024;153850291|True;True;True|desc;desc;desc
d133cb102ad0f81e3fd17a7db090b28afc124c4a|10.1103/PhysRevLett.120.145301||||"
          145301
        "|The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_2017|Lisbon|Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.|2017|49902007;2721737|T. Xie;J. Grossman|Chemistry;Medicine;Computer Science;Materials Science;Physics|a86171e13f84fe32212dd7fb6a1c31a34a47155f;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;30b2a3422332a76663110beae4bfc4d74763f4a0;ec6200bdcc23b79a71555962cde50306c4029f1a;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;799f927692a6c08c5e630bea78c087c5051528fc;427b168f490b56716f22b129ac93aba5425ea08f;0ca26f9a98dda0abb737692f72ffa682df14cb2f;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;cedea36fa3692281b3ac767335fe49a16d00957d;395de0bd3837fdf4b4b5e5f04835bcc69c279481|145617808;143655174;1682174|True;True;True|desc;desc;desc
a85e512d8845bd007b0866b4a97e8341463f8190|10.1109/TPAMI.2014.2321376|NEJM|25.0|9012-3456_25|2227-2240|For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.||||Scalable Nearest Neighbor Algorithms for High Dimensional Data|2014|2658890;35238678|Marius Muja;D. Lowe|Computer Science;Medicine;Mathematics|d517b13f2b152c913b81ce534a149493517dbdad;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;55f44d39630646f36eac91358f8f27d1bead384c;f86f1748d1b6d22870f4347fd5d65314ba800583;88816ae492956f3004daa41357166f1181c0c1bf;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;033f25ad905ef2ed32a8331cf38b83953ff15922;b24972552161cd9eda729e748762a73430983e3a|49902007;35692225;2220290014|True;True;True|desc;desc;desc
d9665992ee36699b8ae4a2e2294552cd4be9003a|10.1214/SS/1042727940|Frontiers for Young Minds|67.0|2345-6789_67|313-314|Fraud is increasing dramatically with the expansion of modem technology and the global superhighways of communication, resulting in the loss of billions of dollars worldwide each year. Although prevention technologies are the best way to reduce fraud, fraudsters are adaptive and, given time, will usually find ways to circumvent such measures. Methodologies for the detection of fraud are essential if we are to catch fraudsters once fraud prevention has failed. Statistics and machine learning provide effective technologies for fraud detection and have been applied successfully to detect activities such as money laundering, e-commerce credit card fraud, telecommunications fraud and computer intrusion, to name but a few. We describe the tools available for statistical fraud detection and the areas in which fraud detection technologies are most used.||||Statistical fraud detection: A review|2002|3002426;1781982|R. J. Bolton;D. Hand|Computer Science;Business;Mathematics;Engineering|cbac8b0d82ea8e9251d5530695841d816cb196b9;f9d119346b0773ea83251598fa5305bc75bac8ab;0e90a73f03902cbe915af1aff54ea7f0b3373680;0e779fd59353a7f1f5b559b9d65fa4bfe367890c;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;16c0ef924da1f6b510c9c783ac764156f5a3d631;1626c940a64ad96a7ed53d7d6c0df63c6696956b;22fe619996b59c09cb73be40103a123d2e328111;4c75b748911ddcd888c5122f7672f69caa5d661f|1752096;2300368826;2072589474|True;True;True|desc;desc;desc
b9518627db25f05930e931f56497602363a75491|10.1073/pnas.1900654116||||22071 - 22080|Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.|ECCV|ECCV_Berlin_2019|Berlin|Definitions, methods, and applications in interpretable machine learning|2019|144585578;145229121;19225295;1405625449;2116415778|W. James Murdoch;Chandan Singh;Karl Kumbier;R. Abbasi-Asl;Bin Yu|Computer Science;Medicine;Mathematics|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;b16408a97170785fb216c9e8b7920d64f478fbc8;d422df8bff4e677a3077635db116679d25142bfc;2346d121f38fc19c77e0b062415519843f478163;6ec7c724aa1d906e9e9f81c58497adddb22175b8;222d3a63d4f81d39ea324530b57328c58f298888;ea58af907495e97c93997119db4a59fab5cd3683;86f0b58404a264a6216e29c78a5c113d900ca461;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;a85e512d8845bd007b0866b4a97e8341463f8190;9691f67f5075bde2fd70da0135a4a70f25ef042b;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;2521c3d76bc439c961b7003080f4a7a661949547;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;e9126a98de0c39dcffe4c4f5158e037460196724|3001544;1970334;7181955|True;True;True|desc;desc;desc
4157ed3db4c656854e69931cb6089b64b08784b9|10.1109/MICRO.2014.58||||609-622|Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cape_Town_2014|Cape Town|DaDianNao: A Machine-Learning Supercomputer|2014|7377735;2068286576;39419985;2145407329;37167270;2110368816;3353457;144049725;1719934;145550877;1731764|Yunji Chen;Tao Luo;Shaoli Liu;Shijin Zhang;Liqiang He;Jia Wang;Ling Li;Tianshi Chen;Zhiwei Xu;Ninghui Sun;O. Temam|Computer Science;Engineering|efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;d133cb102ad0f81e3fd17a7db090b28afc124c4a;bd898f483476e3dcacf83cd85efc64e6319da0e1;0023582fde36430c7e3ae81611a14e558c8f4bae;3cee40494377c0e7d9c7c23a3811b481e55bce39;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;5c45a5d05ac564adb67811eeb9d41d6460c70135;04fd278c01df1564e741b4c6e052fc1c5924ab8d;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;a486e2839291111bb44fa1f07731ada123539f75;0b544dfe355a5070b60986319a3f51fb45d1348e;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;e0535dedb8607d83cd2614317c99913378e89e26;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd|3527446;145481009;1740538|True;True;True|desc;desc;desc
dd9b99fac67c18be82d7763a8fbf231fc3512423|10.1109/TEVC.2015.2504420||||606-626|Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.|IROS|IROS_New_York_2016|New York|A Survey on Evolutionary Computation Approaches to Feature Selection|2016|144395433;145269712;2309030;143901532|Bing Xue;Mengjie Zhang;Will N. Browne;X. Yao|Computer Science|771479c18b586eafae21baf262a220aaa7b2eef6;fbf1c51548ffc9b9e538befcd71529365af23d15;574449170f293dfa868771e9ee0403b56a19b9e9;d997919c30fa6711bc5c25cf8c8aea34fac27b91;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;03cb4e2cb669d3f6344a733e622f07909f87ff0a;de2be42659be5c43c1a992b5d7fe6daf14e571dd;8d1c588d202f150e1797ed113fba7e67bfa43ecb;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;1a827052f01ef830cbc849c71e9da99791243a5f;4157ed3db4c656854e69931cb6089b64b08784b9;f94455176857303605ad423599385a2341c568eb;f3203d0bdefc9670ed508ca776d08aa9f024bafa;a486e2839291111bb44fa1f07731ada123539f75|2330895;7889032;3374545|True;True;True|desc;desc;desc
b3de1062d8a462dfdc2938558258f8884abe9f4e|10.1080/01431161.2018.1433343||||2784 - 2817|ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.|ICLR|ICLR_Rome_2018|Rome|Implementation of machine-learning classification in remote sensing: an applied review|2018|26339328;5875612;2014814754|Aaron E. Maxwell;T. Warner;Fang Fang|Computer Science;Environmental Science|7da323e7103245eeaed32367c46abe3f4913df86;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;3b7d120c0e801ef318bc9c607a0789f175637c7f;402f850dff86fb601d34b2841e6083ac0f928edd;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;b5887d18420e8ac4f4fa4c83c4952138fd956702;467568f1777bc51a15a5100516cd4fe8de62b9ab;b3852f0113fcf8a3913c55ae92393ae6ccde347e;7e7eb0f93c9550d7336f4bbfad5fe89604295705;f4156a05a47fdeda30638e10954d3674cc056ab6;c43025c429b1fbf6f1379f61801a1b40834d62e7;5966d7c7f60898d610812e24c64d4d57855ad86a;08b43d84e6747e370ef307e2ada50675b414514a;d133cb102ad0f81e3fd17a7db090b28afc124c4a;10f919b1a5161b560504c225cfb2d1b3a4768f80;7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7;395de0bd3837fdf4b4b5e5f04835bcc69c279481|145322333;2095214091;2247520360|True;False;True|desc;desc;desc
605402e235bd62437baf3c9ebefe77fb4d92ee95|10.1162/neco.1995.7.5.889||||889-904|Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_Beijing_1995|Beijing|The Helmholtz Machine|1995|1790646;1695689;1764325;1804104|P. Dayan;Geoffrey E. Hinton;Radford M. Neal;R. Zemel|Computer Science;Medicine;Physics;Mathematics|a7a407968c13ced804a063259d72315a43b84f29|2587444;2334455;143711382|True;True;True|desc;desc;desc
ae523e2f137fa2a4f5a6cbcc443ba63db2642a96|10.1109/TASLP.2018.2842159||||1702-1726|Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Sydney_2017|Sydney|Supervised Speech Separation Based on Deep Learning: An Overview|2017|38053687;2855690|Deliang Wang;Jitong Chen|Computer Science;Medicine;Engineering|22adb2413901b74128f2a02584dafa77afbd8d60;49bdeb07b045dd77f0bfe2b44436608770235a23;184ac0766262312ba76bbdece4e7ffad0aa8180b;a27089efabc5f4abd5ddf2be2a409bff41f31199;de2be42659be5c43c1a992b5d7fe6daf14e571dd;d63b884d5ebc739f6e1bdf861fa9276260781404;3a84214cb69ea0b34352285029f368b75718c32b;cbac8b0d82ea8e9251d5530695841d816cb196b9;d0ab11de3077490c80a08abd0fb8827bac84c454;6aae0dc122102693e8136856ffc8b72df7f78386;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;dd971c07879e1ce12b06991319528c06280eeb9b;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;b16408a97170785fb216c9e8b7920d64f478fbc8;05fd1da7b2e34f86ec7f010bef068717ae964332;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;ea58af907495e97c93997119db4a59fab5cd3683;9670485f526f2254c0f34e64d9ca06f665a0bd17;9f387ce140c59a44eaeeea590087351461345164|2660014;1998820;3472959|True;False;True|desc;desc;desc
45557cc70cd6989ab6b03e5aeb787e34299099f7|10.1109/CVPR46437.2021.01501|NEJM|30.0|9012-3456_30|15257-15266|We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.||||Natural Adversarial Examples|2019|3422872;2074109526;104444594;5164568;143711382|Dan Hendrycks;Kevin Zhao;Steven Basart;J. Steinhardt;D. Song|Computer Science;Mathematics|4d1fdd81f033cd58f3723bfc61e7d12079647a7a;546785490ac417be1f83ced6a8272e934934f411;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;a7a407968c13ced804a063259d72315a43b84f29;3cee40494377c0e7d9c7c23a3811b481e55bce39;dd9b99fac67c18be82d7763a8fbf231fc3512423;3def68bd0f856886d34272840a7f81588f2bc082;0090023afc66cd2741568599057f4e82b566137c;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;8592e46a5435d18bba70557846f47290b34c1aa5;86cff4d050beb90fed2e1ceac8940c8221b120aa;62ccd99a65bfc7c735ae1f33b75b107665de95df;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;611544418ca53cdad254df444addc7814abcfddc;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;e50f4d3316d13841c287dcdf5479d7820d593571;908cca0abefc35acc38033603714fbb1bcadc49d;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;771479c18b586eafae21baf262a220aaa7b2eef6|2266084696;145412074;7524887|True;False;False|desc;desc;desc
9b0dd87208a03e78105491e3727213b9b8ac0419|10.1257/JEP.28.2.3||||3-28|Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Moscow_2014|Moscow|Big Data: New Tricks for Econometrics|2014|2070970|H. Varian|Computer Science;Economics|06645d735b59b14479ae1d0392136bbf44227d0f;d0c882bcae6531fa13e75bcc5c297b9985f207f7;872352b0a53ab6cbb4420f81df64d215d86c7d9b;04fd278c01df1564e741b4c6e052fc1c5924ab8d;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;9b539d413393047b28bb7be9b195f142aaf7a80e;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;33e46a618fdb22d46951f548d6ceeb384e7f1687|143931014;10777941;144231976|True;True;True|desc;desc;desc
38f23fe236b152cd4983c8f30d305a568afd0d3e|10.1109/TNNLS.2020.3027314||||4793-4813|Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rio_de_Janeiro_2019|Rio de Janeiro|A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI|2019|71352570;145836900|Erico Tjoa;Cuntai Guan|Computer Science;Medicine|a85e512d8845bd007b0866b4a97e8341463f8190;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;184ac0766262312ba76bbdece4e7ffad0aa8180b;cc1cad12521b5aab43fdda5b4dec67586aef1f87;df2a7756382540e92895f10703cec32d50c4f316;6981ea66000e2c98f8a81f4bef05802234d986a4;694bdf6e5906992dad2987a3cc8d1a176de691c9;cedea36fa3692281b3ac767335fe49a16d00957d;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;8db9df2eadea654f128c1887722c677c708e8a47;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;4f71ab367eb37cfd145d41327f7bb14077e5e7c5;5c5e69387020d7ca7d49487ca841958dc5e08ce6;1904d633fca15140e35d893637232803b6dde6d9;10f919b1a5161b560504c225cfb2d1b3a4768f80;92ace17730c2173e642934d64f96d359697b7a93;6df11b0bb0244d4d36e8955436067cc5d19734fa|2972859;2064241030;10688956|True;True;True|desc;desc;desc
175e37bca3762b3a52c6a0e153060b98a251d061|10.1126/science.aat2663|JAMA|66.0|7890-1234_66|360 - 365|The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.||||Inverse molecular design using machine learning: Generative models for matter engineering|2018|1380248978;1380248954|Benjamín Sánchez-Lengeling;Alán Aspuru-Guzik|Medicine;Computer Science;Materials Science;Physics;Engineering|f354310098e09c1e1dc88758fca36767fd9d084d;d05d86db86a4ac0d95e6dcd951b42a9651939793;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;9257779eed46107bcdce9f4dc86298572ff466ce;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;3b7d120c0e801ef318bc9c607a0789f175637c7f;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;5794141889d0e994c3103b0aaab08a18222c9c43;30b2a3422332a76663110beae4bfc4d74763f4a0;8a5d0579590465494c9aba58a857af43b190b6a6;6adf016e7531c91100d3cf4a74f5d4c87b26b528;22adb2413901b74128f2a02584dafa77afbd8d60;e9126a98de0c39dcffe4c4f5158e037460196724;aaf9069be5a498179cbd2932d793ea1b9d0092de;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;3def68bd0f856886d34272840a7f81588f2bc082;1dae4d61cd74cc919ecc638bde6b7125728ea97b;2ea6a93199c9227fa0c1c7de13725f918c9be3a4|1780112;144225920;3207228|True;True;True|desc;desc;desc
eed9fa4483cab37eacd59db0fac4b1441431ee85|10.1109/TSP.2017.2690524||||3551-3582|"Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=""LaTeX"">$(i,j,k,\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=""LaTeX"">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning."|AAMAS|AAMAS_Beijing_2016|Beijing|Tensor Decomposition for Signal Processing and Machine Learning|2016|73776482;2217213;144406546;2349460;3000659;1702392|N. Sidiropoulos;L. D. Lathauwer;Xiao Fu;Kejun Huang;E. Papalexakis;C. Faloutsos|Computer Science;Mathematics|26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810|7564853;1761370;3259992|True;True;True|desc;desc;desc
0ef9ae1ce8c91ce671a211bdda792bf3752d1522|10.1109/ACCESS.2017.2762418||||21954-21961|Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Cape_Town_2017|Cape Town|A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks|2017|30796665;1733625;2191560;50046030|Chuanlong Yin;Yuefei Zhu;Jin-long Fei;Xin-Zheng He|Computer Science;Engineering|3b7d120c0e801ef318bc9c607a0789f175637c7f;23e44c7c6929bbb1ee5bc111e81e242f4835b712;d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;a85e512d8845bd007b0866b4a97e8341463f8190;427b168f490b56716f22b129ac93aba5425ea08f;b10e4deadf978d8fd6eec97ff18888629f4261ab;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;01b24de15cf337c55b9866c4b534596ca3d93abe;7380e343dd4547e21d5118b16daf03d021d98c4e;94549a171a61039ed1f9b5954ce42181c574ccc3;1904d633fca15140e35d893637232803b6dde6d9|3259992;2149106173;144189388|True;True;True|desc;desc;desc
e6b9fc7aa2996e7afc91c7f223460f9ded85e2da|10.1109/ICC.2019.8761315||||1-7|We envision a mobile edge computing (MEC) framework for machine learning (ML) technologies, which leverages distributed client data and computation resources for training high-performance ML models while preserving client privacy. Toward this future goal, this work aims to extend Federated Learning (FL), a decentralized learning framework that enables privacy-preserving training of models, to work with heterogeneous clients in a practical cellular network. The FL protocol iteratively asks random clients to download a trainable model from a server, update it with own data, and upload the updated model to the server, while asking the server to aggregate multiple client updates to further improve the model. While clients in this protocol are free from disclosing own private data, the overall training process can become inefficient when some clients are with limited computational resources (i.e., requiring longer update time) or under poor wireless channel conditions (longer upload time). Our new FL protocol, which we refer to as FedCS, mitigates this problem and performs FL efficiently while actively managing clients based on their resource conditions. Specifically, FedCS solves a client selection problem with resource constraints, which allows the server to aggregate as many client updates as possible and to accelerate performance improvement in ML models. We conducted an experimental evaluation using publicly-available large-scale image datasets to train deep neural networks on MEC environment simulations. The experimental results show that FedCS is able to complete its training process in a significantly shorter time compared to the original FL protocol.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Rio_de_Janeiro_2018|Rio de Janeiro|Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge|2018|3037568;1899753|T. Nishio;Ryo Yonetani|Computer Science;Engineering|d422df8bff4e677a3077635db116679d25142bfc;3df952d4a724655f7520ff95d4b2cef90fff0cae;2346d121f38fc19c77e0b062415519843f478163;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;8d1c588d202f150e1797ed113fba7e67bfa43ecb;5c5e69387020d7ca7d49487ca841958dc5e08ce6;402f850dff86fb601d34b2841e6083ac0f928edd;a20bfec3c95aad003dcb45a21a220c19cca8bb66;5ed59f49c1bb7de06cfa2a9467d5efb535103277;7ab0f0da686cd4094fd96f5a30e0b6072525fd09|2448692;39743720;39743720|True;True;True|desc;desc;desc
7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7|10.1177/1745691617693393||||1100 - 1122|Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.|AAMAS|AAMAS_New_York_2017|New York|Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning|2017|2075675;48804181|T. Yarkoni;Jacob Westfall|Computer Science;Medicine;Psychology|19e8869f4c29353de0d9b52542c1fe9def4cbc7d;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;1c00df1cb85fa7886b6666599eab59f2b301dd5d;694bdf6e5906992dad2987a3cc8d1a176de691c9;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;819167ace2f0caae7745d2f25a803979be5fbfae|121557847;1751762;2038176754|True;True;True|desc;desc;desc
694bdf6e5906992dad2987a3cc8d1a176de691c9|10.1109/CVPR46437.2021.01018||||10313-10322|Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be available at [1].|ICML|ICML_New_York_2020|New York|D-NeRF: Neural Radiance Fields for Dynamic Scenes|2020|49107901;3425624;1403428213;1397181875|Albert Pumarola;Enric Corona;Gerard Pons-Moll;F. Moreno-Noguer|Computer Science|36652428740cd30d245d55889f01a7fb04a91c93;d63b884d5ebc739f6e1bdf861fa9276260781404;21dfbc88b21b27fe8a245ab1df98edd45f655ae7;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;01b24de15cf337c55b9866c4b534596ca3d93abe;4f975da00a5b2a2f7236e34edcb7274e5fdab937;b57c54350769ffa59ff57f79ee5aad918844d298;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;b10e4deadf978d8fd6eec97ff18888629f4261ab;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;8db9df2eadea654f128c1887722c677c708e8a47;85d727b119304dde458bcd8cf5cb87a906fb41ba;8c8215b7f8111839f0066010a530a3a9f57ba15e;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;2521c3d76bc439c961b7003080f4a7a661949547;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;5cbe278b65a81602a864184bbca37de91448a5f5;d997919c30fa6711bc5c25cf8c8aea34fac27b91;24e6c5bfe9bb0751e5708b501d04e860011b2953|145979410;11852405;1742375|True;True;True|desc;desc;desc
e50f4d3316d13841c287dcdf5479d7820d593571|10.1145/2168752.2168771|PNAS|18.0|4567-8901_18|57:1-57:22|"Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.
 Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM."||||Factorization Machines with libFM|2012|2843982|Steffen Rendle|Computer Science|8db9df2eadea654f128c1887722c677c708e8a47;fbc913faf39b1e369dfcdcfefb354d846a46573c;0e90a73f03902cbe915af1aff54ea7f0b3373680;58a8bead87c8c1e37460dce28285c053c270f6e7;93884d89dfc8c3886f642018227a43fb7b58044f;7380e343dd4547e21d5118b16daf03d021d98c4e;8de174ab5419b9d3127695405efd079808e956e8;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;22adb2413901b74128f2a02584dafa77afbd8d60;402f850dff86fb601d34b2841e6083ac0f928edd;4b149a326e38b9237077d794a0d5f5b4865efacf;693914b7f38c19585e35668fd626aecf62d4c5e7;fbf1c51548ffc9b9e538befcd71529365af23d15;e7e25fd534e9e024da329aea546484938df305a5;7e7eb0f93c9550d7336f4bbfad5fe89604295705|33242383;1399133087;145470158|True;True;True|desc;desc;desc
8fb1b96dcc133b170e7e5b340f93c5f230d495ee|10.1145/502512.502568|The Lancet|32.0|6789-0123_32|377-382|Ensemble methods have recently garnered a great deal of attention in the machine learning community. Techniques such as Boosting and Bagging have proven to be highly effective but require repeated resampling of the training data, making them inappropriate in a data mining context. The methods presented in this paper take advantage of plentiful data, building separate classifiers on sequential chunks of training points. These classifiers are combined into a fixed-size ensemble using a heuristic replacement strategy. The result is a fast algorithm for large-scale or streaming data that classifies as well as a single decision tree built on all the data, requires approximately constant memory, and adjusts quickly to concept drift.||||A streaming ensemble algorithm (SEA) for large-scale classification|2001|2562282;2117912702|W. Street;YongSeog Kim|Computer Science;Mathematics|d05d86db86a4ac0d95e6dcd951b42a9651939793|152394142;144537437;1743808|True;True;False|desc;desc;desc
efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea|10.1001/jama.2017.18391|JACC|2.0|1234-5678_2|"
          1317-1318
        "|Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT||||Big Data and Machine Learning in Health Care.|2018|143649421;1740538|Andrew Beam;I. Kohane|Computer Science;Medicine|799f927692a6c08c5e630bea78c087c5051528fc;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;f3203d0bdefc9670ed508ca776d08aa9f024bafa;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;5ed59f49c1bb7de06cfa2a9467d5efb535103277;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;72e93aa6767ee683de7f001fa72f1314e40a8f35;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081|1878461;48385057;2060230787|True;True;True|desc;desc;desc
8a5d0579590465494c9aba58a857af43b190b6a6|10.1109/COMST.2019.2904897|PNAS|4.0|4567-8901_4|2224-2287|The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.||||Deep Learning in Mobile and Wireless Networking: A Survey|2018|3194878;144555592;1763096|Chaoyun Zhang;P. Patras;H. Haddadi|Computer Science;Engineering|9071775ebcfebddd54d879fe7e6c627673e4d305;546785490ac417be1f83ced6a8272e934934f411;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;3def68bd0f856886d34272840a7f81588f2bc082;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;18bc1d4271abe8dd6e16179cdb06524a4f396e16;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;605402e235bd62437baf3c9ebefe77fb4d92ee95;1904d633fca15140e35d893637232803b6dde6d9;e838ba98e198d2dac047736e77c50c0efa49c2dc;d02927d4de4a2a51cced4970da04b812cbee4342;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;01f29addca4dc6f189f903cb133dea7585813a6f;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e|145534175;2107009096;2116632761|True;True;True|desc;desc;desc
8d1c588d202f150e1797ed113fba7e67bfa43ecb|10.1056/NEJMoa1204471||||"
          1388-97
        "|"BACKGROUND
Persistent pain is measured by means of self-report, the sole reliance on which hampers diagnosis and treatment. Functional magnetic resonance imaging (fMRI) holds promise for identifying objective measures of pain, but brain measures that are sensitive and specific to physical pain have not yet been identified.


METHODS
In four studies involving a total of 114 participants, we developed an fMRI-based measure that predicts pain intensity at the level of the individual person. In study 1, we used machine-learning analyses to identify a pattern of fMRI activity across brain regions--a neurologic signature--that was associated with heat-induced pain. The pattern included the thalamus, the posterior and anterior insulae, the secondary somatosensory cortex, the anterior cingulate cortex, the periaqueductal gray matter, and other regions. In study 2, we tested the sensitivity and specificity of the signature to pain versus warmth in a new sample. In study 3, we assessed specificity relative to social pain, which activates many of the same brain regions as physical pain. In study 4, we assessed the responsiveness of the measure to the analgesic agent remifentanil.


RESULTS
In study 1, the neurologic signature showed sensitivity and specificity of 94% or more (95% confidence interval [CI], 89 to 98) in discriminating painful heat from nonpainful warmth, pain anticipation, and pain recall. In study 2, the signature discriminated between painful heat and nonpainful warmth with 93% sensitivity and specificity (95% CI, 84 to 100). In study 3, it discriminated between physical pain and social pain with 85% sensitivity (95% CI, 76 to 94) and 73% specificity (95% CI, 61 to 84) and with 95% sensitivity and specificity in a forced-choice test of which of two conditions was more painful. In study 4, the strength of the signature response was substantially reduced when remifentanil was administered.


CONCLUSIONS
It is possible to use fMRI to assess pain elicited by noxious heat in healthy persons. Future studies are needed to assess whether the signature predicts clinical pain. (Funded by the National Institute on Drug Abuse and others.)."|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Auckland_2013|Auckland|An fMRI-based neurologic signature of physical pain.|2013|2549424;35244773;1754035;143761817;38550277;2334455|T. Wager;L. Atlas;M. Lindquist;M. Roy;Choong-Wan Woo;E. Kross|Medicine|2fb23de9524b13a32d9ed7f2441c46c81558a3c8;1051280d2b825c04f27d231aba0f8284bb297880;0ca26f9a98dda0abb737692f72ffa682df14cb2f;f986968735459e789890f24b6b277b0920a9725d;4e6238c8613b5b81f81552939bce33296aedfbfe;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;9eb715fe0347445a2d63518cbb476d345ba86233;f762cc39a824de1360e8223222739aaa4cd4168c;81a4fd3004df0eb05d6c1cef96ad33d5407820df;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;cd49acefc8d51e324aa562e5337e1c2aff067053;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;d0ab11de3077490c80a08abd0fb8827bac84c454|144288136;4429495;152290618|True;True;False|desc;desc;desc
55f44d39630646f36eac91358f8f27d1bead384c|10.1145/1150402.1150531|The Lancet|5.0|6789-0123_5|935-940|KDD is a complex and demanding task. While a large number of methods has been established for numerous problems, many challenges remain to be solved. New tasks emerge requiring the development of new methods or processing schemes. Like in software development, the development of such solutions demands for careful analysis, specification, implementation, and testing. Rapid prototyping is an approach which allows crucial design decisions as early as possible. A rapid prototyping system should support maximal re-use and innovative combinations of existing methods, as well as simple and quick integration of new ones.This paper describes Yale, a free open-source environment forKDD and machine learning. Yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary. Additionally, Yale offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool. Following the paradigm of visual programming eases the design of processing schemes. While the graphical user interface supports interactive design, the underlying XML representation enables automated applications after the prototyping phase.After a discussion of the key concepts of Yale, we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization. These case studies cover tasks like feature engineering, text mining, data stream mining and tracking drifting concepts, ensemble methods and distributed data mining. This variety of applications is also reflected in a broad user base, we counted more than 40,000 downloads during the last twelve months.||||YALE: rapid prototyping for complex data mining tasks|2006|1876779;143762617;2274294;144362899;35175146|Ingo Mierswa;M. Wurst;Ralf Klinkenberg;Martin Scholz;Timm Euler|Computer Science|8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;5aefde4203ce46ea900a96835a7c59a5f50800e7;4c75b748911ddcd888c5122f7672f69caa5d661f;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;611544418ca53cdad254df444addc7814abcfddc;819167ace2f0caae7745d2f25a803979be5fbfae;075f328ef87a076151feb4d5b1f97b66aa597a90;afa778ba0ba6333e25671cfb691a4bdda13b2868;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a|1707347;2038165074;145336368|True;True;False|desc;desc;desc
f4a5503783487eba5c5e34b1d02c09016b244b1d|10.18653/v1/D18-1547||||5016-5026|Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.|IJCAI|IJCAI_Moscow_2018|Moscow|MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling|2018|17895970;144256365;33870107;3450866;2295429;2065760904;1768624|Paweł Budzianowski;Tsung-Hsien Wen;Bo-Hsiang Tseng;I. Casanueva;Stefan Ultes;Osman Ramadan;Milica Gasic|Computer Science;Linguistics|7da323e7103245eeaed32367c46abe3f4913df86;668b1277fbece28c4841eeab1c97e4ebd0079700;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;605402e235bd62437baf3c9ebefe77fb4d92ee95;cedea36fa3692281b3ac767335fe49a16d00957d;f354310098e09c1e1dc88758fca36767fd9d084d;e24b8a9531573d284647239affc6c855505b0de4;739769f4862753fc80057194456d758d2a148ee3;049aca6228fb68a263369380eda6d9a4fcbdb382;4a554da55fd9ff76c99e25d2ce937b225dc1100c;9257779eed46107bcdce9f4dc86298572ff466ce;45557cc70cd6989ab6b03e5aeb787e34299099f7;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;a3461eaf51016f9d6e85ea47173b27e019e801c4;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;a206216c3f67605ac6e25b0178c3f156dc0f7ba0;8a5d0579590465494c9aba58a857af43b190b6a6;93884d89dfc8c3886f642018227a43fb7b58044f|1706276;1782658;2855690|True;True;True|desc;desc;desc
cc1cad12521b5aab43fdda5b4dec67586aef1f87|10.3115/1118693.1118703|Frontiers for Young Minds|45.0|2345-6789_45|71-78|We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.||||Kernel Methods for Relation Extraction|2002|3190501;2939759;49754061|D. Zelenko;Chinatsu Aone;A. Richardella|Computer Science;Mathematics|4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;e838ba98e198d2dac047736e77c50c0efa49c2dc;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;24e6c5bfe9bb0751e5708b501d04e860011b2953;07abd02f02774d178f26ca99937e5f94001a9ec9;44c7d9fe583e3d317a619297e7e949070710799f;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;6df11b0bb0244d4d36e8955436067cc5d19734fa;864e7db59f2ccfec1ee9f6eba79566ac7b0634df|145325584;2056204271;2274294|False;True;True|desc;desc;desc
c6bbfb4fcaecc779c899af4bb52083870f4b996a|10.1109/FIT.2012.53|Nature|10.0|1234-5678_10|257-260|The Internet is continuously changing and evolving. The main communication form of present Internet is human-human. The Internet of Things (IoT) can be considered as the future evaluation of the Internet that realizes machine-to-machine (M2M) learning. Thus, IoT provides connectivity for everyone and everything. The IoT embeds some intelligence in Internet-connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. This paper addresses the existing development trends, the generic architecture of IoT, its distinguishing features and possible future applications. This paper also forecast the key challenges associated with the development of IoT. The IoT is getting increasing popularity for academia, industry as well as government that has the potential to bring significant personal, professional and economic benefits.||||Future Internet: The Internet of Things Architecture, Possible Applications and Key Challenges|2012|1874286;1727042;33498709;2111266186|Rafiullah Khan;Sarmad Ullah Khan;R. Zaheer;Shahid Khan|Computer Science;Environmental Science;Engineering|574449170f293dfa868771e9ee0403b56a19b9e9;831edc3d67457db83da40d260e93bfd7559347ae;61e27dbae190b82639c57f180ecf97e4c46fcad9;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;9071775ebcfebddd54d879fe7e6c627673e4d305;5e095981ebf4d389e9356bd56e59e0ade1b42e88;4b149a326e38b9237077d794a0d5f5b4865efacf;49bdeb07b045dd77f0bfe2b44436608770235a23;184ac0766262312ba76bbdece4e7ffad0aa8180b;033f25ad905ef2ed32a8331cf38b83953ff15922;7380e343dd4547e21d5118b16daf03d021d98c4e|31370754;1614034792;145840115|True;False;True|desc;desc;desc
693914b7f38c19585e35668fd626aecf62d4c5e7|10.18637/JSS.V025.I05||||1-54|During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysis methods, text clustering, text classification and string kernels.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Athens_2008|Athens|Text Mining Infrastructure in R|2008|2711164;1764952;153939986|Ingo Feinerer;K. Hornik;David Meyer|Computer Science;Mathematics|5966d7c7f60898d610812e24c64d4d57855ad86a|2152471553;7353741;145124475|True;True;True|desc;desc;desc
5a4631d5d75e3610037f87839628a4d166581e01|10.1093/nar/gkaa379|BMJ|58.0|8901-2345_58|W449 - W454|Abstract Major histocompatibility complex (MHC) molecules are expressed on the cell surface, where they present peptides to T cells, which gives them a key role in the development of T-cell immune responses. MHC molecules come in two main variants: MHC Class I (MHC-I) and MHC Class II (MHC-II). MHC-I predominantly present peptides derived from intracellular proteins, whereas MHC-II predominantly presents peptides from extracellular proteins. In both cases, the binding between MHC and antigenic peptides is the most selective step in the antigen presentation pathway. Therefore, the prediction of peptide binding to MHC is a powerful utility to predict the possible specificity of a T-cell immune response. Commonly MHC binding prediction tools are trained on binding affinity or mass spectrometry-eluted ligands. Recent studies have however demonstrated how the integration of both data types can boost predictive performances. Inspired by this, we here present NetMHCpan-4.1 and NetMHCIIpan-4.0, two web servers created to predict binding between peptides and MHC-I and MHC-II, respectively. Both methods exploit tailored machine learning strategies to integrate different training data types, resulting in state-of-the-art performance and outperforming their competitors. The servers are available at http://www.cbs.dtu.dk/services/NetMHCpan-4.1/ and http://www.cbs.dtu.dk/services/NetMHCIIpan-4.0/.||||NetMHCpan-4.1 and NetMHCIIpan-4.0: improved predictions of MHC antigen presentation by concurrent motif deconvolution and integration of MS MHC eluted ligand data|2020|89715134;144020846;3071266;145580998;143898643|Birkir Reynisson;Bruno Alvarez;S. Paul;Bjoern Peters;M. Nielsen|Computer Science;Biology;Medicine|46f74231b9afeb0c290d6d550043c55045284e5f;9d75cc322a4e06d0a3a868cb91b04219a289c12c;427b168f490b56716f22b129ac93aba5425ea08f;831edc3d67457db83da40d260e93bfd7559347ae;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;12439a6ff384e95ee2262ee982bc055534e30487;22adb2413901b74128f2a02584dafa77afbd8d60;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;cedea36fa3692281b3ac767335fe49a16d00957d;d079a2f877f554e00f71a6975435d8325987bdf5;24e6c5bfe9bb0751e5708b501d04e860011b2953;6aae0dc122102693e8136856ffc8b72df7f78386;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;2ea6a93199c9227fa0c1c7de13725f918c9be3a4;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;0165568bcc1a819c18564567f2ec15d859be2519;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;18bc1d4271abe8dd6e16179cdb06524a4f396e16;1626c940a64ad96a7ed53d7d6c0df63c6696956b|2823893;49107901;2116415778|True;True;True|desc;desc;desc
37a67228271527037c9250ae3fd220199275e42e|10.1109/TSP.2016.2601299||||794-816|This paper gives an overview of the majorization-minimization (MM) algorithmic framework, which can provide guidance in deriving problem-driven algorithms with low computational cost. A general introduction of MM is presented, including a description of the basic principle and its convergence results. The extensions, acceleration schemes, and connection to other algorithmic frameworks are also covered. To bridge the gap between theory and practice, upperbounds for a large number of basic functions, derived based on the Taylor expansion, convexity, and special inequalities, are provided as ingredients for constructing surrogate functions. With the pre-requisites established, the way of applying MM to solving specific problems is elaborated by a wide range of applications in signal processing, communications, and machine learning.|EMNLP|EMNLP_Sydney_2017|Sydney|Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning|2017|48186551;145284465;1743931|Ying Sun;P. Babu;D. Palomar|Computer Science;Mathematics;Engineering|605402e235bd62437baf3c9ebefe77fb4d92ee95;f9d119346b0773ea83251598fa5305bc75bac8ab;0ca26f9a98dda0abb737692f72ffa682df14cb2f;d12864a8acbab1830be755bfb9cb177e31ca5e20;b10e4deadf978d8fd6eec97ff18888629f4261ab;1051280d2b825c04f27d231aba0f8284bb297880;a40f97770296c7fca2e5361cbceba3f4aae399e0;6ec7c724aa1d906e9e9f81c58497adddb22175b8;6aae0dc122102693e8136856ffc8b72df7f78386;76f560991d56ad689ec32f9e9d13291e0193f4cf|1721860;1768624;5733445|True;True;False|desc;desc;desc
9670485f526f2254c0f34e64d9ca06f665a0bd17|10.21037/jtd.2020.02.64||||165 - 174|Background The coronavirus disease 2019 (COVID-19) outbreak originating in Wuhan, Hubei province, China, coincided with chunyun, the period of mass migration for the annual Spring Festival. To contain its spread, China adopted unprecedented nationwide interventions on January 23 2020. These policies included large-scale quarantine, strict controls on travel and extensive monitoring of suspected cases. However, it is unknown whether these policies have had an impact on the epidemic. We sought to show how these control measures impacted the containment of the epidemic. Methods We integrated population migration data before and after January 23 and most updated COVID-19 epidemiological data into the Susceptible-Exposed-Infectious-Removed (SEIR) model to derive the epidemic curve. We also used an artificial intelligence (AI) approach, trained on the 2003 SARS data, to predict the epidemic. Results We found that the epidemic of China should peak by late February, showing gradual decline by end of April. A five-day delay in implementation would have increased epidemic size in mainland China three-fold. Lifting the Hubei quarantine would lead to a second epidemic peak in Hubei province in mid-March and extend the epidemic to late April, a result corroborated by the machine learning prediction. Conclusions Our dynamic SEIR model was effective in predicting the COVID-19 epidemic peaks and sizes. The implementation of control measures on January 23 2020 was indispensable in reducing the eventual COVID-19 epidemic size.|UAI|UAI_Los_Angeles_2020|Los Angeles|Modified SEIR and AI prediction of the epidemics trend of COVID-19 in China under public health interventions|2020|48598873;30489614;1753652328;113837271;2340666;40459179;2217848089;2125517703;2292805636;134905027;1492076802;1491359904;1390903788;89843211;145371438;40527353;2108989415;2115375292;1576228528;1663359881;123624101;1626824540;2145443647;2119049448;145002796;144869300|Zi-feng Yang;Zhiqi Zeng;Ke Wang;Sook-san Wong;W. Liang;Mark Zanin;Peng Liu;Xudong Cao;Zhongqiang Gao;Zhitong Mai;Jingyi Liang;Xiaoqing Liu;Shiyue Li;Yimin Li;F. Ye;W. Guan;Yifan Yang;Fei Li;S. Luo;Yuqi Xie;Bin Liu;Zhoulang Wang;Shaobo Zhang;Yao-dong Wang;N. Zhong;Jianxing He|Medicine;Environmental Science|1696cbf7da0ee845c50591843993e6605adec177;f4a5503783487eba5c5e34b1d02c09016b244b1d;c43025c429b1fbf6f1379f61801a1b40834d62e7;e4a85af3f5dc41e13dc2cae9ee851953709b764e;815c84ab906e43f3e6322f2ca3fd5e1360c64285;31a537c48c2bf2d98f2020df5b72c413d0fea1da;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;398c296d0cc7f9d180f84969f8937e6d3a413796;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;b57c54350769ffa59ff57f79ee5aad918844d298;07abd02f02774d178f26ca99937e5f94001a9ec9;8515a302b8f389f8f1008cc2650e5ec0a6913e24;7ad66cba3b7e3abae7ef33122588512a146f7f77;22adb2413901b74128f2a02584dafa77afbd8d60;0b544dfe355a5070b60986319a3f51fb45d1348e;771ca13f78a6cfda9ed99004a386e9e7e187bd34;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7|1680133;145624000;144036711|True;True;True|desc;desc;desc
e838ba98e198d2dac047736e77c50c0efa49c2dc|10.1109/IIPHDW.2018.8388338||||117-122|These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Madrid_2018|Madrid|Data augmentation for improving deep learning in image classification problem|2018|32920239;31820684|Agnieszka Mikołajczyk;M. Grochowski|Computer Science|b9518627db25f05930e931f56497602363a75491;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081|1792142;2279559513;34911188|True;True;True|desc;desc;desc
31a537c48c2bf2d98f2020df5b72c413d0fea1da|10.1145/1378600.1378605||||29-39|This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2% of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90% contain road anomalies in need of repair.|ICML|ICML_New_York_2008|New York|The pothole patrol: using a mobile sensor network for road surface monitoring|2008|144195041;1780687;143879125;145470158;144478906;145034082|Jakob Eriksson;Lewis Girod;Bret Hull;Ryan Newton;S. Madden;H. Balakrishnan|Computer Science;Environmental Science;Engineering|0090023afc66cd2741568599057f4e82b566137c;f354310098e09c1e1dc88758fca36767fd9d084d;da048cdf883f2ac0551162cb1abd7e6d09e8e86a|2061412126;33805504;2191560|True;True;False|desc;desc;desc
2878d9936f494ed7d0c8aec47e9bcc5e51609f9a|10.1109/TNNLS.2015.2424995||||809-821|Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.|AAAI|AAAI_Toronto_2016|Toronto|Extreme Learning Machine for Multilayer Perceptron|2016|2967405;7175017;145678691|Jiexiong Tang;Chenwei Deng;G. Huang|Computer Science;Medicine|7ea35b35392c6ef5738635cec7d17b24fe3e4f04;b57c54350769ffa59ff57f79ee5aad918844d298;1904d633fca15140e35d893637232803b6dde6d9;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;3def68bd0f856886d34272840a7f81588f2bc082;0165568bcc1a819c18564567f2ec15d859be2519;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;3df952d4a724655f7520ff95d4b2cef90fff0cae;8515a302b8f389f8f1008cc2650e5ec0a6913e24;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;bd898f483476e3dcacf83cd85efc64e6319da0e1;07abd02f02774d178f26ca99937e5f94001a9ec9;d079a2f877f554e00f71a6975435d8325987bdf5;58a8bead87c8c1e37460dce28285c053c270f6e7;dd971c07879e1ce12b06991319528c06280eeb9b;93884d89dfc8c3886f642018227a43fb7b58044f|145678691;2031130601;116292660|True;True;True|desc;desc;desc
e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd|10.1029/2010JG001566||||00-07|We upscaled FLUXNET observations of carbon dioxide, water, and energy fluxes to the global scale using the machine learning technique, model tree ensembles (MTE). We trained MTE to predict site-level gross primary productivity (GPP), terrestrial ecosystem respiration (TER), net ecosystem exchange (NEE), latent energy (LE), and sensible heat (H) based on remote sensing indices, climate and meteorological data, and information on land use. We applied the trained MTEs to generate global flux fields at a 0.5 degrees x 0.5 degrees spatial resolution and a monthly temporal resolution from 1982 to 2008. Cross-validation analyses revealed good performance of MTE in predicting among-site flux variability with modeling efficiencies (MEf) between 0.64 and 0.84, except for NEE (MEf = 0.32). Performance was also good for predicting seasonal patterns (MEf between 0.84 and 0.89, except for NEE (0.64)). By comparison, predictions of monthly anomalies were not as strong (MEf between 0.29 and 0.52). Improved accounting of disturbance and lagged environmental effects, along with improved characterization of errors in the training data set, would contribute most to further reducing uncertainties. Our global estimates of LE (158 +/- 7 J x 10(18) yr(-1)), H (164 +/- 15 J x 10(18) yr(-1)), and GPP (119 +/- 6 Pg C yr(-1)) were similar to independent estimates. Our global TER estimate (96 +/- 6 Pg C yr(-1)) was likely underestimated by 5-10%. Hot spot regions of interannual variability in carbon fluxes occurred in semiarid to semihumid regions and were controlled by moisture supply. Overall, GPP was more important to interannual variability in NEE than TER. Our empirically derived fluxes may be used for calibration and evaluation of land surface process models and for exploratory and diagnostic assessments of the biosphere.|ECCV|ECCV_Cairo_2011|Cairo|Global patterns of land-atmosphere fluxes of carbon dioxide, latent heat, and sensible heat derived from eddy covariance, satellite, and meteorological observations|2011|95799904;2530948;31915023;3298473;31635460;48419707;2792896;90583296;48886858;88566094;3001544;2694442;153377151;35668343;6431329;2675885;49015301;4990733;6931417;2993671;2184976;47448503;40642512;5121611|M. Jung;M. Reichstein;H. Margolis;A. Cescatti;A. Richardson;M. A. Arain;A. Arneth;C. Bernhofer;D. Bonal;Jiquan Chen;D. Gianelle;N. Gobron;G. Kiely;W. Kutsch;G. Lasslop;B. Law;A. Lindroth;L. Merbold;Leonardo Montagnani;E. Moors;D. Papale;M. Sottocornola;F. Vaccari;C. Williams|Environmental Science|5aefde4203ce46ea900a96835a7c59a5f50800e7;872bae24c109f7c30e052ac218b17a8b028d08a0;f9d119346b0773ea83251598fa5305bc75bac8ab;7ad66cba3b7e3abae7ef33122588512a146f7f77;6adf016e7531c91100d3cf4a74f5d4c87b26b528;f7d997a640f2b804676cadb8030d8b2c7bd79d85;5a391667242b4a631acdd5917681b16a86523987;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;34f25a8704614163c4095b3ee2fc969b60de4698;f04df4e20a18358ea2f689b4c129781628ef7fc1;f762cc39a824de1360e8223222739aaa4cd4168c;c6bbfb4fcaecc779c899af4bb52083870f4b996a|1574064065;2079024030;7175017|True;True;True|desc;desc;desc
5e095981ebf4d389e9356bd56e59e0ade1b42e88|10.1136/amiajnl-2011-000203||||"
          552-6
        "|The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.|COLT|COLT_Rome_2011|Rome|2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text|2011|1723337;10208174;1807069;1807331|Özlem Uzuner;B. South;Shuying Shen;S. Duvall|Computer Science;Medicine|eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;38f23fe236b152cd4983c8f30d305a568afd0d3e;a88b3be9b2db0319f8880e60a131b3060dba1eb7|38053687;144019071;49113001|False;True;True|desc;desc;desc
05fd1da7b2e34f86ec7f010bef068717ae964332|10.5555/1577069.1577070||||1-40|Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Sydney_2009|Sydney|Exploring Strategies for Training Deep Neural Networks|2009|1777528;1751762;2373952;3087941|H. Larochelle;Yoshua Bengio;J. Louradour;Pascal Lamblin|Computer Science;Mathematics|45557cc70cd6989ab6b03e5aeb787e34299099f7;f354310098e09c1e1dc88758fca36767fd9d084d;c62043a7d2537bbf40a84b9913957452a47fdb83;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;668b1277fbece28c4841eeab1c97e4ebd0079700;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;78989616eeeac55b202e3e4205225e7135054185;3df952d4a724655f7520ff95d4b2cef90fff0cae;3cee40494377c0e7d9c7c23a3811b481e55bce39;aaf9069be5a498179cbd2932d793ea1b9d0092de;1696cbf7da0ee845c50591843993e6605adec177;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;bcce96a2a074448953fc61a29a84afbdfc8db55a;63861fbeb7ec41986b85965b9780b428d919919e;5d150cec2775f9bc863760448f14104cc8f42368|145033828;40411909;2373318|True;True;True|desc;desc;desc
076af19e50f022ccbe5bf16f413f79b5c6904c05|10.1177/001100007500500202||||10 - 2|"It is my thesis in this paper that we should re-examine and re-evaluate that very special way of being with another person which has been called empathic. I believe we tend to give too little consideration to an element which is extremely important both for the understanding of personality dynamics and for effecting changes in personality and behavior. It is one of the most delicate and powerful ways we have of using ourselves. In spite of all that has been said and written on this topic, it is a way of being which is rarely seen in full bloom in a relationship. I will start with my own somewhat faltering history in relation to this topic. Personal Vacillations Very early in my work as a therapist I discovered that simply listening to my client, very attentively, was an important way of being helpful. So when I was in doubt as to what I should do, in some active way, I listened. It seemed surprising to me that such a passive kind of interaction could be so useful. A little later a social worker, who had a background of Rankian training, helped me to learn that the most effective approach was to listen for the feelings, the emotions whose patterns could be discerned through the client's words. I believe she was the one who suggested that the best response was to ""reflect"" these feelings back to the client-- ""reflect"" becoming in time a word which made me cringe. But at that time it improved my work as therapist, and I was grateful. Then came my transition to a full-time university position where, with the help of students, I was at last able to scrounge equipment for recording our interviews. I cannot exaggerate the excitement of our learnings as we clustered about the machine which enabled us to listen to ourselves, playing over and over some puzzling point at which the interview clearly went wrong, or those moments in which the client moved significantly forward. (I still regard this as the one best way of learning to improve oneself as a therapist.) Among many lessons from these recordings, we came to realize that listening to feelings and ""reflecting"" them was a vastly complex process. We discovered that we could pinpoint the therapist response which caused a fruitful flow of significant expression to become superficial and unprofitable. Likewise we were able to spot the remark which turned a client's dull and desultory talk into a focused selfexploration. In such a context of learning it became quite natural to lay more stress upon the content of the therapist response than upon the empathic quality of the listening. To this extent we became heavily conscious of the techniques which the counselor or therapist was using. We became expert in analyzing, in very minute detail, the ebb and flow of the process in each interview, and"|RSS|RSS_Rome_1975|Rome|Empathic: An Unappreciated Way of Being|1975|34361437|C. Rogers|Philosophy;Psychology|5794141889d0e994c3103b0aaab08a18222c9c43|2144011217;1399279909;1717990|True;True;True|desc;desc;desc
ac3ccaf58ba543fd8d7b787cf939e55345b1659f|10.1109/ICASSP.2001.940586||||3461-3464 vol.6|Over the last 20-30 years, the extended Kalman filter (EKF) has become the algorithm of choice in numerous nonlinear estimation and machine learning applications. These include estimating the state of a nonlinear dynamic system as well estimating parameters for nonlinear system identification (eg, learning the weights of a neural network). The EKF applies the standard linear Kalman filter methodology to a linearization of the true nonlinear system. This approach is sub-optimal, and can easily lead to divergence. Julier et al. (1997), proposed the unscented Kalman filter (UKF) as a derivative-free alternative to the extended Kalman filter in the framework of state estimation. This was extended to parameter estimation by Wan and Van der Merwe et al., (2000). The UKF consistently outperforms the EKF in terms of prediction and estimation error, at an equal computational complexity of (OL/sup 3/)/sup l/ for general state-space problems. When the EKF is applied to parameter estimation, the special form of the state-space equations allows for an O(L/sup 2/) implementation. This paper introduces the square-root unscented Kalman filter (SR-UKF) which is also O(L/sup 3/) for general state estimation and O(L/sup 2/) for parameter estimation (note the original formulation of the UKF for parameter-estimation was O(L/sup 3/)). In addition, the square-root forms have the added benefit of numerical stability and guaranteed positive semi-definiteness of the state covariances.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Moscow_2001|Moscow|The square-root unscented Kalman filter for state and parameter-estimation|2001|1908796;48385057|Rudolph van der Merwe;E. Wan|Computer Science;Mathematics;Engineering|93884d89dfc8c3886f642018227a43fb7b58044f;f762cc39a824de1360e8223222739aaa4cd4168c;b57c54350769ffa59ff57f79ee5aad918844d298;4e6238c8613b5b81f81552939bce33296aedfbfe;605402e235bd62437baf3c9ebefe77fb4d92ee95|145394689;145473095;145557251|True;True;True|desc;desc;desc
4b61c25a86083c20730c9b12737ac6ac4178c364|10.1561/2200000071|The Lancet|92.0|6789-0123_92|219-354|Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.||||An Introduction to Deep Reinforcement Learning|2018|1389921282;40068904;18014232;1792298;145134886|Vincent François-Lavet;Peter Henderson;Riashat Islam;Marc G. Bellemare;Joelle Pineau|Computer Science;Mathematics|5cbe278b65a81602a864184bbca37de91448a5f5;b5887d18420e8ac4f4fa4c83c4952138fd956702;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;01f29addca4dc6f189f903cb133dea7585813a6f;1696cbf7da0ee845c50591843993e6605adec177;8de174ab5419b9d3127695405efd079808e956e8;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;12439a6ff384e95ee2262ee982bc055534e30487;a3461eaf51016f9d6e85ea47173b27e019e801c4;0ca26f9a98dda0abb737692f72ffa682df14cb2f|40975594;2675885;2144545128|True;True;True|desc;desc;desc
4f71ab367eb37cfd145d41327f7bb14077e5e7c5|10.1109/TGRS.2019.2907932||||6690-6709|Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral–spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Sydney_2019|Sydney|Deep Learning for Hyperspectral Image Classification: An Overview|2019|2116066317;145273596;38140728;2597809;2370080;1682001|Shutao Li;Weiwei Song;Leyuan Fang;Yushi Chen;Pedram Ghamisi;J. Benediktsson|Computer Science;Environmental Science;Engineering|f762cc39a824de1360e8223222739aaa4cd4168c;aaf9069be5a498179cbd2932d793ea1b9d0092de;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;f986968735459e789890f24b6b277b0920a9725d;49bdeb07b045dd77f0bfe2b44436608770235a23;5ed59f49c1bb7de06cfa2a9467d5efb535103277;55f44d39630646f36eac91358f8f27d1bead384c;d02927d4de4a2a51cced4970da04b812cbee4342;f9d119346b0773ea83251598fa5305bc75bac8ab|2006869;83889546;2047464842|True;True;False|desc;desc;desc
42ed4a9994e6121a9f325f5b901c5b3d7ce104f5|10.18653/v1/P19-1334||||3428-3448|A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.|NIPS|NIPS_Moscow_2019|Moscow|Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference|2019|145534175;2949185;2467508|R. Thomas McCoy;Ellie Pavlick;Tal Linzen|Computer Science;Linguistics|2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;5cbe278b65a81602a864184bbca37de91448a5f5;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;a27089efabc5f4abd5ddf2be2a409bff41f31199;4c75b748911ddcd888c5122f7672f69caa5d661f;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;a40f97770296c7fca2e5361cbceba3f4aae399e0|46956675;2711164;2320509|True;True;False|desc;desc;desc
3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0|10.1093/brain/awm319|JAMA|91.0|7890-1234_91|"
          681-9
        "|To be diagnostically useful, structural MRI must reliably distinguish Alzheimer's disease (AD) from normal aging in individual scans. Recent advances in statistical learning theory have led to the application of support vector machines to MRI for detection of a variety of disease states. The aims of this study were to assess how successfully support vector machines assigned individual diagnoses and to determine whether data-sets combined from multiple scanners and different centres could be used to obtain effective classification of scans. We used linear support vector machines to classify the grey matter segment of T1-weighted MR scans from pathologically proven AD patients and cognitively normal elderly individuals obtained from two centres with different scanning equipment. Because the clinical diagnosis of mild AD is difficult we also tested the ability of support vector machines to differentiate control scans from patients without post-mortem confirmation. Finally we sought to use these methods to differentiate scans between patients suffering from AD from those with frontotemporal lobar degeneration. Up to 96% of pathologically verified AD patients were correctly classified using whole brain images. Data from different centres were successfully combined achieving comparable results from the separate analyses. Importantly, data from one centre could be used to train a support vector machine to accurately differentiate AD and normal ageing scans obtained from another centre with different subjects and different scanner equipment. Patients with mild, clinically probable AD and age/sex matched controls were correctly separated in 89% of cases which is compatible with published diagnosis rates in the best clinical centres. This method correctly assigned 89% of patients with post-mortem confirmed diagnosis of either AD or frontotemporal lobar degeneration to their respective group. Our study leads to three conclusions: Firstly, support vector machines successfully separate patients with AD from healthy aging subjects. Secondly, they perform well in the differential diagnosis of two different forms of dementia. Thirdly, the method is robust and can be generalized across different centres. This suggests an important role for computer based diagnostic image analysis for clinical practice.||||Automatic classification of MR scans in Alzheimer's disease.|2008|144225920;2609123;145606181;7447906;116292660;1772842;1764768;144402064;3985221;52216909|S. Klöppel;C. Stonnington;C. Chu;Bogdan Draganski;R. Scahill;J. Rohrer;Nick C Fox;C. Jack;J. Ashburner;Richard S. J. Frackowiak|Computer Science;Medicine;Psychology|6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;0e90a73f03902cbe915af1aff54ea7f0b3373680;a9763afda62e960c35c80681f805ddecbef14a92;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;a20bfec3c95aad003dcb45a21a220c19cca8bb66|1847827;2346996570;49370597|True;False;True|desc;desc;desc
5cbe278b65a81602a864184bbca37de91448a5f5|10.1126/science.abq1158||||1092 - 1097|Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Mexico_City_2022|Mexico City|Competition-level code generation with AlphaCode|2022|47002813;2114950020;8270717;1684887;4337102;37212795;2152472076;2152469120;2058168486;49423009;2152469362;2067208983;2070068655;2154435638;2152471553;2256699276;1425082935;2421691;1851564;2071666;2079024030;152394142;2065370007;3187297;2152471960;143967473;2152472162;2152469135;2645384;1689108|Yujia Li;David Choi;Junyoung Chung;Nate Kushman;Julian Schrittwieser;Rémi Leblond;Tom;Eccles;James Keeling;Felix Gimeno;A. D. Lago;T. Hubert;Peter Choy;Cyprien de;Masson d’Autume;Igor Babuschkin;Xinyun Chen;Po-Sen Huang;Johannes Welbl;Sven Gowal;Alexey;Cherepanov;James Molloy;D. Mankowitz;Esme Sutherland Robson;Pushmeet Kohli;Nando de;Freitas;K. Kavukcuoglu;O. Vinyals|Computer Science;Medicine|b10e4deadf978d8fd6eec97ff18888629f4261ab;1592fe924114866c1ac559bae33ea789930daa98;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;a9763afda62e960c35c80681f805ddecbef14a92;d0ab11de3077490c80a08abd0fb8827bac84c454;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972;a486e2839291111bb44fa1f07731ada123539f75;5c7e5248d9eb7f373f10277410bf8506160907ea;877374c2913b787ee9f958f39e31c75d39ebcc15;bd1f14e7531220c39fad8f86985cce7b283f035d;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;a244c47a1d4a8c2894b22807df8c7eec16cc110a;f04df4e20a18358ea2f689b4c129781628ef7fc1;fee4db01f6f981931dfd87376a8f861353d1e494;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;f94455176857303605ad423599385a2341c568eb;6adf016e7531c91100d3cf4a74f5d4c87b26b528;1051280d2b825c04f27d231aba0f8284bb297880|145183709;2055614715;1733113|False;True;True|desc;desc;desc
5c5be36e3111e42247d78a6d529e4b1d7d2ced12|10.1109/ISBI.2011.5872394||||230-233|Segmentation is the process of partitioning digital images into meaningful regions. The analysis of biological high content images often requires segmentation as a first step. We propose ilastik as an easy-to-use tool which allows the user without expertise in image processing to perform segmentation and classification in a unified way. ilastik learns from labels provided by the user through a convenient mouse interface. Based on these labels, ilastik infers a problem specific segmentation. A random forest classifier is used in the learning step, in which each pixel's neighborhood is characterized by a set of generic (nonlinear) features. ilastik supports up to three spatial plus one spectral dimension and makes use of all dimensions in the feature calculation. ilastik provides realtime feedback that enables the user to interactively refine the segmentation result and hence further fine-tune the classifier. An uncertainty measure guides the user to ambiguous regions in the images. Real time performance is achieved by multi-threading which fully exploits the capabilities of modern multi-core machines. Once a classifier has been trained on a set of representative images, it can be exported and used to automatically process a very large number of images (e.g. using the CellProfiler pipeline). ilastik is an open source project and released under the BSD license at www.ilastik.org.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Berlin_2011|Berlin|Ilastik: Interactive learning and segmentation toolkit|2011|2059204163;145486652;1708103;1685187|Christoph Sommer;C. Straehle;U. Köthe;F. Hamprecht|Computer Science;Biology|6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;a7a407968c13ced804a063259d72315a43b84f29;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;d05d86db86a4ac0d95e6dcd951b42a9651939793;bd898f483476e3dcacf83cd85efc64e6319da0e1;611544418ca53cdad254df444addc7814abcfddc;b5887d18420e8ac4f4fa4c83c4952138fd956702;01f29addca4dc6f189f903cb133dea7585813a6f;e7e25fd534e9e024da329aea546484938df305a5;91c380406f5a862b5937e70e720802e5c787968d;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;64be9999b68e12d260ba7423f6b55ffd41552ad3;98c25683fc8d6446448b734b1bcf08e1457f8d85|67092021;144996246;46998035|True;False;True|desc;desc;desc
2077d0f30507d51a0d3bbec4957d55e817d66a59|10.1109/CVPR.2005.160|Radiology|38.0|0123-4567_38|860-867 vol. 2|We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.||||Fields of Experts: a framework for learning image priors|2005|145920814;2105795|S. Roth;Michael J. Black|Computer Science||49933077;2108989415;2002316|True;True;True|desc;desc;desc
1904d633fca15140e35d893637232803b6dde6d9|10.1109/TKDE.2018.2876857||||2346-2363|Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Auckland_2019|Auckland|Learning under Concept Drift: A Review|2019|144864069;1471737704;46279367;2056183624;143931014;46266495|Jie Lu;Anjin Liu;Fan Dong;Feng Gu;João Gama;Guangquan Zhang|Computer Science;Mathematics|9d7c04de906823a60d3ccb5f510fd0029af5c8b0;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;184ac0766262312ba76bbdece4e7ffad0aa8180b;2fb23de9524b13a32d9ed7f2441c46c81558a3c8;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;03cb4e2cb669d3f6344a733e622f07909f87ff0a;9691f67f5075bde2fd70da0135a4a70f25ef042b;9f387ce140c59a44eaeeea590087351461345164;3def68bd0f856886d34272840a7f81588f2bc082;5c7e5248d9eb7f373f10277410bf8506160907ea;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;815c84ab906e43f3e6322f2ca3fd5e1360c64285;668b1277fbece28c4841eeab1c97e4ebd0079700;1592fe924114866c1ac559bae33ea789930daa98;db68a79e59291b85e10300b79c43843b651aa195;36d442f59c61ea2912d227c24dee76778c546b0a;339a8e4cb0eba77675711ac255ac2a5d7ede1d53|143901532;1702172;3115341|True;False;True|desc;desc;desc
759d9a6c9206c366a8d94a06f4eb05659c2bb7f2|10.1109/TPAMI.2012.256|JAMA|71.0|7890-1234_71|1757-1772|To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of “closed set” recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is “open set” recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel “1-vs-set machine,” which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.||||Toward Open Set Recognition|2013|2613438;145603848;27469806;32163276|W. Scheirer;A. Rocha;Archana Sapkota;T. Boult|Computer Science;Medicine|6f24d7a6e1c88828e18d16c6db20f5329f6a6827;1e41ed1ac234cba0138329047e16a8a424389e77;1696cbf7da0ee845c50591843993e6605adec177;693914b7f38c19585e35668fd626aecf62d4c5e7;c6bbfb4fcaecc779c899af4bb52083870f4b996a;a675fe5a7d99ac6f7ff91fa084462faefe616148;a244c47a1d4a8c2894b22807df8c7eec16cc110a;abe8a57dc27598937c2cffde3fc21c1e6d1f11ce;16c0ef924da1f6b510c9c783ac764156f5a3d631;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;9f387ce140c59a44eaeeea590087351461345164;b3de1062d8a462dfdc2938558258f8884abe9f4e;f4156a05a47fdeda30638e10954d3674cc056ab6;10f919b1a5161b560504c225cfb2d1b3a4768f80|2722839;5121611;88566094|True;True;True|desc;desc;desc
86f0b58404a264a6216e29c78a5c113d900ca461|10.1109/TBME.2015.2496264|Science|88.0|3456-7890_88|1455-1462|Today, medical image analysis papers require solid experiments to prove the usefulness of proposed methods. However, experiments are often performed on data selected by the researchers, which may come from different institutions, scanners, and populations. Different evaluation measures may be used, making it difficult to compare the methods. In this paper, we introduce a dataset of 7909 breast cancer histopathology images acquired on 82 patients, which is now publicly available from http://web.inf.ufpr.br/vri/breast-cancer-database. The dataset includes both benign and malignant images. The task associated with this dataset is the automated classification of these images in two classes, which would be a valuable computer-aided diagnosis tool for the clinician. In order to assess the difficulty of this task, we show some preliminary results obtained with state-of-the-art image classification systems. The accuracy ranges from 80% to 85%, showing room for improvement is left. By providing this dataset and a standardized evaluation protocol to the scientific community, we hope to gather researchers in both the medical and the machine learning field to advance toward this clinical application.||||A Dataset for Breast Cancer Histopathological Image Classification|2016|32786132;144925520;144518944;1804638|F. Spanhol;Luiz Oliveira;C. Petitjean;L. Heutte|Computer Science;Medicine|37a67228271527037c9250ae3fd220199275e42e;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;694bdf6e5906992dad2987a3cc8d1a176de691c9;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3|34594667;144019071;46385221|True;True;False|desc;desc;desc
546785490ac417be1f83ced6a8272e934934f411|10.1093/annonc/mdy166|Radiology|55.0|0123-4567_55|1836–1842|"Background
Deep learning convolutional neural networks (CNN) may facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking.


Methods
Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge.


Results
In level-I dermatologists achieved a mean (±standard deviation) sensitivity and specificity for lesion classification of 86.6% (±9.3%) and 71.3% (±11.2%), respectively. More clinical information (level-II) improved the sensitivity to 88.9% (±9.6%, P = 0.19) and specificity to 75.7% (±11.7%, P < 0.05). The CNN ROC curve revealed a higher specificity of 82.5% when compared with dermatologists in level-I (71.3%, P < 0.01) and level-II (75.7%, P < 0.01) at their sensitivities of 86.6% and 88.9%, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P < 0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.


Conclusions
For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they may benefit from assistance by a CNN's image classification.


Clinical trial number
This study was registered at the German Clinical Trial Register (DRKS-Study-ID: DRKS00013570; https://www.drks.de/drks_web/)."||||Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists|2018|5016554;49903386;5580228;2220290014;2223688580;2154835838;34445787;2080112167;2113670271;5840101;2213770;153454907;4023283;6486577;32147295;8735139;2154835838;1404268156;40398486;2135937978;2056204271;1422494434;3026399;4039707;8720212;2135937679;50009466;1471206276;33340370;144821090;1401706768;6248858;46486553;51289782;46560749;49551702;51952495;32516743;32084575;3698245;16126046;2348967;2842984;12155787;12565475;2056679749;46322047;36030514;2420136;4198024;145295514;2135990561;123257978;38676513;37936174;49785210;2102253685;5580228;1485389319;2220290014;41065919;2136001275;146218865;2055574299;117461896;6288635|H. Haenssle;C. Fink;R. Schneiderbauer;F. Toberer;T. Buhl;A. Blum;Aadi Kalloo;A. Hassen;L. Thomas;A. Enk;L. Uhlmann;C. Alt;M. Arenbergerova;R. Bakos;Anne Baltzer;I. Bertlich;A. Blum;Therezia Bokor-Billmann;J. Bowling;Naira Braghiroli;R. Braun;K. Buder-Bakhaya;T. Buhl;H. Cabo;L. Čabrijan;Naciye Cevic;A. Classen;David Deltgen;C. Fink;I. Georgieva;L. Hakim-Meibodi;Susanne Hanner;Franziska Hartmann;J. Hartmann;G. Haus;E. Hoxha;R. Karls;H. Koga;J. Kreusch;A. Lallas;P. Majenka;A. Marghoob;C. Massone;L. Mekokishvili;D. Mestel;Volker Meyer;A. Neuberger;K. Nielsen;M. Oliviero;R. Pampena;J. Paoli;Erika Pawlik;B. Rao;Adriana Rendon;T. Russo;Ahmed Sadek;K. Samhaber;R. Schneiderbauer;A. Schweizer;F. Toberer;L. Trennheuser;Lyobomira Vlahova;Alexander Wald;J. Winkler;Priscila Wölbing;I. Zalaudek|Computer Science;Medicine|b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;06645d735b59b14479ae1d0392136bbf44227d0f;46f74231b9afeb0c290d6d550043c55045284e5f;0e90a73f03902cbe915af1aff54ea7f0b3373680;9257779eed46107bcdce9f4dc86298572ff466ce;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;574449170f293dfa868771e9ee0403b56a19b9e9;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;62ccd99a65bfc7c735ae1f33b75b107665de95df;c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45;1c00df1cb85fa7886b6666599eab59f2b301dd5d;d04d6db5f0df11d0cff57ec7e15134990ac07a4f;07abd02f02774d178f26ca99937e5f94001a9ec9;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;184ac0766262312ba76bbdece4e7ffad0aa8180b;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;e9126a98de0c39dcffe4c4f5158e037460196724;819167ace2f0caae7745d2f25a803979be5fbfae|2347855602;22254044;1796918|True;True;True|desc;desc;desc
1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd|10.1109/CVPR.2019.00046||||374-382|Modern machine learning suffers from \textit{catastrophic forgetting} when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to \textbf{scale up to a large number of classes}. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\% and 13.2\% respectively.|NIPS|NIPS_Los_Angeles_2019|Los Angeles|Large Scale Incremental Learning|2019|2119299240;2109306087;29957038;3105254;2145253136;3133575;46956675|Yue Wu;Yinpeng Chen;Lijuan Wang;Yuancheng Ye;Zicheng Liu;Yandong Guo;Y. Fu|Computer Science|d63b884d5ebc739f6e1bdf861fa9276260781404;63861fbeb7ec41986b85965b9780b428d919919e;a85e512d8845bd007b0866b4a97e8341463f8190;5c45a5d05ac564adb67811eeb9d41d6460c70135;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;55f44d39630646f36eac91358f8f27d1bead384c|88566094;1706280;2198278|True;True;True|desc;desc;desc
b8012351bc5ebce4a4b3039bbbba3ce393bc3315|10.1145/1273496.1273556||||473-480|Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Tokyo_2007|Tokyo|An empirical evaluation of deep architectures on problems with many factors of variation|2007|1777528;1761978;1760871;32837403;1751762|H. Larochelle;D. Erhan;Aaron C. Courville;J. Bergstra;Yoshua Bengio|Computer Science||3476431;69539592;2752609|False;True;True|desc;desc;desc
8a0f17e0ee66ad5f50cd35932747e6a806ef03cf|10.1177/117693510600200030||||59 - 77|Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Athens_2006|Athens|Applications of Machine Learning in Cancer Prediction and Prognosis|2006|34468738;2066145|Joseph A. Cruz;D. Wishart|Computer Science;Medicine|6ec7c724aa1d906e9e9f81c58497adddb22175b8;d0c882bcae6531fa13e75bcc5c297b9985f207f7;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;d12864a8acbab1830be755bfb9cb177e31ca5e20;cc1cad12521b5aab43fdda5b4dec67586aef1f87;a9763afda62e960c35c80681f805ddecbef14a92;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;d05d86db86a4ac0d95e6dcd951b42a9651939793;9eb715fe0347445a2d63518cbb476d345ba86233;d516daff247f7157fccde6649ace91d969cd1973|2285953900;2292805636;1401020033|False;True;True|desc;desc;desc
91c380406f5a862b5937e70e720802e5c787968d|10.1162/coli.2008.34.1.125|The Lancet|45.0|6789-0123_45|125-127|Text mining is the process of discovering information in large text collections, and automatically identifying interesting patterns and relationships in textual data. It is a relatively new research area, which has recently raised much interest among the research and industry communities, mainly due to the continuously increasing amount of information available on the Web and elsewhere. Text mining is a highly interdisci-plinary research area, bringing together research insights from the fields of data mining, natural language processing, machine learning, and information retrieval. In particular, text mining is closely related to the older area of data mining, which targets the extraction of interesting information from data records, although text mining is allegedly more difficult, as the source data consists of unstructured collections of documents rather than structured databases. The book by Feldman and Sanger is a thorough introduction to text mining, covering the general architecture of text mining systems, along with the main techniques used by such systems. It addresses both the theory and practice of text mining, and it illustrates the different techniques with real-world scenarios and practical applications. It is particularly relevant for students and professional practitioners, being structured as a self-contained handbook that does not require previous experience in any of the research fields involved. The book is structured into twelve chapters, which gradually introduce the area of text mining and related topics, starting with an introduction to the task of text mining, and ending with examples of practical applications from three different domains. The first chapter can be regarded as an overview of the book. It starts by defining the problem of text mining and the key elements in text mining: the document collections, the document features (words, terms, and concepts), and the role of background knowledge in text mining. It then briefly touches upon the possible applications of text mining, such as pattern discovery and trend analysis, and shortly discusses the interface layer of text mining systems. The second part of the chapter lays down the general architecture of a text mining system, which also serves as a rough guide for the rest of the book, as it describes the main components of a text mining system that are described in detail in subsequent chapters. Chapter 2 is one of the longest chapters in the book, and also one of the most dense in terms of newly introduced concepts. Despite being a more difficult read compared …||||Book Reviews: The Text Mining Handbook: Advanced Approaches to Analyzing Unstructured Data by Ronen Feldman and James Sanger|2008|145557251|Rada Mihalcea|Computer Science|546785490ac417be1f83ced6a8272e934934f411;9f387ce140c59a44eaeeea590087351461345164;1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435;602f31242e577d2d05f918a3080fd50095e7faed;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;8e51d68250db5637cd6bc1de98a99396441399b2;7da323e7103245eeaed32367c46abe3f4913df86;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;65d53938a12c77e7920b8eb3a49df249c978ba3f;fee4db01f6f981931dfd87376a8f861353d1e494;db68a79e59291b85e10300b79c43843b651aa195;6f24d7a6e1c88828e18d16c6db20f5329f6a6827|1690799;145412074;80243122|True;True;True|desc;desc;desc
6df11b0bb0244d4d36e8955436067cc5d19734fa|10.1109/TNNLS.2016.2599820|Nature|70.0|1234-5678_70|2660-2673|Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.||||Evaluating the Visualization of What a Deep Neural Network Has Learned|2015|1699054;49345823;144535526;3633358;145034054|W. Samek;Alexander Binder;G. Montavon;S. Lapuschkin;K. Müller|Computer Science;Medicine|01f29addca4dc6f189f903cb133dea7585813a6f;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;ec6200bdcc23b79a71555962cde50306c4029f1a;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;ac12c9b9e35e58b55d85a97c47886a7371c14afa;06645d735b59b14479ae1d0392136bbf44227d0f;ff7a293e95c0d44582b7625ee2233916f15cb361;dd9b99fac67c18be82d7763a8fbf231fc3512423;8d1c588d202f150e1797ed113fba7e67bfa43ecb;872bae24c109f7c30e052ac218b17a8b028d08a0|2117912702;2258061;1751812|True;True;True|desc;desc;desc
06645d735b59b14479ae1d0392136bbf44227d0f|10.1609/AIMAG.V40I2.2850|Radiology|16.0|0123-4567_16|44-58|Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.||||DARPA's Explainable Artificial Intelligence (XAI) Program|2019|2121780;1969847|David Gunning;D. Aha|Computer Science|45557cc70cd6989ab6b03e5aeb787e34299099f7;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;e50f4d3316d13841c287dcdf5479d7820d593571;8de174ab5419b9d3127695405efd079808e956e8;8515a302b8f389f8f1008cc2650e5ec0a6913e24;9eb715fe0347445a2d63518cbb476d345ba86233;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;01f29addca4dc6f189f903cb133dea7585813a6f;f7d997a640f2b804676cadb8030d8b2c7bd79d85;2c47bd8bd699914e3535292b17ba46542800845c|2152469120;2255301807;38676513|False;True;True|desc;desc;desc
5c5e69387020d7ca7d49487ca841958dc5e08ce6|10.1198/tech.2006.s353|NEJM|61.0|9012-3456_61|147 - 148|counterpart as a special case, which leads to the construction of more universal ﬁlters that expand the capability and the robustness of data processing||||The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning|2006|40618053|L. Deng|Computer Science;Mathematics|4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;45557cc70cd6989ab6b03e5aeb787e34299099f7;76f560991d56ad689ec32f9e9d13291e0193f4cf;01f702f8b1f9d1314587015f1f038af4d5735e77;2077d0f30507d51a0d3bbec4957d55e817d66a59;0e779fd59353a7f1f5b559b9d65fa4bfe367890c;44c7d9fe583e3d317a619297e7e949070710799f;8c8215b7f8111839f0066010a530a3a9f57ba15e|9706655;3294736;2184976|True;True;True|desc;desc;desc
ea58af907495e97c93997119db4a59fab5cd3683|10.1109/MCI.2010.938364|JACC|51.0|1234-5678_51|13-18|"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work."||||Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]|2010|1804314;2483864;1970334|I. Arel;Derek C. Rose;T. Karnowski|Computer Science|1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;62ccd99a65bfc7c735ae1f33b75b107665de95df;6ec7c724aa1d906e9e9f81c58497adddb22175b8;c2b381b24aabf237394059fed7920cd6fd0e67b8;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;222d3a63d4f81d39ea324530b57328c58f298888;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;739769f4862753fc80057194456d758d2a148ee3;2346d121f38fc19c77e0b062415519843f478163;4f71ab367eb37cfd145d41327f7bb14077e5e7c5;7380e343dd4547e21d5118b16daf03d021d98c4e;10f919b1a5161b560504c225cfb2d1b3a4768f80;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;0165568bcc1a819c18564567f2ec15d859be2519;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|3092435;3194361;3422872|True;True;False|desc;desc;desc
c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45|10.1145/1629575.1629587|PNAS|70.0|4567-8901_70|117-132|Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals.||||Detecting large-scale system problems by mining console logs|2009|40515617;50055322;143608596;1701130;1694621|W. Xu;Ling Huang;A. Fox;D. Patterson;Michael I. Jordan|Computer Science|b3852f0113fcf8a3913c55ae92393ae6ccde347e;01f702f8b1f9d1314587015f1f038af4d5735e77;63861fbeb7ec41986b85965b9780b428d919919e;815c84ab906e43f3e6322f2ca3fd5e1360c64285;8de174ab5419b9d3127695405efd079808e956e8;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;dd971c07879e1ce12b06991319528c06280eeb9b;f7d997a640f2b804676cadb8030d8b2c7bd79d85;1592fe924114866c1ac559bae33ea789930daa98;cc1cad12521b5aab43fdda5b4dec67586aef1f87;3def68bd0f856886d34272840a7f81588f2bc082;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;8592e46a5435d18bba70557846f47290b34c1aa5;36d442f59c61ea2912d227c24dee76778c546b0a;265644f1b6740ca34bfbe9762b90b33021adde62;ff7a293e95c0d44582b7625ee2233916f15cb361|2113693105;2201937;145273596|False;True;False|desc;desc;desc
a7a407968c13ced804a063259d72315a43b84f29|10.1109/ACCESS.2020.2988510||||75264-75278|The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.|NeurIPS|NeurIPS_Tokyo_2020|Tokyo|Artificial Intelligence in Education: A Review|2020|1669767608;47978704;144264986|Lijia Chen;Pingping Chen;Zhijian Lin|Computer Science;Education|01b24de15cf337c55b9866c4b534596ca3d93abe;88816ae492956f3004daa41357166f1181c0c1bf;6981ea66000e2c98f8a81f4bef05802234d986a4;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;45557cc70cd6989ab6b03e5aeb787e34299099f7;0165568bcc1a819c18564567f2ec15d859be2519;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;49bdeb07b045dd77f0bfe2b44436608770235a23;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|1798960;47454309;1735243|True;True;True|desc;desc;desc
a20bfec3c95aad003dcb45a21a220c19cca8bb66|10.1162/089120101753342653|PNAS|24.0|4567-8901_24|521-544|In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.||||A Machine Learning Approach to Coreference Resolution of Noun Phrases|2001|2248565;34789794;3216372|Wee Meng Soon;H. Ng;Chung Yong Lim|Computer Science|78989616eeeac55b202e3e4205225e7135054185;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;7ad66cba3b7e3abae7ef33122588512a146f7f77;63861fbeb7ec41986b85965b9780b428d919919e;a85e512d8845bd007b0866b4a97e8341463f8190;ac12c9b9e35e58b55d85a97c47886a7371c14afa;fbf1c51548ffc9b9e538befcd71529365af23d15;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;3a84214cb69ea0b34352285029f368b75718c32b;bd898f483476e3dcacf83cd85efc64e6319da0e1;c6bbfb4fcaecc779c899af4bb52083870f4b996a|143635540;2095762;2135937679|False;True;True|desc;desc;desc
12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4|10.1109/TSP.2004.830991|PNAS|85.0|4567-8901_85|2165-2176|Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.||||Online learning with kernels|2001|1700597;46234526;143957317|Jyrki Kivinen;Alex Smola;R. C. Williamson|Computer Science;Mathematics|61394599ed0aabe04b724c7ca3a778825c7e776f;62ccd99a65bfc7c735ae1f33b75b107665de95df;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;ea58af907495e97c93997119db4a59fab5cd3683;b3852f0113fcf8a3913c55ae92393ae6ccde347e;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;872352b0a53ab6cbb4420f81df64d215d86c7d9b;78947497cbbffc691aac3f590d972130259af9ce;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;6f24d7a6e1c88828e18d16c6db20f5329f6a6827|1869515;143762617;46659335|True;True;False|desc;desc;desc
a206216c3f67605ac6e25b0178c3f156dc0f7ba0|10.1109/ANZIIS.1994.396988||||357-361|WEKA is a workbench for machine learning that is intended to aid in the application of machine learning techniques to a variety of real-world problems, in particular, those arising from agricultural and horticultural domains. Unlike other machine learning projects, the emphasis is on providing a working environment for the domain specialist rather than the machine learning expert. Lessons learned include the necessity of providing a wealth of interactive tools for data manipulation, result visualization, database linkage, and cross-validation and comparison of rule sets, to complement the basic machine learning tools.<<ETX>>|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Lisbon_1994|Lisbon|WEKA: a machine learning workbench|1994|144189431;2074980634;9419406|G. Holmes;A. Donkin;I. Witten|Computer Science;Agricultural and Food Sciences|7380e343dd4547e21d5118b16daf03d021d98c4e;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;85d727b119304dde458bcd8cf5cb87a906fb41ba;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;9b0dd87208a03e78105491e3727213b9b8ac0419;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;23e44c7c6929bbb1ee5bc111e81e242f4835b712;cedea36fa3692281b3ac767335fe49a16d00957d;dd9b99fac67c18be82d7763a8fbf231fc3512423;441c31274f4535a4a50892c1ad6e19eacfd17f8c;6aae0dc122102693e8136856ffc8b72df7f78386;d12864a8acbab1830be755bfb9cb177e31ca5e20;e0535dedb8607d83cd2614317c99913378e89e26;402f850dff86fb601d34b2841e6083ac0f928edd;9d46dc975aeed3f96bddb144079b50238f746ecd;a244c47a1d4a8c2894b22807df8c7eec16cc110a|1693065;51162051;1840524|True;True;True|desc;desc;desc
21dfbc88b21b27fe8a245ab1df98edd45f655ae7|10.1056/NEJMra1814259||||1347–1358|Machine Learning in Medicine In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. The...|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Athens_2019|Athens|Machine Learning in Medicine|2019|8638650;2056947059;1740538|A. Rajkomar;Jeffrey Dean;I. Kohane|Computer Science;Medicine|1f87134a630c2dbb9a3645ba658954f00b620a77;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;6adf016e7531c91100d3cf4a74f5d4c87b26b528;5ed59f49c1bb7de06cfa2a9467d5efb535103277;076af19e50f022ccbe5bf16f413f79b5c6904c05;76f560991d56ad689ec32f9e9d13291e0193f4cf;24e6c5bfe9bb0751e5708b501d04e860011b2953;668b1277fbece28c4841eeab1c97e4ebd0079700;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;2521c3d76bc439c961b7003080f4a7a661949547;8a5d0579590465494c9aba58a857af43b190b6a6;7ab0f0da686cd4094fd96f5a30e0b6072525fd09;0165568bcc1a819c18564567f2ec15d859be2519;0ba86604228b555475496e200f31878df3aabd6e;ec6200bdcc23b79a71555962cde50306c4029f1a;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;63861fbeb7ec41986b85965b9780b428d919919e;f9d119346b0773ea83251598fa5305bc75bac8ab;9d46dc975aeed3f96bddb144079b50238f746ecd;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a|40221187;3158246;5580228|False;False;False|desc;desc;desc
a40f97770296c7fca2e5361cbceba3f4aae399e0|10.1109/CVPR.2018.00459||||4367-4375|"The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on ""unseen"" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results."|ICML|ICML_Athens_2018|Athens|Dynamic Few-Shot Visual Learning Without Forgetting|2018|2475428;2505902|Spyros Gidaris;N. Komodakis|Computer Science|b57c54350769ffa59ff57f79ee5aad918844d298;4b61c25a86083c20730c9b12737ac6ac4178c364;574449170f293dfa868771e9ee0403b56a19b9e9;0b544dfe355a5070b60986319a3f51fb45d1348e;f04df4e20a18358ea2f689b4c129781628ef7fc1;16c0ef924da1f6b510c9c783ac764156f5a3d631;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;1f87134a630c2dbb9a3645ba658954f00b620a77|8638650;88566094;3353457|True;False;True|desc;desc;desc
45c9f19b1eb46095e61f3c1a9970a6161c13a861|10.1109/ICNN.1991.163370||||341-342|Summary form only given. The authors introduced a neural network architecture, called ARTMAP, that autonomously learns to classify arbitrarily many, arbitrarily ordered vectors into recognition categories based on predictive success. This supervised learning system is built up from a pair of adaptive resonance theory modules (ART/sub a/ and ART/sub b/) that are capable of self-organizing stable recognition categories in response to arbitrary sequences of input patterns. Tested on a benchmark machine learning database in both online and offline simulations, the ARTMAP system learns orders of magnitude more quickly, efficiently, and accurately than alternative algorithms, and achieves 100% accuracy after training on less than half of the input patterns in the database.<<ETX>>|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_1991|Lisbon|ARTMAP: supervised real-time learning and classification of nonstationary data by a self-organizing neural network|1991|143809344;1682174;2255301807|G. Carpenter;S. Grossberg;J. Reynolds|Computer Science|2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;34f25a8704614163c4095b3ee2fc969b60de4698;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;2521c3d76bc439c961b7003080f4a7a661949547;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175|1720124;69054960;6199470|True;True;False|desc;desc;desc
b5887d18420e8ac4f4fa4c83c4952138fd956702|10.1109/ACCESS.2020.2988796||||80716-80727|The k-means algorithm is generally the most known and used clustering method. There are various extensions of k-means to be proposed in the literature. Although it is an unsupervised learning to clustering in pattern recognition and machine learning, the k-means algorithm and its extensions are always influenced by initializations with a necessary number of clusters a priori. That is, the k-means algorithm is not exactly an unsupervised clustering method. In this paper, we construct an unsupervised learning schema for the k-means algorithm so that it is free of initializations without parameter selection and can also simultaneously find an optimal number of clusters. That is, we propose a novel unsupervised k-means (U-k-means) clustering algorithm with automatically finding an optimal number of clusters without giving any initialization and parameter selection. The computational complexity of the proposed U-k-means clustering algorithm is also analyzed. Comparisons between the proposed U-k-means and other existing methods are made. Experimental results and comparisons actually demonstrate these good aspects of the proposed U-k-means clustering algorithm.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Beijing_2020|Beijing|Unsupervised K-Means Clustering Algorithm|2020|2300368826;1741064|Kristina P. Sinaga;Miin-Shen Yang|Computer Science|d37fc9e9c4fedc32865b08661e7fb950df1f8fbe;1a827052f01ef830cbc849c71e9da99791243a5f;4157ed3db4c656854e69931cb6089b64b08784b9;5d150cec2775f9bc863760448f14104cc8f42368;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;df40ce107a71b770c9d0354b78fdd8989da80d2f;bd898f483476e3dcacf83cd85efc64e6319da0e1;a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e;877374c2913b787ee9f958f39e31c75d39ebcc15;602f31242e577d2d05f918a3080fd50095e7faed;049aca6228fb68a263369380eda6d9a4fcbdb382|34684035;30352875;2164604|True;True;True|desc;desc;desc
61e27dbae190b82639c57f180ecf97e4c46fcad9|10.1109/ACCESS.2020.2990567||||89497-89509|Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.|ICML|ICML_Beijing_2020|Beijing|Pymoo: Multi-Objective Optimization in Python|2020|31519649;145080287|Julian Blank;K. Deb|Computer Science|88816ae492956f3004daa41357166f1181c0c1bf;85d727b119304dde458bcd8cf5cb87a906fb41ba;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;3fa5f45ddbd5184f10bfb92e367493c5a344f207;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;53834f0ee8df731cf0e629cd594dce0afaaa3d97;f762cc39a824de1360e8223222739aaa4cd4168c;16c0ef924da1f6b510c9c783ac764156f5a3d631;c6a83c4fcc99ba6753109301949c5b7cfa978079;5a391667242b4a631acdd5917681b16a86523987;aaf9069be5a498179cbd2932d793ea1b9d0092de;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;a27089efabc5f4abd5ddf2be2a409bff41f31199;d98d0d1900b13b87aa4ffd6b69c046beb63f0434|2231981905;9215658;1403820685|True;True;True|desc;desc;desc
d02927d4de4a2a51cced4970da04b812cbee4342|10.1002/asi.21662||||163-173|Sentiment analysis is concerned with the automatic extraction of sentiment-related information from text. Although most sentiment analysis addresses commercial tasks, such as extracting opinions from product reviews, there is increasing interest in the affective dimension of the social web, and Twitter in particular. Most sentiment analysis algorithms are not ideally suited to this task because they exploit indirect indicators of sentiment that can reflect genre or topic instead. Hence, such algorithms used to process social web texts can identify spurious sentiment patterns caused by topics rather than affective phenomena. This article assesses an improved version of the algorithm SentiStrength for sentiment strength detection across the social web that primarily uses direct indications of sentiment. The results from six diverse social web data sets (MySpace, Twitter, YouTube, Digg, RunnersWorld, BBCForums) indicate that SentiStrength 2 is successful in the sense of performing better than a baseline approach for all data sets in both supervised and unsupervised cases. SentiStrength is not always better than machine-learning approaches that exploit indirect indicators of sentiment, however, and is particularly weaker for positive sentiment in news-related discussions. Overall, the results suggest that, even unsupervised, SentiStrength is robust enough to be applied to a wide variety of different social web contexts.|ICCV|ICCV_Los_Angeles_2012|Los Angeles|Sentiment strength detection for the social web|2012|1701298;144563108;1718676|M. Thelwall;K. Buckley;G. Paltoglou|Computer Science|ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;58a8bead87c8c1e37460dce28285c053c270f6e7;cd49acefc8d51e324aa562e5337e1c2aff067053;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;09622b0c84bf812814af5b64b0c83dce796899c4;1f87134a630c2dbb9a3645ba658954f00b620a77;c6a83c4fcc99ba6753109301949c5b7cfa978079;5cbe278b65a81602a864184bbca37de91448a5f5;53834f0ee8df731cf0e629cd594dce0afaaa3d97;9691f67f5075bde2fd70da0135a4a70f25ef042b;3df952d4a724655f7520ff95d4b2cef90fff0cae;bd1f14e7531220c39fad8f86985cce7b283f035d;fee4db01f6f981931dfd87376a8f861353d1e494;e838ba98e198d2dac047736e77c50c0efa49c2dc;3b7d120c0e801ef318bc9c607a0789f175637c7f;df40ce107a71b770c9d0354b78fdd8989da80d2f;72e93aa6767ee683de7f001fa72f1314e40a8f35;23e44c7c6929bbb1ee5bc111e81e242f4835b712|8896870;1764325;7447906|True;False;True|desc;desc;desc
19e8869f4c29353de0d9b52542c1fe9def4cbc7d|10.1002/asi.21234|BMJ|61.0|8901-2345_61|I-XXI, 1-482|Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.||||Introduction to Information Retrieval|2008|2309291535|Ray R. Larson|Computer Science;Mathematics|a85e512d8845bd007b0866b4a97e8341463f8190;4a554da55fd9ff76c99e25d2ce937b225dc1100c;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;825ca26af5a2a510dbc1a7b97587212bc98ae968;94549a171a61039ed1f9b5954ce42181c574ccc3;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;a40f97770296c7fca2e5361cbceba3f4aae399e0;9b0dd87208a03e78105491e3727213b9b8ac0419;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;0165568bcc1a819c18564567f2ec15d859be2519|2972859;2755582;2289341703|True;True;True|desc;desc;desc
48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016|10.1561/2200000044|Frontiers for Young Minds|28.0|2345-6789_28|123-286|Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.||||Determinantal Point Processes for Machine Learning|2012|145500336;1685978|Alex Kulesza;B. Taskar|Computer Science;Mathematics|5b66b1c65dcb97d1d0b18014e2e32e8522e66932;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;9e475a514f54665478aac6038c262e5a6bac5e64|3259992;1701686;2038165074|False;True;False|desc;desc;desc
35b3233e521f1e9a34837c30be1957858f8f35fe|10.1145/382912.382914|Frontiers for Young Minds|22.0|2345-6789_22|227-261|Intrusion detection (ID) is an important component of infrastructure protection mechanisms. Intrusion detection systems (IDSs) need to be accurate, adaptive, and extensible. Given these requirements and the complexities of today's network environments, we need a more systematic and automated IDS development process rather that the pure knowledge encoding and engineering approaches. This article describes a novel framework, MADAM ID, for Mining Audit Data for Automated Models for Instrusion Detection. This framework uses data mining algorithms to compute activity patterns from system audit data and extracts predictive features from the patterns. It then applies machine learning algorithms to the audit records taht are processed according to the feature definitions to generate intrusion detection rules. Results from the 1998 DARPA Intrusion Detection Evaluation showed that our ID model was one of the best performing of all the participating systems. We also briefly discuss our experience in converting the detection models produced by off-line data mining programs to real-time modules of existing IDSs.||||A framework for constructing features and models for intrusion detection systems|2000|1738428;1807433|Wenke Lee;S. Stolfo|Computer Science|1904d633fca15140e35d893637232803b6dde6d9;1f87134a630c2dbb9a3645ba658954f00b620a77;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;81a4fd3004df0eb05d6c1cef96ad33d5407820df;574449170f293dfa868771e9ee0403b56a19b9e9;f3203d0bdefc9670ed508ca776d08aa9f024bafa;9691f67f5075bde2fd70da0135a4a70f25ef042b;831edc3d67457db83da40d260e93bfd7559347ae;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;a85e512d8845bd007b0866b4a97e8341463f8190;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;602f31242e577d2d05f918a3080fd50095e7faed;5a391667242b4a631acdd5917681b16a86523987|3177811;2246856610;48506828|True;True;True|desc;desc;desc
ec6200bdcc23b79a71555962cde50306c4029f1a|10.11989/JEST.1674-862X.80904120||||26-40|Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Sydney_2019|Sydney|Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization|2019|153171583;2109243152;1682058;2068236437;1878461;48136123|Jia Wu;Xiuyun Chen;H. Zhang;Li-Dong Xiong;Hang Lei;S. Deng|Computer Science|a675fe5a7d99ac6f7ff91fa084462faefe616148;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;a9763afda62e960c35c80681f805ddecbef14a92;48234756b7cf798bfeb47328f7c5d597fd4838c2|3913651;2649909;145412074|True;False;True|desc;desc;desc
16c0ef924da1f6b510c9c783ac764156f5a3d631|10.1109/TKDE.2020.2981314||||50-70|Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Los_Angeles_2018|Los Angeles|A Survey on Deep Learning for Named Entity Recognition|2018|39682944;1735962;3192562;2829009|J. Li;Aixin Sun;Jianglei Han;Chenliang Li|Computer Science|693914b7f38c19585e35668fd626aecf62d4c5e7;a88b3be9b2db0319f8880e60a131b3060dba1eb7;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;cbac8b0d82ea8e9251d5530695841d816cb196b9;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;78989616eeeac55b202e3e4205225e7135054185|143931014;1830349;3349310|True;False;True|desc;desc;desc
24e6c5bfe9bb0751e5708b501d04e860011b2953|10.21873/CGP.20063||||"
          41-51
        "|Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.|ICLR|ICLR_Tokyo_2018|Tokyo|Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.|2018|47156522;27122362;47479574;32286482;50232365|Shujun Huang;Nianguang Cai;Pedro Penzuti Pacheco;Shavira Narrandes;Yang Wang;Wayne W. Xu|Computer Science;Medicine;Biology|668b1277fbece28c4841eeab1c97e4ebd0079700;220ac48a22547a455d05f416e1fd22bbd0b0788d;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;d7701e78e0bfc92b03a89582e80cfb751ac03f26;b954efe5e46b8952f5a8daf42e7e535119b5408b;01b24de15cf337c55b9866c4b534596ca3d93abe;7ad66cba3b7e3abae7ef33122588512a146f7f77;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;88816ae492956f3004daa41357166f1181c0c1bf;22adb2413901b74128f2a02584dafa77afbd8d60|143750713;1738575;47653392|False;True;True|desc;desc;desc
0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa|10.18653/V1/E17-1042|The Lancet|92.0|6789-0123_92|438-449|Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.||||A Network-based End-to-End Trainable Task-oriented Dialogue System|2016|1388702112;51175233;3334541;2131709;2295429;144256365;145259603;92480907|L. Rojas-Barahona;M. Gašić;N. Mrksic;Pei-hao Su;Stefan Ultes;Tsung-Hsien Wen;S. Young;David Vandyke|Computer Science;Mathematics|6ec7c724aa1d906e9e9f81c58497adddb22175b8;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;7380e343dd4547e21d5118b16daf03d021d98c4e|144402064;33918804;2448692|True;True;True|desc;desc;desc
93884d89dfc8c3886f642018227a43fb7b58044f|10.1109/TKDE.2022.3178128||||8052-8072|Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.|AAAI|AAAI_Rome_2021|Rome|Generalizing to Unseen Domains: A Survey on Domain Generalization|2021|1519290245;40093162;2144545128;1796267433;143826491|Jindong Wang;Cuiling Lan;Chang Liu;Yidong Ouyang;Tao Qin|Computer Science|d04d6db5f0df11d0cff57ec7e15134990ac07a4f;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;bcce96a2a074448953fc61a29a84afbdfc8db55a;3b7d120c0e801ef318bc9c607a0789f175637c7f;cc5afe344cc7ed7acd68a28b9774ea8023a162dc;9e475a514f54665478aac6038c262e5a6bac5e64;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;01f702f8b1f9d1314587015f1f038af4d5735e77;b3de1062d8a462dfdc2938558258f8884abe9f4e;a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|2143711163;2064366548;1749827|True;True;True|desc;desc;desc
402f850dff86fb601d34b2841e6083ac0f928edd|10.1145/3079856.3080254||||27-40|Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7× and 2.3×, respectively, over a comparably provisioned dense CNN accelerator.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_London_2017|London|SCNN: An accelerator for compressed-sparse convolutional neural networks|2017|1790421;1998820;3374545;2274681;3172075;2125244;1775477;1715863;80724002|A. Parashar;Minsoo Rhu;Anurag Mukkara;A. Puglielli;Rangharajan Venkatesan;Brucek Khailany;J. Emer;S. Keckler;W. Dally|Computer Science;Engineering|b16408a97170785fb216c9e8b7920d64f478fbc8;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;9691f67f5075bde2fd70da0135a4a70f25ef042b;f86f1748d1b6d22870f4347fd5d65314ba800583;06645d735b59b14479ae1d0392136bbf44227d0f;398c296d0cc7f9d180f84969f8937e6d3a413796;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;f9d119346b0773ea83251598fa5305bc75bac8ab;7380e343dd4547e21d5118b16daf03d021d98c4e;d517b13f2b152c913b81ce534a149493517dbdad;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7|1694551096;2262448783;144189388|True;True;True|desc;desc;desc
d517b13f2b152c913b81ce534a149493517dbdad|10.1109/ACCESS.2014.2325029|Radiology|99.0|0123-4567_99|514-525|Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.||||Big Data Deep Learning: Challenges and Perspectives|2014|2145447101;39376164|Xue-wen Chen;Xiaotong Lin|Computer Science|5c45a5d05ac564adb67811eeb9d41d6460c70135;8515a302b8f389f8f1008cc2650e5ec0a6913e24;8c8215b7f8111839f0066010a530a3a9f57ba15e;36652428740cd30d245d55889f01a7fb04a91c93;0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;6aae0dc122102693e8136856ffc8b72df7f78386;395de0bd3837fdf4b4b5e5f04835bcc69c279481;91c380406f5a862b5937e70e720802e5c787968d;fbf1c51548ffc9b9e538befcd71529365af23d15;fee4db01f6f981931dfd87376a8f861353d1e494;3cee40494377c0e7d9c7c23a3811b481e55bce39;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;e9126a98de0c39dcffe4c4f5158e037460196724;a206216c3f67605ac6e25b0178c3f156dc0f7ba0|144402064;2108989415;1717452|True;True;True|desc;desc;desc
4e6238c8613b5b81f81552939bce33296aedfbfe|10.5555/1756006.1859912|PNAS|85.0|4567-8901_85|1803-1831|After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.||||How to Explain Individual Classification Decisions|2009|1742375;34954622;1734990;1716788;39960184;145034054|D. Baehrens;T. Schroeter;S. Harmeling;M. Kawanabe;K. Hansen;K. Müller|Computer Science;Mathematics|2077d0f30507d51a0d3bbec4957d55e817d66a59;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;d0c882bcae6531fa13e75bcc5c297b9985f207f7;1c00df1cb85fa7886b6666599eab59f2b301dd5d;075f328ef87a076151feb4d5b1f97b66aa597a90;d12864a8acbab1830be755bfb9cb177e31ca5e20;b10e4deadf978d8fd6eec97ff18888629f4261ab;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;5cbe278b65a81602a864184bbca37de91448a5f5;5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7;d05d86db86a4ac0d95e6dcd951b42a9651939793;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;1e41ed1ac234cba0138329047e16a8a424389e77|2475428;1974599;34789794|True;True;True|desc;desc;desc
22fe619996b59c09cb73be40103a123d2e328111|10.1109/IJCNN.2011.6033395||||1453-1460|The “German Traffic Sign Recognition Benchmark” is a multi-category classification competition held at IJCNN 2011. Automatic recognition of traffic signs is required in advanced driver assistance systems and constitutes a challenging real-world computer vision and pattern recognition problem. A comprehensive, lifelike dataset of more than 50,000 traffic sign images has been collected. It reflects the strong variations in visual appearance of signs due to distance, illumination, weather conditions, partial occlusions, and rotations. The images are complemented by several precomputed feature sets to allow for applying machine learning algorithms without background knowledge in image processing. The dataset comprises 43 classes with unbalanced class frequencies. Participants have to classify two test sets of more than 12,500 images each. Here, the results on the first of these sets, which was used in the first evaluation stage of the two-fold challenge, are reported. The methods employed by the participants who achieved the best results are briefly described and compared to human traffic sign recognition performance and baseline results.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Moscow_2011|Moscow|The German Traffic Sign Recognition Benchmark: A multi-class classification competition|2011|69539592;2502317;2743486;1748824|J. Stallkamp;Marc Schlipsing;J. Salmen;C. Igel|Computer Science;Engineering|01b24de15cf337c55b9866c4b534596ca3d93abe;e9126a98de0c39dcffe4c4f5158e037460196724;9b0dd87208a03e78105491e3727213b9b8ac0419;0090023afc66cd2741568599057f4e82b566137c;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;9f387ce140c59a44eaeeea590087351461345164;c43025c429b1fbf6f1379f61801a1b40834d62e7;184ac0766262312ba76bbdece4e7ffad0aa8180b;dd971c07879e1ce12b06991319528c06280eeb9b;0023582fde36430c7e3ae81611a14e558c8f4bae|145606181;2053884612;1716788|True;True;False|desc;desc;desc
d05d86db86a4ac0d95e6dcd951b42a9651939793|10.1109/ACCESS.2019.2895334||||41525-41550|Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01–0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Paris_2019|Paris|Deep Learning Approach for Intelligent Intrusion Detection System|2019|51162051;2474250;2285465435;2916235;1399133087;145710905|R. Vinayakumar;M. Alazab;I. K. P. S. Senior Member;P. Poornachandran;Ameer Al-Nemrat;S. Venkatraman|Computer Science;Engineering|fa25610fb8586c2b50a3654edc5bb42fa7fc4729;01f702f8b1f9d1314587015f1f038af4d5735e77|2070956511;1699645;2146245769|True;True;True|desc;desc;desc
877374c2913b787ee9f958f39e31c75d39ebcc15|10.1109/MSP.2012.2183771|JAMA|8.0|7890-1234_8|101-116|The ever-increasing demand for higher data rates in wireless communications in the face of limited or underutilized spectral resources has motivated the introduction of cognitive radio. Traditionally, licensed spectrum is allocated over relatively long time periods and is intended to be used only by licensees. Various measurements of spectrum utilization have shown substantial unused resources in frequency, time, and space [1], [2]. The concept behind cognitive radio is to exploit these underutilized spectral resources by reusing unused spectrum in an opportunistic manner [3], [4]. The phrase cognitive radio is usually attributed to Mitola [4], but the idea of using learning and sensing machines to probe the radio spectrum was envisioned several decades earlier (cf., [5]).||||Spectrum Sensing for Cognitive Radio : State-of-the-Art and Recent Advances|2012|2983295;143668698;1701766;145967056|E. Axell;G. Leus;E. Larsson;H. Poor|Computer Science;Engineering|df40ce107a71b770c9d0354b78fdd8989da80d2f;831edc3d67457db83da40d260e93bfd7559347ae;8515a302b8f389f8f1008cc2650e5ec0a6913e24;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;398c296d0cc7f9d180f84969f8937e6d3a413796;04fd278c01df1564e741b4c6e052fc1c5924ab8d|2075675;1792616;144902513|True;True;True|desc;desc;desc
b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57|10.14778/2212351.2212354||||716-727|"While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. 
 
We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations."|Workshop on Machine Learning for Health|Workshop_on_Machine_Learning_for_Health_Cape_Town_2012|Cape Town|Distributed GraphLab: A Framework for Machine Learning in the Cloud|2012|1680638;2119113835;1717990;1741745;1730156;1695576|Yucheng Low;Joseph Gonzalez;Aapo Kyrola;Danny Bickson;Carlos Guestrin;J. Hellerstein|Computer Science|d0ab11de3077490c80a08abd0fb8827bac84c454;b24972552161cd9eda729e748762a73430983e3a;602f31242e577d2d05f918a3080fd50095e7faed;24e6c5bfe9bb0751e5708b501d04e860011b2953;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;f3203d0bdefc9670ed508ca776d08aa9f024bafa|39943835;1684887;1699054|True;True;True|desc;desc;desc
771ca13f78a6cfda9ed99004a386e9e7e187bd34|10.3115/1119355.1119383|NEJM|50.0|9012-3456_50|216-223|In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the PoS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.||||Improved Automatic Keyword Extraction Given More Linguistic Knowledge|2003|47571286|A. Hulth|Computer Science;Linguistics|694bdf6e5906992dad2987a3cc8d1a176de691c9;86cff4d050beb90fed2e1ceac8940c8221b120aa;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;76f560991d56ad689ec32f9e9d13291e0193f4cf;5cbe278b65a81602a864184bbca37de91448a5f5;8c8215b7f8111839f0066010a530a3a9f57ba15e;01f29addca4dc6f189f903cb133dea7585813a6f;a34e35dbbc6911fa7b94894dffdc0076a261b6f0;10f919b1a5161b560504c225cfb2d1b3a4768f80;1c1ea4eaf2c5ec2fb55debcbfa2bc8c07a821435;c6bbfb4fcaecc779c899af4bb52083870f4b996a;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;d133cb102ad0f81e3fd17a7db090b28afc124c4a;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;2b7f9117eb6608a58be4c078ca3d69c0e5ccb875;22fe619996b59c09cb73be40103a123d2e328111|1744726;2346817245;48342565|False;True;True|desc;desc;desc
441c31274f4535a4a50892c1ad6e19eacfd17f8c|10.1063/1.4966192||||"
          170901
        "|Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.|IROS|IROS_Cairo_2016|Cairo|Perspective: Machine learning potentials for atomistic simulations.|2016|144136091|J. Behler|Chemistry;Computer Science;Medicine;Materials Science|0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376;033f25ad905ef2ed32a8331cf38b83953ff15922|38550277;47156522;5791678|False;True;False|desc;desc;desc
9a12ac1e3b51f30042abdadc5636c88d41bf0ca7|10.1145/313238.313437|Science|52.0|3456-7890_52|254-255|Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machinelearning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea’s effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available.||||KEA: practical automatic keyphrase extraction|1999|9419406;2171407;1767318;1693768;1389909404|I. Witten;G. Paynter;E. Frank;C. Gutwin;C. Nevill-Manning|Computer Science|3df952d4a724655f7520ff95d4b2cef90fff0cae;0b544dfe355a5070b60986319a3f51fb45d1348e;38f23fe236b152cd4983c8f30d305a568afd0d3e;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;220ac48a22547a455d05f416e1fd22bbd0b0788d;5794141889d0e994c3103b0aaab08a18222c9c43;61e27dbae190b82639c57f180ecf97e4c46fcad9;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;0c3751db5a24c636c1aa8abfd9d63321b38cfce5;c6a83c4fcc99ba6753109301949c5b7cfa978079|2451356;3115341;2108348149|True;True;True|desc;desc;desc
cbac8b0d82ea8e9251d5530695841d816cb196b9|10.21105/JOSS.01026||||1026|Python is currently the fastest growing programming language in the world, thanks to its ease-of-use, fast learning curve and its numerous high quality packages for data science and machine-learning. Surprisingly however, Python is far behind the R programming language when it comes to general statistics and for this reason many scientists still rely heavily on R to perform their statistical analyses.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2018|New York|Pingouin: statistics in Python|2018|2095214091|Raphael Vallat|Computer Science;Mathematics||50695457;1697397;2152472162|True;True;True|desc;desc;desc
abe8a57dc27598937c2cffde3fc21c1e6d1f11ce|10.1109/JBHI.2017.2767063||||1589-1604|The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research.|ACL|ACL_Madrid_2017|Madrid|Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis|2017|3383528;40449634;5484714;1715006|B. Shickel;P. Tighe;A. Bihorac;Parisa Rashidi|Computer Science;Medicine;Mathematics|8592e46a5435d18bba70557846f47290b34c1aa5;53834f0ee8df731cf0e629cd594dce0afaaa3d97;771ca13f78a6cfda9ed99004a386e9e7e187bd34;dd971c07879e1ce12b06991319528c06280eeb9b;d98d0d1900b13b87aa4ffd6b69c046beb63f0434;872352b0a53ab6cbb4420f81df64d215d86c7d9b;9f387ce140c59a44eaeeea590087351461345164;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;1592fe924114866c1ac559bae33ea789930daa98;5c7e5248d9eb7f373f10277410bf8506160907ea;441c31274f4535a4a50892c1ad6e19eacfd17f8c|2116899260;2108611657;2114485554|False;False;True|desc;desc;desc
0ba86604228b555475496e200f31878df3aabd6e|10.1145/3191513||||103 - 115|Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.|ECCV|ECCV_Buenos_Aires_2015|Buenos Aires|Never-Ending Learning|2015|40975594;50056360;1842532;2406435;2119660368;31779043;143818235;40135250;40642935;16411658;2517825;1914797;2406799;35645263;3115592;144888672;1863425;32402038;1717452;2108772203;2129412;1726095131;39717886;2407368;2062798496;122360608|Tom Michael Mitchell;William W. Cohen;Estevam Hruschka;Partha P. Talukdar;Bo Yang;J. Betteridge;Andrew Carlson;Bhavana Dalvi;Matt Gardner;B. Kisiel;Jayant Krishnamurthy;N. Lao;Kathryn Mazaitis;Thahir Mohamed;Ndapandula Nakashole;Emmanouil Antonios Platanios;Alan Ritter;M. Samadi;Burr Settles;Richard C. Wang;Derry Tanti Wijaya;A. Gupta;Xinlei Chen;Abulhair Saparov;Malcolm Greaves;Joel Welling|Computer Science;Linguistics;Psychology|53834f0ee8df731cf0e629cd594dce0afaaa3d97;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7;64be9999b68e12d260ba7423f6b55ffd41552ad3;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;f762cc39a824de1360e8223222739aaa4cd4168c;61394599ed0aabe04b724c7ca3a778825c7e776f;a27089efabc5f4abd5ddf2be2a409bff41f31199;db68a79e59291b85e10300b79c43843b651aa195;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;a675fe5a7d99ac6f7ff91fa084462faefe616148;22adb2413901b74128f2a02584dafa77afbd8d60;e838ba98e198d2dac047736e77c50c0efa49c2dc;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895;f86f1748d1b6d22870f4347fd5d65314ba800583;91c380406f5a862b5937e70e720802e5c787968d|145678691;1688882;1741702|False;True;True|desc;desc;desc
adc61e21eafecfbf6ebecc570f9f913659a2bfb2|10.1145/3439726||||1 - 40|Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.|CVPR|CVPR_Rio_de_Janeiro_2020|Rio de Janeiro|Deep Learning--based Text Classification|2020|2164604;49943757;48441311|Shervin Minaee;E. Cambria;Jianfeng Gao|Computer Science;Mathematics|864e7db59f2ccfec1ee9f6eba79566ac7b0634df;12d1d070a53d4084d88a77b8b143bad51c40c38f;61394599ed0aabe04b724c7ca3a778825c7e776f;739769f4862753fc80057194456d758d2a148ee3;2077d0f30507d51a0d3bbec4957d55e817d66a59;3fa5f45ddbd5184f10bfb92e367493c5a344f207;b3de1062d8a462dfdc2938558258f8884abe9f4e;a244c47a1d4a8c2894b22807df8c7eec16cc110a;5a391667242b4a631acdd5917681b16a86523987;0090023afc66cd2741568599057f4e82b566137c;8d1c588d202f150e1797ed113fba7e67bfa43ecb;1a827052f01ef830cbc849c71e9da99791243a5f;78989616eeeac55b202e3e4205225e7135054185;9691f67f5075bde2fd70da0135a4a70f25ef042b;03cb4e2cb669d3f6344a733e622f07909f87ff0a|34660073;1832448;143649421|True;True;False|desc;desc;desc
94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b|10.1145/3472291||||1 - 40|Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.|CVPR|CVPR_Cape_Town_2020|Cape Town|A Survey of Deep Active Learning|2020|51056374;39924919;144950946;2319973;49969948;2466164;2153688316|Pengzhen Ren;Yun Xiao;Xiaojun Chang;Po-Yao (Bernie) Huang;Zhihui Li;Xiaojiang Chen;Xin Wang|Computer Science;Mathematics|e838ba98e198d2dac047736e77c50c0efa49c2dc;f354310098e09c1e1dc88758fca36767fd9d084d;b9518627db25f05930e931f56497602363a75491;afa778ba0ba6333e25671cfb691a4bdda13b2868;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;dd971c07879e1ce12b06991319528c06280eeb9b;fbc913faf39b1e369dfcdcfefb354d846a46573c;d422df8bff4e677a3077635db116679d25142bfc;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;e7e25fd534e9e024da329aea546484938df305a5;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;db68a79e59291b85e10300b79c43843b651aa195|2225350;47197693;2279559513|False;True;True|desc;desc;desc
c6a83c4fcc99ba6753109301949c5b7cfa978079|10.1162/0891201042544884||||417-449|A phrase-based statistical machine translation approach the alignment template approach is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various system components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.|ICML|ICML_Rio_de_Janeiro_2004|Rio de Janeiro|The Alignment Template Approach to Statistical Machine Translation|2004|2002316;145322333|F. Och;H. Ney|Computer Science;Linguistics|b8012351bc5ebce4a4b3039bbbba3ce393bc3315;2bc3644ce4de7fce5812c1455e056649a47c1bbf;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;9d75cc322a4e06d0a3a868cb91b04219a289c12c;c43025c429b1fbf6f1379f61801a1b40834d62e7;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;12439a6ff384e95ee2262ee982bc055534e30487;e7e25fd534e9e024da329aea546484938df305a5;739769f4862753fc80057194456d758d2a148ee3;2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7;d98d0d1900b13b87aa4ffd6b69c046beb63f0434|2127057;6593936;2054745187|False;True;True|desc;desc;desc
01f29addca4dc6f189f903cb133dea7585813a6f|10.1109/COMST.2021.3077737||||1546-1577|Reconfigurable intelligent surfaces (RISs), also known as intelligent reflecting surfaces (IRSs), or large intelligent surfaces (LISs),1 have received significant attention for their potential to enhance the capacity and coverage of wireless networks by smartly reconfiguring the wireless propagation environment. Therefore, RISs are considered a promising technology for the sixth-generation (6G) of communication networks. In this context, we provide a comprehensive overview of the state-of-the-art on RISs, with focus on their operating principles, performance evaluation, beamforming design and resource management, applications of machine learning to RIS-enhanced wireless networks, as well as the integration of RISs with other emerging technologies. We describe the basic principles of RISs both from physics and communications perspectives, based on which we present performance evaluation of multiantenna assisted RIS systems. In addition, we systematically survey existing designs for RIS-enhanced wireless networks encompassing performance analysis, information theory, and performance optimization perspectives. Furthermore, we survey existing research contributions that apply machine learning for tackling challenges in dynamic scenarios, such as random fluctuations of wireless channels and user mobility in RIS-enhanced wireless networks. Last but not least, we identify major issues and research opportunities associated with the integration of RISs and other emerging technologies for applications to next-generation networks.1Without loss of generality, we use the name of RIS in the remainder of this paper.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Madrid_2020|Madrid|Reconfigurable Intelligent Surfaces: Principles and Opportunities|2020|47909642;2111310469;7837350;49097095;2047464842;121644245;1390094118|Yuanwei Liu;Xiao Liu;Xidong Mu;Tianwei Hou;Jiaqi Xu;M. Di Renzo;N. Al-Dhahir|Computer Science;Engineering|c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;b5887d18420e8ac4f4fa4c83c4952138fd956702;3aa1b70fdc97ae96091c5fb39cd911015ac5253e;9b0dd87208a03e78105491e3727213b9b8ac0419|51162051;1763295;145322333|False;True;True|desc;desc;desc
3df952d4a724655f7520ff95d4b2cef90fff0cae|10.1145/3359786||||68 - 77|Uncovering the mysterious ways machine learning models make decisions.|NIPS|NIPS_Buenos_Aires_2018|Buenos Aires|Techniques for interpretable machine learning|2018|3432460;47717322;48539382|Mengnan Du;Ninghao Liu;Xia Hu|Computer Science;Mathematics|dd9b99fac67c18be82d7763a8fbf231fc3512423;872bae24c109f7c30e052ac218b17a8b028d08a0;885af28a751553be48a25b411a5d492767d4cf65;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;602f31242e577d2d05f918a3080fd50095e7faed;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;a244c47a1d4a8c2894b22807df8c7eec16cc110a;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;872352b0a53ab6cbb4420f81df64d215d86c7d9b;e7e25fd534e9e024da329aea546484938df305a5|134712615;1702392;2062798496|False;True;True|desc;desc;desc
c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7|10.1109/CSF.2018.00027|BMJ|52.0|8901-2345_52|268-282|Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.||||Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting|2017|26378728;3025831;2623167;1680133|Samuel Yeom;Irene Giacomelli;Matt Fredrikson;S. Jha|Computer Science|08b43d84e6747e370ef307e2ada50675b414514a;e4a85af3f5dc41e13dc2cae9ee851953709b764e;b954efe5e46b8952f5a8daf42e7e535119b5408b|6289332;46502933;144131273|False;True;True|desc;desc;desc
66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593|10.1093/nar/gkab447||||W317 - W325|Abstract Gene set enrichment (GSE) analysis plays an essential role in extracting biological insight from genome-scale experiments. ORA (overrepresentation analysis), FCS (functional class scoring), and PT (pathway topology) approaches are three generations of GSE methods along the timeline of development. Previous versions of KOBAS provided services based on just the ORA method. Here we presented version 3.0 of KOBAS, which is named KOBAS-i (short for KOBAS intelligent version). It introduced a novel machine learning-based method we published earlier, CGPS, which incorporates seven FCS tools and two PT tools into a single ensemble score and intelligently prioritizes the relevant biological pathways. In addition, KOBAS has expanded the downstream exploratory visualization for selecting and understanding the enriched results. The tool constructs a novel view of cirFunMap, which presents different enriched terms and their correlations in a landscape. Finally, based on the previous version's framework, KOBAS increased the number of supported species from 1327 to 5944. For an easier local run, it also provides a prebuilt Docker image that requires no installation, as a supplementary to the source code version. KOBAS can be freely accessed at http://kobas.cbi.pku.edu.cn, and a mirror site is available at http://bioinfo.org/kobas.|ICAPS|ICAPS_Madrid_2021|Madrid|KOBAS-i: intelligent prioritization and exploratory visualization of biological functions for gene enrichment analysis|2021|1934073;47030051;2031130914;2108348149;2116575879;2040814523;2149122113;34990576;2108434617;2157957189;134712615;2031130601;2107009096;2109916949;145336368|Dechao Bu;Haitao Luo;Peipei Huo;Zhihao Wang;Shan Zhang;Zihao He;Yang Wu;Lianhe Zhao;Jingjia Liu;Jincheng Guo;Shuangsang Fang;Wanchen Cao;Lan Yi;Yi Zhao;Lei Kong|Computer Science;Medicine;Biology|f86f1748d1b6d22870f4347fd5d65314ba800583;bd898f483476e3dcacf83cd85efc64e6319da0e1;184ac0766262312ba76bbdece4e7ffad0aa8180b;5c45a5d05ac564adb67811eeb9d41d6460c70135;872352b0a53ab6cbb4420f81df64d215d86c7d9b;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5;402f850dff86fb601d34b2841e6083ac0f928edd|5769290;145658292;32721322|True;False;True|desc;desc;desc
33e46a618fdb22d46951f548d6ceeb384e7f1687|10.1109/TPAMI.2008.75||||1713-1727|We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_London_2008|London|Pedestrian Detection via Classification on Riemannian Manifolds|2008|2577513;29905643;145776090|Oncel Tuzel;F. Porikli;P. Meer|Computer Science;Medicine;Mathematics|1a827052f01ef830cbc849c71e9da99791243a5f;815c84ab906e43f3e6322f2ca3fd5e1360c64285;6adf016e7531c91100d3cf4a74f5d4c87b26b528;45557cc70cd6989ab6b03e5aeb787e34299099f7;9071775ebcfebddd54d879fe7e6c627673e4d305;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;a206216c3f67605ac6e25b0178c3f156dc0f7ba0;2d2fe4a73c98933ae9b8df73c8452b0d8be6475e;0ca26f9a98dda0abb737692f72ffa682df14cb2f;78989616eeeac55b202e3e4205225e7135054185;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;0023582fde36430c7e3ae81611a14e558c8f4bae;1e41ed1ac234cba0138329047e16a8a424389e77|2066145;49933077;145678691|True;True;False|desc;desc;desc
2bc3644ce4de7fce5812c1455e056649a47c1bbf|10.1109/ACCESS.2019.2923707||||81542-81554|Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).|Workshop on AI for Earth|Workshop_on_AI_for_Earth_Athens_2019|Athens|Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques|2019|150302778;9727014;144369609|Senthilkumar Mohan;Chandrasegar Thirumalai;Gautam Srivastava|Computer Science;Medicine|b24972552161cd9eda729e748762a73430983e3a;d133cb102ad0f81e3fd17a7db090b28afc124c4a;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;9d75cc322a4e06d0a3a868cb91b04219a289c12c;fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4;8db9df2eadea654f128c1887722c677c708e8a47;b16408a97170785fb216c9e8b7920d64f478fbc8;8592e46a5435d18bba70557846f47290b34c1aa5|40397208;2656573;1409068337|True;False;True|desc;desc;desc
d63b884d5ebc739f6e1bdf861fa9276260781404|10.1109/COMST.2018.2844341||||2923-2960|In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Mexico_City_2017|Mexico City|Deep Learning for IoT Big Data and Streaming Analytics: A Survey|2017|4488990;1404786833;1683936;145837053|M. Mohammadi;Ala I. Al-Fuqaha;Sameh Sorour;M. Guizani|Computer Science;Environmental Science;Engineering|0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;5c5be36e3111e42247d78a6d529e4b1d7d2ced12;d133cb102ad0f81e3fd17a7db090b28afc124c4a;46f74231b9afeb0c290d6d550043c55045284e5f;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;2bc3644ce4de7fce5812c1455e056649a47c1bbf;efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea|37535930;2108611657;3037568|True;True;True|desc;desc;desc
b10e4deadf978d8fd6eec97ff18888629f4261ab|10.1111/J.1475-679X.2010.00382.X|Science|44.0|3456-7890_44|1049-1102|"ABSTRACT This paper examines the information content of the forward-looking statements (FLS) in the Management Discussion and Analysis section (MD&A) of 10-K and 10-Q filings using a Naive Bayesian machine learning algorithm. I find that firms with better current performance, lower accruals, smaller size, lower market-to-book ratio, less return volatility, lower MD&A Fog index, and longer history tend to have more positive FLSs. The average tone of the FLS is positively associated with future earnings even after controlling for other determinants of future performance. The results also show that, despite increased regulations aimed at strengthening MD&A disclosures, there is no systematic change in the information content of MD&As over time. In addition, the tone in MD&As seems to mitigate the mispricing of accruals. When managers ""warn"" about the future performance implications of accruals (i.e., the MD&A tone is positive (negative) when accruals are negative (positive)), accruals are not associated with future returns. The tone measures based on three commonly used dictionaries (Diction, General Inquirer, and the Linguistic Inquiry and Word Count) do not positively predict future performance. This result suggests that these dictionaries might not work well for analyzing corporate filings. Copyright (c), University of Chicago on behalf of the Accounting Research Center, 2010."||||The Information Content of Forward-Looking Statements in Corporate Filings—A Naïve Bayesian Machine Learning Approach|2010|2146312811|Feng Li|Computer Science;Economics;Business|9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;07abd02f02774d178f26ca99937e5f94001a9ec9;872352b0a53ab6cbb4420f81df64d215d86c7d9b;3a84214cb69ea0b34352285029f368b75718c32b;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;075f328ef87a076151feb4d5b1f97b66aa597a90;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;63861fbeb7ec41986b85965b9780b428d919919e;4b61c25a86083c20730c9b12737ac6ac4178c364;1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd;48234756b7cf798bfeb47328f7c5d597fd4838c2|2609123;3105254;35363891|True;True;True|desc;desc;desc
64be9999b68e12d260ba7423f6b55ffd41552ad3|10.1109/ACCESS.2017.2788044|JAMA|47.0|7890-1234_47|9375-9389|The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.||||Deep Learning Applications in Medical Image Analysis|2018|34812292;46659335;39917910;48508646|Justin Ker;Lipo Wang;J. Rao;Tchoyoson C. C. Lim|Computer Science;Medicine|6aae0dc122102693e8136856ffc8b72df7f78386;d02927d4de4a2a51cced4970da04b812cbee4342;1dae4d61cd74cc919ecc638bde6b7125728ea97b;c6bbfb4fcaecc779c899af4bb52083870f4b996a;e838ba98e198d2dac047736e77c50c0efa49c2dc;265644f1b6740ca34bfbe9762b90b33021adde62|2792896;7828998;47571286|False;True;False|desc;desc;desc
0c3751db5a24c636c1aa8abfd9d63321b38cfce5|10.5555/2567709.2502598|Science|86.0|3456-7890_86|567-599|Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.||||Stochastic dual coordinate ascent methods for regularized loss|2012|1389955537;2300131692|Shai Shalev-Shwartz;Tong Zhang|Computer Science;Mathematics|01f702f8b1f9d1314587015f1f038af4d5735e77;815c84ab906e43f3e6322f2ca3fd5e1360c64285;6ec7c724aa1d906e9e9f81c58497adddb22175b8;831edc3d67457db83da40d260e93bfd7559347ae;0ba86604228b555475496e200f31878df3aabd6e;9a12ac1e3b51f30042abdadc5636c88d41bf0ca7;62ccd99a65bfc7c735ae1f33b75b107665de95df;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;d05d86db86a4ac0d95e6dcd951b42a9651939793;ac12c9b9e35e58b55d85a97c47886a7371c14afa;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;01b24de15cf337c55b9866c4b534596ca3d93abe;4609f6bdc3beab00c9beceaa12dd8101fefe6f1c;e4a85af3f5dc41e13dc2cae9ee851953709b764e;4a554da55fd9ff76c99e25d2ce937b225dc1100c;8de174ab5419b9d3127695405efd079808e956e8;16c0ef924da1f6b510c9c783ac764156f5a3d631;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;0165568bcc1a819c18564567f2ec15d859be2519|2613438;145197293;11573257|True;True;True|desc;desc;desc
1f87134a630c2dbb9a3645ba658954f00b620a77|10.1109/ICCV.2011.6126251||||263-270|Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (accurate estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we are able to avoid the need for an intermediate classification step. Our method uses a kernelized structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow for real-time application, we introduce a budgeting mechanism which prevents the unbounded growth in the number of support vectors which would otherwise occur during tracking. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased performance.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Madrid_2011|Madrid|Struck: Structured output tracking with kernels|2011|1837057;1741702;143635540|Sam Hare;Amir Saffari;Philip H. S. Torr|Computer Science;Medicine|45557cc70cd6989ab6b03e5aeb787e34299099f7;f4156a05a47fdeda30638e10954d3674cc056ab6;bd898f483476e3dcacf83cd85efc64e6319da0e1;546785490ac417be1f83ced6a8272e934934f411;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd;3cee40494377c0e7d9c7c23a3811b481e55bce39;819167ace2f0caae7745d2f25a803979be5fbfae;4c75b748911ddcd888c5122f7672f69caa5d661f;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c;4157ed3db4c656854e69931cb6089b64b08784b9;65d53938a12c77e7920b8eb3a49df249c978ba3f;3adcfd254b271bcc2fb7e2a62d750db17e6c2c08;398c296d0cc7f9d180f84969f8937e6d3a413796;a40f97770296c7fca2e5361cbceba3f4aae399e0;01b24de15cf337c55b9866c4b534596ca3d93abe;5e095981ebf4d389e9356bd56e59e0ade1b42e88;eebe93355a2efb479d5cb0abda23ccbb4e5c7c9a;38f23fe236b152cd4983c8f30d305a568afd0d3e;a7a407968c13ced804a063259d72315a43b84f29|46691607;48226007;2061819040|True;True;False|desc;desc;desc
265644f1b6740ca34bfbe9762b90b33021adde62|10.3348/kjr.2017.18.4.570|NEJM|93.0|9012-3456_93|570 - 584|The artificial neural network (ANN)–a machine learning technique inspired by the human neuronal synapse system–was introduced in the 1950s. However, the ANN was previously limited in its ability to solve actual problems, due to the vanishing gradient and overfitting problems with training of deep architecture, lack of computing power, and primarily the absence of sufficient data to train the computer system. Interest in this concept has lately resurfaced, due to the availability of big data, enhanced computing power with the current graphics processing units, and novel algorithms to train the deep neural network. Recent studies on this technology suggest its potentially to perform better than humans in some visual and auditory recognition tasks, which may portend its applications in medicine and healthcare, especially in medical imaging, in the foreseeable future. This review article offers perspectives on the history, development, and applications of deep learning technology, particularly regarding its applications in medical imaging.||||Deep Learning in Medical Imaging: General Overview|2017|120704132;2052576935;2116632761;46901084;38628528;46844846;145979410|June-Goo Lee;Sanghoon Jun;Younghoon Cho;Hyunna Lee;G. Kim;J. Seo;Namkug Kim|Computer Science;Medicine|65b16da51891a6b98140d425804c8a0fd0299219;91c380406f5a862b5937e70e720802e5c787968d;b8ebda42e272d3617375118542d4675a0c0e501d;cedea36fa3692281b3ac767335fe49a16d00957d;c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45;04fd278c01df1564e741b4c6e052fc1c5924ab8d;adc61e21eafecfbf6ebecc570f9f913659a2bfb2;24e6c5bfe9bb0751e5708b501d04e860011b2953;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;0ba86604228b555475496e200f31878df3aabd6e;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;fa25610fb8586c2b50a3654edc5bb42fa7fc4729;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;bf7dcbee272428a2aa3c534200743ff7ab2047f8;611544418ca53cdad254df444addc7814abcfddc;2bc3644ce4de7fce5812c1455e056649a47c1bbf;3a84214cb69ea0b34352285029f368b75718c32b|1807250;33221685;144231976|False;True;True|desc;desc;desc
ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2|10.1609/AAAI.V34I05.6311||||8018-8025|Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1|UAI|UAI_Cape_Town_2019|Cape Town|Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment|2019|2068347799;8752221;10638646;1679873|Di Jin;Zhijing Jin;Joey Tianyi Zhou;Peter Szolovits|Computer Science|2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;5794141889d0e994c3103b0aaab08a18222c9c43;220ac48a22547a455d05f416e1fd22bbd0b0788d;65d53938a12c77e7920b8eb3a49df249c978ba3f;53834f0ee8df731cf0e629cd594dce0afaaa3d97;9e475a514f54665478aac6038c262e5a6bac5e64|1765161;51289782;3002426|False;True;False|desc;desc;desc
9257779eed46107bcdce9f4dc86298572ff466ce|10.1145/183422.183423|The Lancet|49.0|6789-0123_49|233-251|We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.||||Automated learning of decision rules for text categorization|1994|145272844;68982679;145700185|C. Apté;Fred J. Damerau;S. Weiss|Computer Science|f762cc39a824de1360e8223222739aaa4cd4168c;12d1d070a53d4084d88a77b8b143bad51c40c38f;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9|143711421;1393935479;1780687|True;True;True|desc;desc;desc
65d53938a12c77e7920b8eb3a49df249c978ba3f|10.1109/TASLP.2020.3030497||||2880-2894|Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.|NAACL|NAACL_Athens_2019|Athens|PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition|2019|8391640;2107994160;35965227;2115828379;144144027;1804703|Qiuqiang Kong;Yin Cao;Turab Iqbal;Yuxuan Wang;Wenwu Wang;Mark D. Plumbley|Computer Science;Engineering|222d3a63d4f81d39ea324530b57328c58f298888;12439a6ff384e95ee2262ee982bc055534e30487;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;fbc913faf39b1e369dfcdcfefb354d846a46573c;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;6aae0dc122102693e8136856ffc8b72df7f78386;7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7;08b43d84e6747e370ef307e2ada50675b414514a;819167ace2f0caae7745d2f25a803979be5fbfae;bb144c04b9eb44579b19d21c3d5954401408440b;872bae24c109f7c30e052ac218b17a8b028d08a0|2264432703;1754252;1748758|True;False;True|desc;desc;desc
e7e25fd534e9e024da329aea546484938df305a5|10.5555/1756006.1953029||||3011-3015|The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Madrid_2010|Madrid|Gaussian Processes for Machine Learning (GPML) Toolbox|2010|3472959;1748758|C. Rasmussen;H. Nickisch|Computer Science;Mathematics|f8b7a3434f887ce4570b7e98c7f1b91c008042d4;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;fbc913faf39b1e369dfcdcfefb354d846a46573c;76f560991d56ad689ec32f9e9d13291e0193f4cf;fd8ce955dc0c570b66305dfbc65e4ed5f37658d0;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;b16408a97170785fb216c9e8b7920d64f478fbc8;3df952d4a724655f7520ff95d4b2cef90fff0cae;ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e|144994682;50260319;2903226|True;False;True|desc;desc;desc
40f391bc3860e8cd71d7ea68ec11d80d5e12c02c|10.5555/2627435.2638581|Science|59.0|3456-7890_59|1455-1459|"Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. 
 
The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. By dealing internally with most of the differential geometry, the package aims particularly at lowering the entrance barrier."||||Manopt, a matlab toolbox for optimization on manifolds|2013|2418520;37585320;50356391;1707966|Nicolas Boumal;Bamdev Mishra;P. Absil;R. Sepulchre|Computer Science;Mathematics;Engineering|24e6c5bfe9bb0751e5708b501d04e860011b2953;8fb1b96dcc133b170e7e5b340f93c5f230d495ee;30b2a3422332a76663110beae4bfc4d74763f4a0;a675fe5a7d99ac6f7ff91fa084462faefe616148;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;f86f1748d1b6d22870f4347fd5d65314ba800583;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;2a3842f6070b4554ff21fe62b2a486657d9a304a;18d026ec5d0eebd17ee2c762da89540c0b3d7bde;d422df8bff4e677a3077635db116679d25142bfc;a7a407968c13ced804a063259d72315a43b84f29;e43ea2d3d6a64c7ddf225f8516d268f28a44c2fd|2878072;2057009;35099951|False;True;True|desc;desc;desc
18bc1d4271abe8dd6e16179cdb06524a4f396e16|10.14778/3157794.3157797||||"
          269-282
        "|Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.|AAAI|AAAI_Moscow_2017|Moscow|Snorkel: Rapid Training Data Creation with Weak Supervision|2017|143711421;2870504;33918804;31592365;144766615;2114485554|Alexander J. Ratner;Stephen H. Bach;Henry R. Ehrenberg;Jason Alan Fries;Sen Wu;C. Ré|Computer Science;Medicine;Mathematics|6adf016e7531c91100d3cf4a74f5d4c87b26b528;16c0ef924da1f6b510c9c783ac764156f5a3d631;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;d0c882bcae6531fa13e75bcc5c297b9985f207f7;8e51d68250db5637cd6bc1de98a99396441399b2;cd49acefc8d51e324aa562e5337e1c2aff067053|2645384;2967405;10208174|True;False;True|desc;desc;desc
9d46dc975aeed3f96bddb144079b50238f746ecd|10.1080/21693277.2016.1192517|JACC|19.0|1234-5678_19|23 - 45|The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.||||Machine learning in manufacturing: advantages, challenges, and applications|2016|2511781;2051446256;1852549;144199043|Thorsten Wuest;Daniel Weimer;C. Irgens;K. Thoben|Computer Science;Engineering|ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;6ec7c724aa1d906e9e9f81c58497adddb22175b8;9b539d413393047b28bb7be9b195f142aaf7a80e;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;222d3a63d4f81d39ea324530b57328c58f298888;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;21cea8f56a0d067d640f923b2d69e18ed5542f6d;c62043a7d2537bbf40a84b9913957452a47fdb83;7ad66cba3b7e3abae7ef33122588512a146f7f77;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;65b16da51891a6b98140d425804c8a0fd0299219;45557cc70cd6989ab6b03e5aeb787e34299099f7|2903226;90583296;3259992|True;True;True|desc;desc;desc
3fa5f45ddbd5184f10bfb92e367493c5a344f207|10.1609/AIMAG.V18I4.1324||||97-136|Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.|ICAPS|ICAPS_Cape_Town_1997|Cape Town|Machine-Learning Research Four Current Directions|1997|144299726|Thomas G. Dietterich|Computer Science;Mathematics|18bc1d4271abe8dd6e16179cdb06524a4f396e16;e0535dedb8607d83cd2614317c99913378e89e26;864e7db59f2ccfec1ee9f6eba79566ac7b0634df;f4156a05a47fdeda30638e10954d3674cc056ab6;d0ab11de3077490c80a08abd0fb8827bac84c454;ea58af907495e97c93997119db4a59fab5cd3683;35b3233e521f1e9a34837c30be1957858f8f35fe|9802604;39924919;2145447101|True;True;True|desc;desc;desc
5a391667242b4a631acdd5917681b16a86523987|10.1109/TGRS.2016.2636241|Cell|12.0|5678-9012_12|3639-3655|In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.||||Deep Recurrent Neural Networks for Hyperspectral Image Classification|2017|35041003;2370080;46875441|Lichao Mou;Pedram Ghamisi;Xiaoxiang Zhu|Computer Science;Environmental Science;Engineering|81a4fd3004df0eb05d6c1cef96ad33d5407820df;5e095981ebf4d389e9356bd56e59e0ade1b42e88;9eb715fe0347445a2d63518cbb476d345ba86233;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;dd9b99fac67c18be82d7763a8fbf231fc3512423;d422df8bff4e677a3077635db116679d25142bfc;a486e2839291111bb44fa1f07731ada123539f75;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;184ac0766262312ba76bbdece4e7ffad0aa8180b|4337102;2346939080;2274681|True;True;True|desc;desc;desc
885af28a751553be48a25b411a5d492767d4cf65|10.1109/TIFS.2011.2175919|JACC|46.0|1234-5678_46|432-444|Today, the most accurate steganalysis methods for digital media are built as supervised classifiers on feature vectors extracted from the media. The tool of choice for the machine learning seems to be the support vector machine (SVM). In this paper, we propose an alternative and well-known machine learning tool-ensemble classifiers implemented as random forests-and argue that they are ideally suited for steganalysis. Ensemble classifiers scale much more favorably w.r.t. the number of training examples and the feature dimensionality with performance comparable to the much more complex SVMs. The significantly lower training complexity opens up the possibility for the steganalyst to work with rich (high-dimensional) cover models and train on larger training sets-two key elements that appear necessary to reliably detect modern steganographic algorithms. Ensemble classification is portrayed here as a powerful developer tool that allows fast construction of steganography detectors with markedly improved detection accuracy across a wide range of embedding methods. The power of the proposed framework is demonstrated on three steganographic methods that hide messages in JPEG images.||||Ensemble Classifiers for Steganalysis of Digital Media|2012|1808384;1751812;37127008|Jan Kodovský;J. Fridrich;Vojtech Holub|Computer Science|4b61c25a86083c20730c9b12737ac6ac4178c364;cc1cad12521b5aab43fdda5b4dec67586aef1f87;220ac48a22547a455d05f416e1fd22bbd0b0788d;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;ac12c9b9e35e58b55d85a97c47886a7371c14afa;a7976c2bacfbb194ddbe7fd10c2e50a545cf4081;e4a85af3f5dc41e13dc2cae9ee851953709b764e;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7|50009466;145658292;143762617|True;True;True|desc;desc;desc
7e7eb0f93c9550d7336f4bbfad5fe89604295705|10.1103/PhysRevLett.122.040504||||"
          040504
        "|A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Buenos_Aires_2018|Buenos Aires|Quantum Machine Learning in Feature Hilbert Spaces.|2018|3048564;3399181|M. Schuld;N. Killoran|Computer Science;Medicine;Physics|8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;0023582fde36430c7e3ae81611a14e558c8f4bae;4b149a326e38b9237077d794a0d5f5b4865efacf;e9126a98de0c39dcffe4c4f5158e037460196724;5ed59f49c1bb7de06cfa2a9467d5efb535103277;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;a1874aafa8730bdd4b28f29d025141c13ee28b58|1764325;6199470;2348306915|True;True;True|desc;desc;desc
85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175|10.1080/00107514.2014.964942||||172 - 185|Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Tokyo_2014|Tokyo|An introduction to quantum machine learning|2014|3048564;2498866;2258749|M. Schuld;I. Sinayskiy;Francesco Petruccione|Computer Science;Physics|d04d6db5f0df11d0cff57ec7e15134990ac07a4f;24e6c5bfe9bb0751e5708b501d04e860011b2953;a486e2839291111bb44fa1f07731ada123539f75;c6a83c4fcc99ba6753109301949c5b7cfa978079;43d2ed5c3c55c1100450cd74dc1031afa24d37b2;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a;0b544dfe355a5070b60986319a3f51fb45d1348e;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1;8515a302b8f389f8f1008cc2650e5ec0a6913e24;b9518627db25f05930e931f56497602363a75491;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;ec76f55da5c6df30f6e4c9e4945bd3304d508ef7|2258061;1485389319;80724002|True;True;False|desc;desc;desc
58a8bead87c8c1e37460dce28285c053c270f6e7|10.1145/2743025||||1 - 41|The advances in location-acquisition and mobile computing techniques have generated massive spatial trajectory data, which represent the mobility of a diversity of moving objects, such as people, vehicles, and animals. Many techniques have been proposed for processing, managing, and mining trajectory data in the past decade, fostering a broad range of applications. In this article, we conduct a systematic survey on the major research into trajectory data mining, providing a panorama of the field as well as the scope of its research topics. Following a road map from the derivation of trajectory data, to trajectory data preprocessing, to trajectory data management, and to a variety of mining tasks (such as trajectory pattern mining, outlier detection, and trajectory classification), the survey explores the connections, correlations, and differences among these existing techniques. This survey also introduces the methods that transform trajectories into other data formats, such as graphs, matrices, and tensors, to which more data mining and machine learning techniques can be applied. Finally, some public trajectory datasets are presented. This survey can help shape the field of trajectory data mining, providing a quick understanding of this field to the community.|ECCV|ECCV_Toronto_2015|Toronto|Trajectory Data Mining|2015|145473095|Yu Zheng|Computer Science;Geography;Engineering|605402e235bd62437baf3c9ebefe77fb4d92ee95;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b;0ba86604228b555475496e200f31878df3aabd6e;eed9fa4483cab37eacd59db0fac4b1441431ee85;c6bbfb4fcaecc779c899af4bb52083870f4b996a;1e41ed1ac234cba0138329047e16a8a424389e77;a675fe5a7d99ac6f7ff91fa084462faefe616148;e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da;3b7d120c0e801ef318bc9c607a0789f175637c7f;d02927d4de4a2a51cced4970da04b812cbee4342;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;ea58af907495e97c93997119db4a59fab5cd3683|1876779;2075675;6486577|True;True;True|desc;desc;desc
1dd6c46d868accd5acffd02e4b08b003534b924e|10.1177/030913339501900403|Science|28.0|3456-7890_28|474 - 499|Predictive vegetation mapping can be defined as predicting the geographic distribution of the vegetation composition across a landscape from mapped environmental variables. Comput erized predictive vegetation mapping is made possible by the availability of digital maps of topography and other environmental variables such as soils, geology and climate variables, and geographic information system software for manipulating these data. Especially important to predictive vegetation mapping are interpolated climatic variables related to physiological tolerances, and topographic variables, derived from digital elevation grids, related to site energy and moisture balance. Predictive vegetation mapping is founded in ecological niche theory and gradient analysis, and driven by the need to map vegetation patterns over large areas for resource conservation planning, and to predict the effects of environmental change on vegetation distributions. Predictive vegetation mapping has advanced over the past two decades especially in conjunction with the development of remote sensing-based vegetation mapping and digital geographic information analysis. A number of statistical and, more recently, machine-learning methods have been used to develop and implement predictive vegetation models.||||Predictive vegetation mapping: geographic modelling of biospatial patterns in relation to environmental gradients|1995|144735103|J. Franklin|Environmental Science;Geography|f4a5503783487eba5c5e34b1d02c09016b244b1d;f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d;df40ce107a71b770c9d0354b78fdd8989da80d2f;f354310098e09c1e1dc88758fca36767fd9d084d;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;f86f1748d1b6d22870f4347fd5d65314ba800583;24e6c5bfe9bb0751e5708b501d04e860011b2953;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;19e8869f4c29353de0d9b52542c1fe9def4cbc7d;48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016;6981ea66000e2c98f8a81f4bef05802234d986a4;ea58af907495e97c93997119db4a59fab5cd3683;9257779eed46107bcdce9f4dc86298572ff466ce;398c296d0cc7f9d180f84969f8937e6d3a413796;1051280d2b825c04f27d231aba0f8284bb297880;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d|144537437;9215658;144945435|True;False;True|desc;desc;desc
db68a79e59291b85e10300b79c43843b651aa195|10.1080/01431160512331314083||||1007 - 1011|Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi‐class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat‐7 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high‐dimensional data.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_2005|Lisbon|Support vector machines for classification in remote sensing|2005|3639504;34684035|M. Pal;P. Mather|Computer Science;Environmental Science|66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;1c00df1cb85fa7886b6666599eab59f2b301dd5d;dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;78947497cbbffc691aac3f590d972130259af9ce;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;f4156a05a47fdeda30638e10954d3674cc056ab6;c1e25bb0c3d0c3a8c3b2bf8e9735a9aadc175b45;a20bfec3c95aad003dcb45a21a220c19cca8bb66;b954efe5e46b8952f5a8daf42e7e535119b5408b;d02927d4de4a2a51cced4970da04b812cbee4342;86f0b58404a264a6216e29c78a5c113d900ca461|49933077;48598873;49754061|True;True;False|desc;desc;desc
bf7dcbee272428a2aa3c534200743ff7ab2047f8|10.1177/0037549705058073||||517 - 527|MASON is a fast, easily extensible, discrete-event multi-agent simulation toolkit in Java, designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments. MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run. This paper describes the MASON system, its motivation, and its basic architectural design. It then compares MASON to related multi-agent libraries in the public domain, and discusses six applications of the system built over the past year which suggest its breadth of utility.|ICML|ICML_Berlin_2005|Berlin|MASON: A Multiagent Simulation Environment|2005|1706276;1403820685;1703826;153850291;35149981|S. Luke;C. Cioffi-Revilla;Liviu Panait;Keith Sullivan;G. Balan|Computer Science;Engineering|ec6200bdcc23b79a71555962cde50306c4029f1a;9eb715fe0347445a2d63518cbb476d345ba86233;81a4fd3004df0eb05d6c1cef96ad33d5407820df;9f387ce140c59a44eaeeea590087351461345164;a6f835ca6e12245a835ab6074bc6ec2c3c60b85a;6adf016e7531c91100d3cf4a74f5d4c87b26b528;38f23fe236b152cd4983c8f30d305a568afd0d3e;bcce96a2a074448953fc61a29a84afbdfc8db55a;6a775ba9287f33ba62a7a4f353dd1e0a77aa236a;f04df4e20a18358ea2f689b4c129781628ef7fc1;8e51d68250db5637cd6bc1de98a99396441399b2;48e752c719d33ff55b3b3bec3538727f8ce69399;a1874aafa8730bdd4b28f29d025141c13ee28b58;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;09622b0c84bf812814af5b64b0c83dce796899c4;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;45557cc70cd6989ab6b03e5aeb787e34299099f7|1857137;2652207;153570946|False;True;False|desc;desc;desc
f9d119346b0773ea83251598fa5305bc75bac8ab|10.1109/JSTARS.2015.2388577||||2381-2392|Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_London_2015|London|Spectral–Spatial Classification of Hyperspectral Data Based on Deep Belief Network|2015|2597809;2143711163;144787387|Yushi Chen;Xing Zhao;X. Jia|Computer Science;Environmental Science;Engineering||1863425;145394689;1748758|True;True;True|desc;desc;desc
1592fe924114866c1ac559bae33ea789930daa98|10.7551/mitpress/3206.001.0001|NEJM|12.0|9012-3456_12|I-XVIII, 1-248|Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes.||||Gaussian Processes for Machine Learning|2005|2247411478;2248834664|Carl E. Rasmussen;Christopher K. I. Williams|Computer Science;Mathematics|37a67228271527037c9250ae3fd220199275e42e;36bca41eba5a7cea8d69a89ee7bc24923bc380ba;4c75b748911ddcd888c5122f7672f69caa5d661f;771479c18b586eafae21baf262a220aaa7b2eef6;a88b3be9b2db0319f8880e60a131b3060dba1eb7;a9763afda62e960c35c80681f805ddecbef14a92;eed9fa4483cab37eacd59db0fac4b1441431ee85;e50f4d3316d13841c287dcdf5479d7820d593571;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;0e90a73f03902cbe915af1aff54ea7f0b3373680;93884d89dfc8c3886f642018227a43fb7b58044f;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92|3083169;46659335;8909922|False;True;True|desc;desc;desc
a244c47a1d4a8c2894b22807df8c7eec16cc110a|10.1109/TNN.2003.820556||||"
          1506-18
        "|A novel type of learning machine called support vector machine (SVM) has been receiving increasing interest in areas ranging from its original application in pattern recognition to other applications such as regression estimation due to its remarkable generalization performance. This paper deals with the application of SVM in financial time series forecasting. The feasibility of applying SVM in financial forecasting is first examined by comparing it with the multilayer back-propagation (BP) neural network and the regularized radial basis function (RBF) neural network. The variability in performance of SVM with respect to the free parameters is investigated experimentally. Adaptive parameters are then proposed by incorporating the nonstationarity of financial time series into SVM. Five real futures contracts collated from the Chicago Mercantile Market are used as the data sets. The simulation shows that among the three methods, SVM outperforms the BP neural network in financial forecasting, and there are comparable generalization performance between SVM and the regularized RBF neural network. Furthermore, the free parameters of SVM have a great effect on the generalization performance. SVM with adaptive parameters can both achieve higher generalization performance and use fewer support vectors than the standard SVM in financial forecasting.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Los_Angeles_2003|Los Angeles|Support vector machine with adaptive parameters in financial time series forecasting|2003|50260319;144869088|Lijuan Cao;F. Tay|Computer Science;Medicine;Business|9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;85d727b119304dde458bcd8cf5cb87a906fb41ba;2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0;9d75cc322a4e06d0a3a868cb91b04219a289c12c;48e752c719d33ff55b3b3bec3538727f8ce69399;a86171e13f84fe32212dd7fb6a1c31a34a47155f;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;61394599ed0aabe04b724c7ca3a778825c7e776f|40221187;2373952;39682944|True;True;True|desc;desc;desc
7ea35b35392c6ef5738635cec7d17b24fe3e4f04|10.1093/nsr/nwy108||||74 - 86|Abstract Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation.|Workshop on AI for Earth|Workshop_on_AI_for_Earth_Cape_Town_2017|Cape Town|Deep forest|2017|145624000;2108993600|Zhi-Hua Zhou;Ji Feng|Computer Science;Medicine;Mathematics|e4a85af3f5dc41e13dc2cae9ee851953709b764e;b954efe5e46b8952f5a8daf42e7e535119b5408b;01f29addca4dc6f189f903cb133dea7585813a6f|1403820685;1878461;144126414|False;True;True|desc;desc;desc
971766088dfaf63fb55e6f0190b14f28f2c98ad0|10.1109/COMST.2015.2494502|PNAS|19.0|4567-8901_19|1153-1176|This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.||||A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection|2016|2343019;1922657|A. Buczak;Erhan Guven|Computer Science;Engineering|831edc3d67457db83da40d260e93bfd7559347ae;884895a86fe15cb9601df4a15a1475c07f28da3c;1626c940a64ad96a7ed53d7d6c0df63c6696956b;5c5e69387020d7ca7d49487ca841958dc5e08ce6;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;739769f4862753fc80057194456d758d2a148ee3;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;2e55ba6c97ce5eb55abd959909403fe8da7e9fe9;ea58af907495e97c93997119db4a59fab5cd3683;5966d7c7f60898d610812e24c64d4d57855ad86a;4a554da55fd9ff76c99e25d2ce937b225dc1100c;4c75b748911ddcd888c5122f7672f69caa5d661f|1410457573;2793728;2220290014|False;True;True|desc;desc;desc
2f7f5d0e989c74d6279e2620e10e8d0b0c021cb7|10.1109/ICDMW.2010.172||||170-177|S4 is a general-purpose, distributed, scalable, partially fault-tolerant, pluggable platform that allows programmers to easily develop applications for processing continuous unbounded streams of data. Keyed data events are routed with affinity to Processing Elements (PEs), which consume the events and do one or both of the following: (1) emit one or more events which may be consumed by other PEs, (2) publish results. The architecture resembles the Actors model, providing semantics of encapsulation and location transparency, thus allowing applications to be massively concurrent while exposing a simple programming interface to application developers. In this paper, we outline the S4 architecture in detail, describe various applications, including real-life deployments. Our design is primarily driven by large scale applications for data mining and machine learning in a production environment. We show that the S4 design is surprisingly flexible and lends itself to run in large clusters built with commodity hardware.|ECCV|ECCV_Los_Angeles_2010|Los Angeles|S4: Distributed Stream Computing Platform|2010|2115641;2064096900;2144011217;2073139181|L. Neumeyer;B. Robbins;Anish Nair;Anand Kesari|Computer Science;Engineering|94549a171a61039ed1f9b5954ce42181c574ccc3;0ca26f9a98dda0abb737692f72ffa682df14cb2f;f4156a05a47fdeda30638e10954d3674cc056ab6;ac3ccaf58ba543fd8d7b787cf939e55345b1659f;3df952d4a724655f7520ff95d4b2cef90fff0cae;2878d9936f494ed7d0c8aec47e9bcc5e51609f9a|1807998;50397921;145624000|True;True;True|desc;desc;desc
22adb2413901b74128f2a02584dafa77afbd8d60|10.1109/TII.2018.2864759|Frontiers for Young Minds|69.0|2345-6789_69|2446-2455|We develop a novel deep learning framework to achieve highly accurate machine fault diagnosis using transfer learning to enable and accelerate the training of deep neural network. Compared with existing methods, the proposed method is faster to train and more accurate. First, original sensor data are converted to images by conducting a Wavelet transformation to obtain time-frequency distributions. Next, a pretrained network is used to extract lower level features. The labeled time-frequency images are then used to fine-tune the higher levels of the neural network architecture. This paper creates a machine fault diagnosis pipeline and experiments are carried out to verify the effectiveness and generalization of the pipeline on three main mechanical datasets including induction motors, gearboxes, and bearings with sizes of 6000, 9000, and 5000 time series samples, respectively. We achieve state-of-the-art results on each dataset, with most datasets showing test accuracy near 100%, and in the gearbox dataset, we achieve significant improvement from 94.8% to 99.64%. We created a repository including these datasets located at mlmechanics.ics.uci.edu.||||Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning|2019|9122533;143953836;35374692;144902513|Siyu Shao;S. McAleer;Ruqiang Yan;P. Baldi|Computer Science;Engineering|033f25ad905ef2ed32a8331cf38b83953ff15922;f9d119346b0773ea83251598fa5305bc75bac8ab;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;8d1c588d202f150e1797ed113fba7e67bfa43ecb;c6bbfb4fcaecc779c899af4bb52083870f4b996a;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;f3203d0bdefc9670ed508ca776d08aa9f024bafa;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;03cb4e2cb669d3f6344a733e622f07909f87ff0a;739769f4862753fc80057194456d758d2a148ee3;ae523e2f137fa2a4f5a6cbcc443ba63db2642a96|8230559;145905113;2109306087|True;True;True|desc;desc;desc
9b539d413393047b28bb7be9b195f142aaf7a80e|10.18653/v1/2021.eacl-main.24|PNAS|26.0|4567-8901_26|300-325|Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.||||Recipes for Building an Open-Domain Chatbot|2020|144745718;31461304;39589154;3092435;2066769956;11323179;2155954521;40511414;35752280;51324296;2656573;145183709|Stephen Roller;Emily Dinan;Naman Goyal;Da Ju;Mary Williamson;Yinhan Liu;Jing Xu;Myle Ott;Kurt Shuster;Eric Michael Smith;Y-Lan Boureau;J. Weston|Computer Science|bd1f14e7531220c39fad8f86985cce7b283f035d;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;9d7c04de906823a60d3ccb5f510fd0029af5c8b0;34f25a8704614163c4095b3ee2fc969b60de4698;759d9a6c9206c366a8d94a06f4eb05659c2bb7f2;04fd278c01df1564e741b4c6e052fc1c5924ab8d|69539592;39589154;2054745187|False;True;True|desc;desc;desc
5aefde4203ce46ea900a96835a7c59a5f50800e7|10.1021/ci500747n|Radiology|37.0|0123-4567_37|"
          263-74
        "|Neural networks were widely used for quantitative structure-activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Merck's drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.||||Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships|2015|47793005;1838644;2587444;35188630;3305021|Junshui Ma;R. Sheridan;Andy Liaw;George E. Dahl;V. Svetnik|Chemistry;Computer Science;Medicine|693914b7f38c19585e35668fd626aecf62d4c5e7;574449170f293dfa868771e9ee0403b56a19b9e9;4a554da55fd9ff76c99e25d2ce937b225dc1100c;6f24d7a6e1c88828e18d16c6db20f5329f6a6827;48e752c719d33ff55b3b3bec3538727f8ce69399;f94455176857303605ad423599385a2341c568eb;8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;f3203d0bdefc9670ed508ca776d08aa9f024bafa;07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b;9eb715fe0347445a2d63518cbb476d345ba86233;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;fbf1c51548ffc9b9e538befcd71529365af23d15;3def68bd0f856886d34272840a7f81588f2bc082|16411658;3353457;3048564|True;True;True|desc;desc;desc
96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7|10.21248/jlcl.20.2005.68|Nature|29.0|1234-5678_29|19-62|The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.||||A Brief Survey of Text Mining|2005|1792623;1759689;1683459|A. Hotho;A. Nürnberger;G. Paass|Computer Science;Linguistics|45557cc70cd6989ab6b03e5aeb787e34299099f7;4d1fdd81f033cd58f3723bfc61e7d12079647a7a;2de0a40e9a5d4f1feb07d61af5a5d87a069653f0;6aae0dc122102693e8136856ffc8b72df7f78386|145124475;1695217;2529354|True;True;True|desc;desc;desc
831edc3d67457db83da40d260e93bfd7559347ae|10.1145/122344.122377|JACC|47.0|1234-5678_47|160-163|Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.||||Dyna, an integrated architecture for learning, planning, and reacting|1990|1699645|R. Sutton|Computer Science|971766088dfaf63fb55e6f0190b14f28f2c98ad0;c6bbfb4fcaecc779c899af4bb52083870f4b996a;c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3;4a554da55fd9ff76c99e25d2ce937b225dc1100c;d02927d4de4a2a51cced4970da04b812cbee4342|49551702;1679209;2061819040|True;True;True|desc;desc;desc
872352b0a53ab6cbb4420f81df64d215d86c7d9b|10.3115/1220575.1220648||||579-586|In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a naive baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Beijing_2005|Beijing|Emotions from Text: Machine Learning for Text-based Emotion Prediction|2005|144648940;144590225;145421878|Cecilia Ovesdotter Alm;D. Roth;R. Sproat|Computer Science|9670485f526f2254c0f34e64d9ca06f665a0bd17;a7a407968c13ced804a063259d72315a43b84f29;78947497cbbffc691aac3f590d972130259af9ce;b8012351bc5ebce4a4b3039bbbba3ce393bc3315;36d442f59c61ea2912d227c24dee76778c546b0a;dd9b99fac67c18be82d7763a8fbf231fc3512423;a20bfec3c95aad003dcb45a21a220c19cca8bb66;30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70;605402e235bd62437baf3c9ebefe77fb4d92ee95;184ac0766262312ba76bbdece4e7ffad0aa8180b|2044918426;15660940;2380233|True;True;True|desc;desc;desc
9d7c04de906823a60d3ccb5f510fd0029af5c8b0|10.1109/72.572107||||"
          694-713
        "|This paper presents a new evolutionary system, i.e., EPNet, for evolving artificial neural networks (ANNs). The evolutionary algorithm used in EPNet is based on Fogel's evolutionary programming (EP). Unlike most previous studies on evolving ANN's, this paper puts its emphasis on evolving ANN's behaviors. Five mutation operators proposed in EPNet reflect such an emphasis on evolving behaviors. Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting. EPNet evolves ANN's architectures and connection weights (including biases) simultaneously in order to reduce the noise in fitness evaluation. The parsimony of evolved ANN's is encouraged by preferring node/connection deletion to addition. EPNet has been tested on a number of benchmark problems in machine learning and ANNs, such as the parity problem, the medical diagnosis problems, the Australian credit card assessment problem, and the Mackey-Glass time series prediction problem. The experimental results show that EPNet can produce very compact ANNs with good generalization ability in comparison with other algorithms.|NIPS|NIPS_New_York_1997|New York|A new evolutionary system for evolving artificial neural networks|1997|143901532;152891866|X. Yao;Yong Liu|Computer Science;Medicine|5cbe278b65a81602a864184bbca37de91448a5f5;9071775ebcfebddd54d879fe7e6c627673e4d305;e24b8a9531573d284647239affc6c855505b0de4;09622b0c84bf812814af5b64b0c83dce796899c4;d133cb102ad0f81e3fd17a7db090b28afc124c4a;31a537c48c2bf2d98f2020df5b72c413d0fea1da;1fcbefeb0beae4470cf40df74cd116b1d4bdcae4;a675fe5a7d99ac6f7ff91fa084462faefe616148;b8ebda42e272d3617375118542d4675a0c0e501d;1f84b3a01a5ad5c7d3c838fb040d4634e79bf32d;de2be42659be5c43c1a992b5d7fe6daf14e571dd;d05d86db86a4ac0d95e6dcd951b42a9651939793;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;c2b381b24aabf237394059fed7920cd6fd0e67b8;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;222d3a63d4f81d39ea324530b57328c58f298888;864e7db59f2ccfec1ee9f6eba79566ac7b0634df|1485389319;3216372;2116066317|False;True;True|desc;desc;desc
2d2fe4a73c98933ae9b8df73c8452b0d8be6475e|10.1109/TIE.2016.2519325||||3137-3147|Intelligent fault diagnosis is a promising tool to deal with mechanical big data due to its ability in rapidly and efficiently processing collected signals and providing accurate diagnosis results. In traditional intelligent diagnosis methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise. Such processes take advantage of human ingenuity but are time-consuming and labor-intensive. Inspired by the idea of unsupervised feature learning that uses artificial intelligence techniques to learn features from raw data, a two-stage learning method is proposed for intelligent diagnosis of machines. In the first learning stage of the method, sparse filtering, an unsupervised two-layer neural network, is used to directly learn features from mechanical vibration signals. In the second stage, softmax regression is employed to classify the health conditions based on the learned features. The proposed method is validated by a motor bearing dataset and a locomotive bearing dataset, respectively. The results show that the proposed method obtains fairly high diagnosis accuracies and is superior to the existing methods for the motor bearing dataset. Because of learning features adaptively, the proposed method reduces the need of human labor and makes intelligent fault diagnosis handle big data more easily.|COLT|COLT_New_York_2016|New York|An Intelligent Fault Diagnosis Method Using Unsupervised Feature Learning Towards Mechanical Big Data|2016|1829456;46691607;143922195;3372340;144558270|Y. Lei;Feng Jia;Jing Lin;Saibo Xing;S. Ding|Computer Science;Engineering|5c45a5d05ac564adb67811eeb9d41d6460c70135;602f31242e577d2d05f918a3080fd50095e7faed;d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f;9008cdacbdcff8a218a6928e94fe7c6dfc237b24;6ec7c724aa1d906e9e9f81c58497adddb22175b8;1626c940a64ad96a7ed53d7d6c0df63c6696956b;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;b954efe5e46b8952f5a8daf42e7e535119b5408b;a675fe5a7d99ac6f7ff91fa084462faefe616148;7bb6bdf4ed609e5e72d4206d1b308323e73dceec;48234756b7cf798bfeb47328f7c5d597fd4838c2;cedea36fa3692281b3ac767335fe49a16d00957d;36652428740cd30d245d55889f01a7fb04a91c93;884895a86fe15cb9601df4a15a1475c07f28da3c;92ace17730c2173e642934d64f96d359697b7a93|48385057;2471253;1796267433|False;True;False|desc;desc;desc
305d689afb6574ffec7b01e24431d541d0ce6f5d|10.1109/SFCS.1995.492488||||322-331|"In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the expected per-round payoff of our algorithm approaches that of the best arm at the rate O(T/sup -1/3/), and we give an improved rate of convergence when the best arm has fairly low payoff. We also consider a setting in which the player has a team of ""experts"" advising him on which arm to play; here, we give a strategy that will guarantee expected payoff close to that of the best expert. Finally, we apply our result to the problem of learning to play an unknown repeated matrix game against an all-powerful adversary."|NAACL|NAACL_Beijing_1995|Beijing|Gambling in a rigged casino: The adversarial multi-armed bandit problem|1995|144543541;1388387856;1703537;1716301|P. Auer;N. Cesa-Bianchi;Y. Freund;R. Schapire|Computer Science;Mathematics|2521c3d76bc439c961b7003080f4a7a661949547;7da323e7103245eeaed32367c46abe3f4913df86;06645d735b59b14479ae1d0392136bbf44227d0f;872352b0a53ab6cbb4420f81df64d215d86c7d9b;1051280d2b825c04f27d231aba0f8284bb297880;1a827052f01ef830cbc849c71e9da99791243a5f;58a8bead87c8c1e37460dce28285c053c270f6e7;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;46f74231b9afeb0c290d6d550043c55045284e5f;8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92;48234756b7cf798bfeb47328f7c5d597fd4838c2;c2b381b24aabf237394059fed7920cd6fd0e67b8;a20bfec3c95aad003dcb45a21a220c19cca8bb66;16c0ef924da1f6b510c9c783ac764156f5a3d631;831edc3d67457db83da40d260e93bfd7559347ae;18d026ec5d0eebd17ee2c762da89540c0b3d7bde|145306271;39589154;145233583|True;True;True|desc;desc;desc
908cca0abefc35acc38033603714fbb1bcadc49d|10.1109/cvpr42600.2020.00028||||200-210|This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code is available at: https://github.com/facebookresearch/SlowFast.|ICRA|ICRA_Moscow_2020|Moscow|X3D: Expanding Architectures for Efficient Video Recognition|2020|2322150|Christoph Feichtenhofer|Computer Science|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;5a4631d5d75e3610037f87839628a4d166581e01;26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810;31a537c48c2bf2d98f2020df5b72c413d0fea1da;88816ae492956f3004daa41357166f1181c0c1bf;e6b9fc7aa2996e7afc91c7f223460f9ded85e2da;6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91;ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc;9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;2c47bd8bd699914e3535292b17ba46542800845c;7da323e7103245eeaed32367c46abe3f4913df86;e9126a98de0c39dcffe4c4f5158e037460196724;db68a79e59291b85e10300b79c43843b651aa195;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;d9d2fe0d5ae1ec67068f1b4be2e8ac828ac989ac;e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772;546785490ac417be1f83ced6a8272e934934f411|144555592;143898643;37167270|True;True;True|desc;desc;desc
1051280d2b825c04f27d231aba0f8284bb297880|10.1109/TSE.2005.112|Science|81.0|3456-7890_81|897-910|Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.||||Empirical validation of object-oriented metrics on open source software for fault prediction|2005|1681248;3172077;1868524|T. Gyimóthy;R. Ferenc;István Siket|Computer Science|bb144c04b9eb44579b19d21c3d5954401408440b;2c8ac3e1f0edeed1fbd76813e61efdc384c319c7;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;a86171e13f84fe32212dd7fb6a1c31a34a47155f;1c00df1cb85fa7886b6666599eab59f2b301dd5d;338a891907dce447da9a0fa2f27221bd35164163;ac12c9b9e35e58b55d85a97c47886a7371c14afa;35b3233e521f1e9a34837c30be1957858f8f35fe;bf7dcbee272428a2aa3c534200743ff7ab2047f8|1740765;1771659;46956675|True;True;True|desc;desc;desc
e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3|10.1142/S0219622006002258||||597-604|In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.|AAMAS|AAMAS_Tokyo_2006|Tokyo|10 Challenging Problems in Data Mining Research|2006|152290618;1748808|Qiang Yang;Xindong Wu|Computer Science|a85e512d8845bd007b0866b4a97e8341463f8190;602f31242e577d2d05f918a3080fd50095e7faed;46f74231b9afeb0c290d6d550043c55045284e5f|3025831;15186612;29905643|False;False;True|desc;desc;desc
8db9df2eadea654f128c1887722c677c708e8a47|10.2352/ISSN.2470-1173.2017.19.AVM-023||||70-76|Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.|CVPR|CVPR_Mexico_City_2017|Mexico City|Deep Reinforcement Learning framework for Autonomous Driving|2017|7353741;144026300;8211183;2601522|Ahmad El Sallab;Mohammed Abdou;E. Perot;S. Yogamani|Computer Science;Mathematics;Engineering|65b16da51891a6b98140d425804c8a0fd0299219;cd49acefc8d51e324aa562e5337e1c2aff067053;265644f1b6740ca34bfbe9762b90b33021adde62;877374c2913b787ee9f958f39e31c75d39ebcc15;5ed59f49c1bb7de06cfa2a9467d5efb535103277;f354310098e09c1e1dc88758fca36767fd9d084d;adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1;467568f1777bc51a15a5100516cd4fe8de62b9ab;a675fe5a7d99ac6f7ff91fa084462faefe616148;1f87134a630c2dbb9a3645ba658954f00b620a77;338a891907dce447da9a0fa2f27221bd35164163;175e37bca3762b3a52c6a0e153060b98a251d061|30880777;144902513;48539382|True;False;True|desc;desc;desc
4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895|10.1109/ROBOT.1989.100065||||253-262|Most animals have significant behavioral expertise built in without having to explicitly learn it all from scratch. This expertise is a product of evolution of the organism; it can be viewed as a very long-term form of learning which provides a structured system within which individuals might learn more specialized skills or abilities. This paper suggests one possible mechanism for analagous robot evolution by describing a carefully designed series of networks, each one being a strict augmentation of the previous one, which control a six-legged walking machine capable of walking over rough terrain and following a person passively sensed in the infrared spectrum. As the completely decentralized networks are augmented, the robot's performance and behavior repertoire demonstrably improve. The rationale for such demonstrations is that they may provide a hint as to the requirements for automatically building massive networks to carry out complex sensory-motor tasks. The experiments with an actual robot ensure that an essence of reality is maintained and that no critical disabling problems have been ignored.|NAACL|NAACL_Auckland_1989|Auckland|A Robot that Walks; Emergent Behaviors from a Carefully Evolved Network|1989|72419159|R. Brooks|Computer Science;Engineering|08b43d84e6747e370ef307e2ada50675b414514a;31a537c48c2bf2d98f2020df5b72c413d0fea1da|1726587;2113693105;34911188|True;True;True|desc;desc;desc
825ca26af5a2a510dbc1a7b97587212bc98ae968|10.1609/AIMAG.V35I4.2513|Nature|81.0|1234-5678_81|105-120|Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.||||Power to the People: The Role of Humans in Interactive Machine Learning|2014|1719124;35096370;144288136;1847827|Saleema Amershi;M. Cakmak;W. B. Knox;Todd Kulesza|Computer Science|f986968735459e789890f24b6b277b0920a9725d;85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175;dd971c07879e1ce12b06991319528c06280eeb9b;81a4fd3004df0eb05d6c1cef96ad33d5407820df;9691f67f5075bde2fd70da0135a4a70f25ef042b;d133cb102ad0f81e3fd17a7db090b28afc124c4a;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;d02927d4de4a2a51cced4970da04b812cbee4342;f354310098e09c1e1dc88758fca36767fd9d084d;b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b;bd1f14e7531220c39fad8f86985cce7b283f035d;6ec7c724aa1d906e9e9f81c58497adddb22175b8;5d150cec2775f9bc863760448f14104cc8f42368;9257779eed46107bcdce9f4dc86298572ff466ce|40068904;2044918426;123257978|False;False;True|desc;desc;desc
799f927692a6c08c5e630bea78c087c5051528fc|10.1177/1745691612454304||||464 - 481|A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient “active learning” algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.|RSS|RSS_Buenos_Aires_2012|Buenos Aires|Self-Directed Learning|2012|3013567;3164019|T. Gureckis;D. Markant|Computer Science;Medicine;Education;Psychology|a34e35dbbc6911fa7b94894dffdc0076a261b6f0;43d2ed5c3c55c1100450cd74dc1031afa24d37b2|3105120;2314654;48385057|True;True;True|desc;desc;desc
ff7a293e95c0d44582b7625ee2233916f15cb361|10.3233/978-1-60750-611-9-125||||125-138|Since the beginning of the Internet age and the increased use of ubiquitous computing devices, the large volume and continuous flow of distributed data have imposed new constraints on the design of learning algorithms. Exploring how to extract knowledge structures from evolving and time-changing data, Knowledge Discovery from Data Streams presents a coherent overview of state-of-the-art research in learning from data streams. The book covers the fundamentals that are imperative to understanding data streams and describes important applications, such as TCP/IP traffic, GPS data, sensor networks, and customer click streams. It also addresses several challenges of data mining in the future, when stream mining will be at the core of many applications. These challenges involve designing useful and efficient data mining solutions applicable to real-world problems. In the appendix, the author includes examples of publicly available software and online data sets. This practical, up-to-date book focuses on the new requirements of the next generation of data mining. Although the concepts presented in the text are mainly about data streams, they also are valid for different areas of machine learning and data mining.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Mexico_City_2009|Mexico City|Knowledge discovery from data streams|2009|143931014;83889546;2274294|João Gama;J. Aguilar-Ruiz;Ralf Klinkenberg|Computer Science|cc1cad12521b5aab43fdda5b4dec67586aef1f87;bb144c04b9eb44579b19d21c3d5954401408440b;61e27dbae190b82639c57f180ecf97e4c46fcad9;b9518627db25f05930e931f56497602363a75491;37a67228271527037c9250ae3fd220199275e42e;40f391bc3860e8cd71d7ea68ec11d80d5e12c02c;265644f1b6740ca34bfbe9762b90b33021adde62;f3203d0bdefc9670ed508ca776d08aa9f024bafa;bd1f14e7531220c39fad8f86985cce7b283f035d;5b66b1c65dcb97d1d0b18014e2e32e8522e66932;d997919c30fa6711bc5c25cf8c8aea34fac27b91;a85e512d8845bd007b0866b4a97e8341463f8190;877374c2913b787ee9f958f39e31c75d39ebcc15;611544418ca53cdad254df444addc7814abcfddc;d12864a8acbab1830be755bfb9cb177e31ca5e20;03cb4e2cb669d3f6344a733e622f07909f87ff0a;5a391667242b4a631acdd5917681b16a86523987;339a8e4cb0eba77675711ac255ac2a5d7ede1d53|2348963575;1712994;145034054|True;False;True|desc;desc;desc
a88b3be9b2db0319f8880e60a131b3060dba1eb7|10.1145/1866307.1866335||||237-249|We show in this paper how several proposed Physical Unclonable Functions (PUFs) can be broken by numerical modeling attacks. Given a set of challenge-response pairs (CRPs) of a PUF, our attacks construct a computer algorithm which behaves indistinguishably from the original PUF on almost all CRPs. This algorithm can subsequently impersonate the PUF, and can be cloned and distributed arbitrarily. This breaks the security of essentially all applications and protocols that are based on the respective PUF. The PUFs we attacked successfully include standard Arbited PUFs and Ring Oscillator PUFs of arbitrary sizes, and XO Arbiter PUFs, Lightweight Secure PUFs, and Feed-Forward Arbiter PUFs of up to a given size and complexity. Our attacks are based upon various machine learning techniques including Logistic Regression and Evolution Strategies. Our work leads to new design requirements for secure electrical PUFs, and will be useful to PUF designers and attackers alike.|CVPR|CVPR_Buenos_Aires_2010|Buenos Aires|Modeling attacks on physical unclonable functions|2010|2057173;1702866;47663455;2088846;1695217;145341374|U. Rührmair;Frank Sehnke;J. Sölter;G. Dror;S. Devadas;J. Schmidhuber|Computer Science|8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;b16408a97170785fb216c9e8b7920d64f478fbc8;64be9999b68e12d260ba7423f6b55ffd41552ad3;8db9df2eadea654f128c1887722c677c708e8a47;78989616eeeac55b202e3e4205225e7135054185;37a67228271527037c9250ae3fd220199275e42e;09622b0c84bf812814af5b64b0c83dce796899c4;43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402;62ed272e0e8b7be356c7f7595f5b7a22797a1c3e;dd971c07879e1ce12b06991319528c06280eeb9b;e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3;a675fe5a7d99ac6f7ff91fa084462faefe616148;5ed59f49c1bb7de06cfa2a9467d5efb535103277;5a391667242b4a631acdd5917681b16a86523987;afcc28d71be4ea6a48a339f9e4e5557d1b2b25be;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0|2421691;1409068337;48180545|True;True;True|desc;desc;desc
5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7|10.1145/1128817.1128824||||16-25|"Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, ""Can machine learning be secure?"" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems."|NAACL|NAACL_Moscow_2006|Moscow|Can machine learning be secure?|2006|145140480;39743720;145879573;1687701;1787610|M. Barreno;B. Nelson;Russell Sears;A. Joseph;J. D. Tygar|Computer Science|825ca26af5a2a510dbc1a7b97587212bc98ae968;92ace17730c2173e642934d64f96d359697b7a93;12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4;2a3842f6070b4554ff21fe62b2a486657d9a304a;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;d9665992ee36699b8ae4a2e2294552cd4be9003a;c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7;45557cc70cd6989ab6b03e5aeb787e34299099f7;427b168f490b56716f22b129ac93aba5425ea08f;5c5e69387020d7ca7d49487ca841958dc5e08ce6;e9126a98de0c39dcffe4c4f5158e037460196724;273dfbcb68080251f5e9ff38b4413d7bd84b10a1;cc5afe344cc7ed7acd68a28b9774ea8023a162dc|145325584;1804489;145520804|False;True;False|desc;desc;desc
ac12c9b9e35e58b55d85a97c47886a7371c14afa|10.1093/bioinformatics/bth261|Cell|1.0|5678-9012_1|"
          2479-81
        "|"UNLABELLED
The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.


AVAILABILITY
http://www.cs.waikato.ac.nz/ml/weka."||||Data mining in bioinformatics using Weka|2004|1767318;118860642;33614647;2084564300;9419406|E. Frank;M. Hall;Leonard E. Trigg;G. Holmes;I. Witten|Computer Science;Medicine;Biology|24e6c5bfe9bb0751e5708b501d04e860011b2953;5d150cec2775f9bc863760448f14104cc8f42368;b16408a97170785fb216c9e8b7920d64f478fbc8;8592e46a5435d18bba70557846f47290b34c1aa5;86cff4d050beb90fed2e1ceac8940c8221b120aa;b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57;85d727b119304dde458bcd8cf5cb87a906fb41ba|1413794065;51997673;2105795|True;True;False|desc;desc;desc
a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1|10.1561/0600000035||||81-227|"This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks. 
 
Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning, and active learning under the same decision forest framework. This gives us the opportunity to write and optimize the core implementation only once, with application to many diverse tasks. 
 
The proposed model may be used both in a discriminative or generative way and may be applied to discrete or continuous, labeled or unlabeled data. 
 
The main contributions of this review are: (1) Proposing a unified, probabilistic and efficient model for a variety of learning tasks; (2) Demonstrating margin-maximizing properties of classification forests; (3) Discussing probabilistic regression forests in comparison with other nonlinear regression algorithms; (4) Introducing density forests for estimating probability density functions; (5) Proposing an efficient algorithm for sampling from a density forest; (6) Introducing manifold forests for nonlinear dimensionality reduction; (7) Proposing new algorithms for transductive learning and active learning. Finally, we discuss how alternatives such as random ferns and extremely randomized trees stem from our more general forest model. 
 
This document is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in the new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state-of-the-art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed. The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility."|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Madrid_2012|Madrid|Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning|2012|1716777;143774737;1796918|A. Criminisi;J. Shotton;E. Konukoglu|Computer Science;Medicine|db68a79e59291b85e10300b79c43843b651aa195;f3203d0bdefc9670ed508ca776d08aa9f024bafa;885af28a751553be48a25b411a5d492767d4cf65;18bc1d4271abe8dd6e16179cdb06524a4f396e16;bd1f14e7531220c39fad8f86985cce7b283f035d;d079a2f877f554e00f71a6975435d8325987bdf5;7e7eb0f93c9550d7336f4bbfad5fe89604295705;a3461eaf51016f9d6e85ea47173b27e019e801c4;b8ebda42e272d3617375118542d4675a0c0e501d|32837403;37167270;2675885|True;True;True|desc;desc;desc
