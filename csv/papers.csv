paperID|doi|journal_name|volume|volume_id|pages|abstract|publicationVenue_name|edition_id|city_venue|title|year|authorIDs|authorNames|fields|citedPaperID|reviewerIDs|reviewsApprovements|reviewsDesc
0b544dfe355a5070b60986319a3f51fb45d1348e|10.3115/v1/D14-1179||||1724-1734|In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.|Workshop on AI for Accessibility|Workshop_on_AI_for_Accessibility_London_2014|London|Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation|2014|1979489;3158246;1854385;3335364;2076086;144518416;1751762|Kyunghyun Cho;B. V. Merrienboer;Çaglar Gülçehre;Dzmitry Bahdanau;Fethi Bougares;Holger Schwenk;Yoshua Bengio|Computer Science;Mathematics|dd971c07879e1ce12b06991319528c06280eeb9b;10f919b1a5161b560504c225cfb2d1b3a4768f80;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;5a4631d5d75e3610037f87839628a4d166581e01|2109675545;70627370;2344478|False;False;True|desc;desc;desc
339a8e4cb0eba77675711ac255ac2a5d7ede1d53|10.1093/molbev/msab120|PNAS|94.0|4567-8901_94|3022 - 3027|Abstract The Molecular Evolutionary Genetics Analysis (MEGA) software has matured to contain a large collection of methods and tools of computational molecular evolution. Here, we describe new additions that make MEGA a more comprehensive tool for building timetrees of species, pathogens, and gene families using rapid relaxed-clock methods. Methods for estimating divergence times and confidence intervals are implemented to use probability densities for calibration constraints for node-dating and sequence sampling dates for tip-dating analyses. They are supported by new options for tagging sequences with spatiotemporal sampling information, an expanded interactive Node Calibrations Editor, and an extended Tree Explorer to display timetrees. Also added is a Bayesian method for estimating neutral evolutionary probabilities of alleles in a species using multispecies sequence alignments and a machine learning method to test for the autocorrelation of evolutionary rates in phylogenies. The computer memory requirements for the maximum likelihood analysis are reduced significantly through reprogramming, and the graphical user interface has been made more responsive and interactive for very big data sets. These enhancements will improve the user experience, quality of results, and the pace of biological discovery. Natively compiled graphical user interface and command-line versions of MEGA11 are available for Microsoft Windows, Linux, and macOS from www.megasoftware.net.||||MEGA11: Molecular Evolutionary Genetics Analysis Version 11|2021|3066242;48226007;2110208881|K. Tamura;G. Stecher;Sudhir Kumar|Computer Science;Medicine;Biology|cd49acefc8d51e324aa562e5337e1c2aff067053;5a4631d5d75e3610037f87839628a4d166581e01;44c7d9fe583e3d317a619297e7e949070710799f;265644f1b6740ca34bfbe9762b90b33021adde62;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;395de0bd3837fdf4b4b5e5f04835bcc69c279481;10f919b1a5161b560504c225cfb2d1b3a4768f80;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;12d1d070a53d4084d88a77b8b143bad51c40c38f;0b544dfe355a5070b60986319a3f51fb45d1348e;8c8215b7f8111839f0066010a530a3a9f57ba15e;4b61c25a86083c20730c9b12737ac6ac4178c364;9d46dc975aeed3f96bddb144079b50238f746ecd;d517b13f2b152c913b81ce534a149493517dbdad;dd971c07879e1ce12b06991319528c06280eeb9b;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;98c25683fc8d6446448b734b1bcf08e1457f8d85;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7|1745644;2112890973;47156522|True;True;True|desc;desc;desc
395de0bd3837fdf4b4b5e5f04835bcc69c279481|10.18653/v1/2020.acl-main.703||||7871-7880|We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.|ECCV|ECCV_Cape_Town_2019|Cape Town|BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension|2019|35084211;11323179;39589154;2320509;113947684;39455775;1759422;1982950|M. Lewis;Yinhan Liu;Naman Goyal;Marjan Ghazvininejad;Abdel-rahman Mohamed;Omer Levy;Veselin Stoyanov;Luke Zettlemoyer|Computer Science;Mathematics;Linguistics|0b544dfe355a5070b60986319a3f51fb45d1348e;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;546785490ac417be1f83ced6a8272e934934f411;693914b7f38c19585e35668fd626aecf62d4c5e7;9d46dc975aeed3f96bddb144079b50238f746ecd;a27089efabc5f4abd5ddf2be2a409bff41f31199;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;9670485f526f2254c0f34e64d9ca06f665a0bd17;9f387ce140c59a44eaeeea590087351461345164;b3de1062d8a462dfdc2938558258f8884abe9f4e;4b61c25a86083c20730c9b12737ac6ac4178c364;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;f4a5503783487eba5c5e34b1d02c09016b244b1d;12d1d070a53d4084d88a77b8b143bad51c40c38f;d05d86db86a4ac0d95e6dcd951b42a9651939793;44c7d9fe583e3d317a619297e7e949070710799f|17895970;48999874;47479574|True;True;True|desc;desc;desc
12d1d070a53d4084d88a77b8b143bad51c40c38f|10.1613/jair.301||||237-285|"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ""reinforcement."" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."|ICML|ICML_New_York_1996|New York|Reinforcement Learning: A Survey|1996|1709512;144885169;1760402|L. Kaelbling;M. Littman;A. Moore|Computer Science|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;61e27dbae190b82639c57f180ecf97e4c46fcad9;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;8c8215b7f8111839f0066010a530a3a9f57ba15e;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;dd971c07879e1ce12b06991319528c06280eeb9b;38f23fe236b152cd4983c8f30d305a568afd0d3e;693914b7f38c19585e35668fd626aecf62d4c5e7;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;d133cb102ad0f81e3fd17a7db090b28afc124c4a;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;b5887d18420e8ac4f4fa4c83c4952138fd956702;a7a407968c13ced804a063259d72315a43b84f29;98c25683fc8d6446448b734b1bcf08e1457f8d85|2109916949;2031130914;145080287|True;True;False|desc;desc;desc
5ded2b8c64491b4a67f6d39ce473d4b9347a672e|10.18653/v1/N18-1101|Science|43.0|3456-7890_43|1112-1122|This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.||||A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference|2017|81840293;10666396;3644767|Adina Williams;Nikita Nangia;Samuel R. Bowman|Computer Science;Linguistics|339a8e4cb0eba77675711ac255ac2a5d7ede1d53;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;10f919b1a5161b560504c225cfb2d1b3a4768f80;b5887d18420e8ac4f4fa4c83c4952138fd956702;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;4b61c25a86083c20730c9b12737ac6ac4178c364;98c25683fc8d6446448b734b1bcf08e1457f8d85;9d46dc975aeed3f96bddb144079b50238f746ecd;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;9f387ce140c59a44eaeeea590087351461345164;9b539d413393047b28bb7be9b195f142aaf7a80e;a27089efabc5f4abd5ddf2be2a409bff41f31199;f4a5503783487eba5c5e34b1d02c09016b244b1d;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;395de0bd3837fdf4b4b5e5f04835bcc69c279481;f94455176857303605ad423599385a2341c568eb|1766624;2309030;2340666|True;True;True|desc;desc;desc
98c25683fc8d6446448b734b1bcf08e1457f8d85|10.1093/bioinformatics/btm344||||"
          2507-17
        "|Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.|ICML|ICML_Cape_Town_2007|Cape Town|A review of feature selection techniques in bioinformatics|2007|2247520360;1788277;2285903189|Yvan Saeys;Iñaki Inza;Pedro Larrañaga|Computer Science;Medicine;Biology|395de0bd3837fdf4b4b5e5f04835bcc69c279481;64be9999b68e12d260ba7423f6b55ffd41552ad3;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;24e6c5bfe9bb0751e5708b501d04e860011b2953;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;8c8215b7f8111839f0066010a530a3a9f57ba15e;5a4631d5d75e3610037f87839628a4d166581e01|49903386;39589154;32084575|True;False;True|desc;desc;desc
611544418ca53cdad254df444addc7814abcfddc|10.1080/24754269.2021.1980261||||87 - 87|The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an|ICLR|ICLR_Toronto_2021|Toronto|An introduction to statistical learning with applications in R|2021|151004165;2134162313;1840524|Fariha Sohil;Muhammad Umair Sohali;J. Shabbir|Computer Science;Mathematics|d517b13f2b152c913b81ce534a149493517dbdad;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;44c7d9fe583e3d317a619297e7e949070710799f;339a8e4cb0eba77675711ac255ac2a5d7ede1d53|2112890973;144264986;2842984|True;False;True|desc;desc;desc
44c7d9fe583e3d317a619297e7e949070710799f|10.1093/nar/gkac1052||||D523 - D531|Abstract The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this publication we describe enhancements made to our data processing pipeline and to our website to adapt to an ever-increasing information content. The number of sequences in UniProtKB has risen to over 227 million and we are working towards including a reference proteome for each taxonomic group. We continue to extract detailed annotations from the literature to update or create reviewed entries, while unreviewed entries are supplemented with annotations provided by automated systems using a variety of machine-learning techniques. In addition, the scientific community continues their contributions of publications and annotations to UniProt entries of their interest. Finally, we describe our new website (https://www.uniprot.org/), designed to enhance our users’ experience and make our data easily accessible to the research community. This interface includes access to AlphaFold structures for more than 85% of all entries as well as improved visualisations for subcellular localisation of proteins.|ICRA|ICRA_Cairo_2022|Cairo|UniProt: the Universal Protein Knowledgebase in 2023|2022|46888415;46502933;143945011;3093838;49967517;2009348;1578100923;35510297;1413794065;2038165074;2191753901;2407842;15309498;2108094514;11852405;2191751245;1413140200;100546775;5059205;2038176754;6289332;2055614715;47457030;2191763301;5219418;34594667;121557847;2116783356;6593936;1694551096;145684421;2038173616;2112890973;15186612;144057990;2595483;39589857;48506828;153529797;2053629650;1766624;2115667193;1914833;144916619;1907575;2061819040;50693907;1865802;153825989;1979856;5240016;1694538098;145718058;2380233;1403651727;38473019;2910143;15660940;3004899;2191754156;48999874;3349310;2649909;2314654;2090895670;1411865594;2044918426;2344478;2509510;40660788;35099951;3375775;1765161;1784123;1873696;3168502;2092438037;2057009;5385807;3304621;1399776494;87711179;2594256;8926744;3177854;143827678;2258061;2375564;11541096;3275738;2649548;2070956511;3322351;37120465;2562543;1876480;40291345;1897062;1491922105;40660854;1744726;1734053;3177811;1763713;3090725;2446311;50401356;3250223;1776336;6199470;153787929;123451953;2108034421;2151810209|A. Bateman;M. Martin;S. Orchard;M. Magrane;Shadab Ahmad;E. Alpi;E. Bowler-Barnett;R. Britto;Hema Bye-A-Jee;Austra Cukura;Paul Denny;Tunca Dogan;Thankgod Ebenezer;Jun Fan;Penelope Garmiri;Leonardo Jose da Costa Gonzales;E. Hatton-Ellis;Abdulrahman Hussein;A. Ignatchenko;Giuseppe Insana;Rizwan Ishtiaq;Vishal Joshi;D. Jyothi;Swaathi Kandasaamy;A. Lock;Aurélien Luciani;Marija Lugarić;Jie Luo;Yvonne Lussi;Alistair MacDougall;F. Madeira;Mahdi Mahmoudy;Alok Mishra;Katie Moulang;Andrew Nightingale;Sangya Pundir;G. Qi;Shriya Raj;P. Raposo;Daniel L Rice;Rabie Saidi;Rafael Santos;Elena Speretta;J. Stephenson;Prabhat Totoo;Edward Turner;N. Tyagi;Preethi Vasudev;Kate Warner;Xavier Watkins;Rossana Zaru;H. Zellner;A. Bridge;L. Aimo;Ghislaine Argoud-Puy;A. Auchincloss;K. Axelsen;Parit Bansal;Delphine Baratin;Teresa M Batista Neto;M. Blatter;Jerven T. Bolleman;E. Boutet;L. Breuza;B. Gil;Cristina Casals-Casas;Kamal Chikh Echioukh;E. Coudert;Béatrice A. Cuche;Edouard de Castro;A. Estreicher;M. Famiglietti;M. Feuermann;E. Gasteiger;P. Gaudet;S. Gehant;V. Gerritsen;A. Gos;N. Gruaz;C. Hulo;Nevila Hyka-Nouspikel;F. Jungo;A. Kerhornou;Philippe le Mercier;D. Lieberherr;P. Masson;A. Morgat;Venkatesh Muthukrishnan;S. Paesano;I. Pedruzzi;S. Pilbout;L. Pourcel;S. Poux;Monica Pozzato;Manuela Pruess;Nicole Redaschi;C. Rivoire;Christian J. A. Sigrist;K. Sonesson;S. Sundaram;Cathy H. Wu;C. Arighi;L. Arminski;Chuming Chen;Yongxing Chen;Hongzhan Huang;K. Laiho;P. McGarvey;D. Natale;K. Ross;C. R. Vinayaka;Qinghua Wang;Yuqi Wang;Jian Zhang|Computer Science;Medicine;Biology|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;61e27dbae190b82639c57f180ecf97e4c46fcad9;4b61c25a86083c20730c9b12737ac6ac4178c364;f94455176857303605ad423599385a2341c568eb;d05d86db86a4ac0d95e6dcd951b42a9651939793|2068347799;80243122;144821090|False;True;True|desc;desc;desc
a27089efabc5f4abd5ddf2be2a409bff41f31199|10.5220/0006639801080116|JAMA|53.0|7890-1234_53|108-116|: With exponential growth in the size of computer networks and developed applications, the signiﬁcant in-creasing of the potential damage that can be caused by launching attacks is becoming obvious. Meanwhile, Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are one of the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of adequate dataset, anomaly-based approaches in intrusion detection systems are suffering from accurate deployment, analysis and evaluation. There exist a number of such datasets such as DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their proposed intrusion detection and intrusion prevention approaches. Based on our study over eleven available datasets since 1998, many such datasets are out of date and unreliable to use. Some of these datasets suffer from lack of trafﬁc diversity and volumes, some of them do not cover the variety of attacks, while others anonymized packet information and payload which cannot reﬂect the current trends, or they lack feature set and metadata. This paper produces a reliable dataset that contains benign and seven common attack network ﬂows, which meets real world criteria and is publicly avaliable. Consequently, the paper evaluates the performance of a comprehensive set of network trafﬁc features and machine learning algorithms to indicate the best set of features for detecting the certain attack categories.||||Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization|2018|9787897;2037871;1698268|Iman Sharafaldin;Arash Habibi Lashkari;A. Ghorbani|Computer Science;Engineering|96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7|144661829;4334655;2052576935|True;True;True|desc;desc;desc
9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a|10.1093/nar/gky1016|Cell|6.0|5678-9012_6|D886 - D894|Abstract Combined Annotation-Dependent Depletion (CADD) is a widely used measure of variant deleteriousness that can effectively prioritize causal variants in genetic analyses, particularly highly penetrant contributors to severe Mendelian disorders. CADD is an integrative annotation built from more than 60 genomic features, and can score human single nucleotide variants and short insertion and deletions anywhere in the reference assembly. CADD uses a machine learning model trained on a binary distinction between simulated de novo variants and variants that have arisen and become fixed in human populations since the split between humans and chimpanzees; the former are free of selective pressure and may thus include both neutral and deleterious alleles, while the latter are overwhelmingly neutral (or, at most, weakly deleterious) by virtue of having survived millions of years of purifying selection. Here we review the latest updates to CADD, including the most recent version, 1.4, which supports the human genome build GRCh38. We also present updates to our website that include simplified variant lookup, extended documentation, an Application Program Interface and improved mechanisms for integrating CADD scores into other tools or applications. CADD scores, software and documentation are available at https://cadd.gs.washington.edu.||||CADD: predicting the deleteriousness of variants throughout the human genome|2018|80243122;2113693105;31273465;2431330;6534571|Philipp Rentzsch;D. Witten;G. Cooper;J. Shendure;Martin Kircher|Computer Science;Medicine;Biology|9b539d413393047b28bb7be9b195f142aaf7a80e;24e6c5bfe9bb0751e5708b501d04e860011b2953;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;9670485f526f2254c0f34e64d9ca06f665a0bd17;3cee40494377c0e7d9c7c23a3811b481e55bce39;3fea7ba9490c306484a8fdfe94323ff4e009e1c7;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;64be9999b68e12d260ba7423f6b55ffd41552ad3;2bc3644ce4de7fce5812c1455e056649a47c1bbf;9f387ce140c59a44eaeeea590087351461345164;693914b7f38c19585e35668fd626aecf62d4c5e7;98c25683fc8d6446448b734b1bcf08e1457f8d85|1784123;2065760904;1422494434|True;False;True|desc;desc;desc
10f919b1a5161b560504c225cfb2d1b3a4768f80|10.1136/svn-2017-000101||||230 - 243|Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.|UAI|UAI_Tokyo_2017|Tokyo|Artificial intelligence in healthcare: past, present and future|2017|67092021;2117937034;1976425918;1974599;36156845;119918227;47454309;46829048;2108094216|F. Jiang;Yong Jiang;Hui Zhi;Yi Dong;Hao Li;Sufeng Ma;Yilong Wang;Q. Dong;Haipeng Shen;Yongjun Wang|Computer Science;Medicine|4b61c25a86083c20730c9b12737ac6ac4178c364;9670485f526f2254c0f34e64d9ca06f665a0bd17;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;f4a5503783487eba5c5e34b1d02c09016b244b1d;265644f1b6740ca34bfbe9762b90b33021adde62;9d46dc975aeed3f96bddb144079b50238f746ecd;611544418ca53cdad254df444addc7814abcfddc;d05d86db86a4ac0d95e6dcd951b42a9651939793;a7a407968c13ced804a063259d72315a43b84f29;dd971c07879e1ce12b06991319528c06280eeb9b;395de0bd3837fdf4b4b5e5f04835bcc69c279481;d133cb102ad0f81e3fd17a7db090b28afc124c4a;9f387ce140c59a44eaeeea590087351461345164;0b544dfe355a5070b60986319a3f51fb45d1348e;98c25683fc8d6446448b734b1bcf08e1457f8d85;2bc3644ce4de7fce5812c1455e056649a47c1bbf;f94455176857303605ad423599385a2341c568eb|2108989415;1698268;2380233|True;True;False|desc;desc;desc
3fea7ba9490c306484a8fdfe94323ff4e009e1c7|10.1002/hbm.23730||||5391 - 5420|Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end‐to‐end learning, that is, learning from the raw data. There is increasing interest in using deep ConvNets for end‐to‐end EEG analysis, but a better understanding of how to design and train ConvNets for end‐to‐end EEG decoding and how to visualize the informative EEG features the ConvNets learn is still needed. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed tasks from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching at least as good performance as the widely used filter bank common spatial patterns (FBCSP) algorithm (mean decoding accuracies 82.1% FBCSP, 84.0% deep ConvNets). While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta, and high gamma frequencies, and proved useful for spatially mapping the learned features by revealing the topography of the causal contributions of features in different frequency bands to the decoding decision. Our study thus shows how to design and train ConvNets to decode task‐related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG‐based brain mapping. Hum Brain Mapp 38:5391–5420, 2017. © 2017 Wiley Periodicals, Inc.|NeurIPS|NeurIPS_Auckland_2017|Auckland|Deep learning with convolutional neural networks for EEG decoding and visualization|2017|9948572;2060551;1704186;9936842;2607675;2345823;144661829;2106871731;145928182|R. Schirrmeister;Jost Tobias Springenberg;L. Fiederer;M. Glasstetter;Katharina Eggensperger;M. Tangermann;F. Hutter;Wolfram Burgard;T. Ball|Computer Science;Medicine|66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;4b61c25a86083c20730c9b12737ac6ac4178c364;9d46dc975aeed3f96bddb144079b50238f746ecd;d517b13f2b152c913b81ce534a149493517dbdad;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;a7a407968c13ced804a063259d72315a43b84f29|51289782;12155787;37257989|True;True;True|desc;desc;desc
a486e2839291111bb44fa1f07731ada123539f75|10.1162/tacl_a_00065|BMJ|19.0|8901-2345_19|339-351|We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.||||Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation|2016|2109675545;144927151;2827616;2048712;48607963;2545358;144203200;1765169;145233583;2227182886;48342565;2056946837|Melvin Johnson;M. Schuster;Quoc V. Le;M. Krikun;Yonghui Wu;Z. Chen;Nikhil Thorat;F. Viégas;M. Wattenberg;Gregory S. Corrado;Macduff Hughes;Jeffrey Dean|Computer Science;Linguistics|24e6c5bfe9bb0751e5708b501d04e860011b2953;a7a407968c13ced804a063259d72315a43b84f29;d133cb102ad0f81e3fd17a7db090b28afc124c4a;4b61c25a86083c20730c9b12737ac6ac4178c364;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;f4a5503783487eba5c5e34b1d02c09016b244b1d;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;d05d86db86a4ac0d95e6dcd951b42a9651939793;cd49acefc8d51e324aa562e5337e1c2aff067053;265644f1b6740ca34bfbe9762b90b33021adde62;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;9670485f526f2254c0f34e64d9ca06f665a0bd17;dd971c07879e1ce12b06991319528c06280eeb9b;546785490ac417be1f83ced6a8272e934934f411;3cee40494377c0e7d9c7c23a3811b481e55bce39;94549a171a61039ed1f9b5954ce42181c574ccc3;38f23fe236b152cd4983c8f30d305a568afd0d3e|47479574;2117937034;5929246|True;True;False|desc;desc;desc
94549a171a61039ed1f9b5954ce42181c574ccc3|10.1093/nar/gkp356||||W652 - W660|Metabolomics is a newly emerging field of ‘omics’ research that is concerned with characterizing large numbers of metabolites using NMR, chromatography and mass spectrometry. It is frequently used in biomarker identification and the metabolic profiling of cells, tissues or organisms. The data processing challenges in metabolomics are quite unique and often require specialized (or expensive) data analysis software and a detailed knowledge of cheminformatics, bioinformatics and statistics. In an effort to simplify metabolomic data analysis while at the same time improving user accessibility, we have developed a freely accessible, easy-to-use web server for metabolomic data analysis called MetaboAnalyst. Fundamentally, MetaboAnalyst is a web-based metabolomic data processing tool not unlike many of today's web-based microarray analysis packages. It accepts a variety of input data (NMR peak lists, binned spectra, MS peak lists, compound/concentration data) in a wide variety of formats. It also offers a number of options for metabolomic data processing, data normalization, multivariate statistical analysis, graphing, metabolite identification and pathway mapping. In particular, MetaboAnalyst supports such techniques as: fold change analysis, t-tests, PCA, PLS-DA, hierarchical clustering and a number of more sophisticated statistical or machine learning methods. It also employs a large library of reference spectra to facilitate compound identification from most kinds of input spectra. MetaboAnalyst guides users through a step-by-step analysis pipeline using a variety of menus, information hyperlinks and check boxes. Upon completion, the server generates a detailed report describing each method used, embedded with graphical and tabular outputs. MetaboAnalyst is capable of handling most kinds of metabolomic data and was designed to perform most of the common kinds of metabolomic data analyses. MetaboAnalyst is accessible at http://www.metaboanalyst.ca|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Moscow_2009|Moscow|MetaboAnalyst: a web server for metabolomic data analysis and interpretation|2009|144545434;2722839;36093977;2066145|J. Xia;N. Psychogios;N. Young;D. Wishart|Computer Science;Chemistry;Medicine;Biology|38f23fe236b152cd4983c8f30d305a568afd0d3e;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;2bc3644ce4de7fce5812c1455e056649a47c1bbf;12d1d070a53d4084d88a77b8b143bad51c40c38f;cd49acefc8d51e324aa562e5337e1c2aff067053;44c7d9fe583e3d317a619297e7e949070710799f;f4a5503783487eba5c5e34b1d02c09016b244b1d;0b544dfe355a5070b60986319a3f51fb45d1348e;546785490ac417be1f83ced6a8272e934934f411|15186612;92480907;2108348149|True;True;True|desc;desc;desc
ade03d0c772c35dc8e865bdb41d7bc54d5b782d1|10.18637/JSS.V011.I09||||1-20|kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.|ICML|ICML_Beijing_2004|Beijing|kernlab - An S4 Package for Kernel Methods in R|2004|1713164;116865041;1764952;2144516|Alexandros Karatzoglou;A. Smola;K. Hornik;A. Zeileis|Computer Science;Mathematics|3fea7ba9490c306484a8fdfe94323ff4e009e1c7|46829048;2113670271;2068205528|True;True;False|desc;desc;desc
f94455176857303605ad423599385a2341c568eb|10.1093/nar/gky318|Frontiers for Young Minds|89.0|2345-6789_89|W257 - W263|Abstract Advancement in the field of computational research has made it possible for the in silico methods to offer significant benefits to both regulatory needs and requirements for risk assessments, and pharmaceutical industry to assess the safety profile of a chemical. Here, we present ProTox-II that incorporates molecular similarity, pharmacophores, fragment propensities and machine-learning models for the prediction of various toxicity endpoints; such as acute toxicity, hepatotoxicity, cytotoxicity, carcinogenicity, mutagenicity, immunotoxicity, adverse outcomes pathways (Tox21) and toxicity targets. The predictive models are built on data from both in vitro assays (e.g. Tox21 assays, Ames bacterial mutation assays, hepG2 cytotoxicity assays, Immunotoxicity assays) and in vivo cases (e.g. carcinogenicity, hepatotoxicity). The models have been validated on independent external sets and have shown strong performance. ProTox-II provides a freely available webserver for in silico toxicity prediction for toxicologists, regulatory agencies, computational and medicinal chemists, and all users without login at http://tox.charite.de/protox_II. The webserver takes a two-dimensional chemical structure as an input and reports the possible toxicity profile of the chemical for 33 models with confidence scores, and an overall toxicity radar chart along with three most similar compounds with known acute toxicity.||||ProTox-II: a webserver for the prediction of toxicity of chemicals|2018|48180545;2069534421;32721322;2785761|Priyanka Banerjee;Andreas Eckert;Anna K. Schrey;R. Preissner|Chemistry;Biology;Medicine;Computer Science;Environmental Science|cd49acefc8d51e324aa562e5337e1c2aff067053;12d1d070a53d4084d88a77b8b143bad51c40c38f;38f23fe236b152cd4983c8f30d305a568afd0d3e;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5|2135937978;31461304;39589154|True;False;True|desc;desc;desc
8c8215b7f8111839f0066010a530a3a9f57ba15e|10.1613/jair.530|The Lancet|25.0|6789-0123_25|317-365|This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.||||AntNet: Distributed Stigmergetic Control for Communications Networks|1998|1744127;153570946|G. D. Caro;M. Dorigo|Computer Science|98c25683fc8d6446448b734b1bcf08e1457f8d85;dd9b99fac67c18be82d7763a8fbf231fc3512423;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;9d46dc975aeed3f96bddb144079b50238f746ecd;339a8e4cb0eba77675711ac255ac2a5d7ede1d53|6288635;2407842;2108034421|True;True;True|desc;desc;desc
d0ab11de3077490c80a08abd0fb8827bac84c454|10.1039/c7sc02664a|JACC|45.0|1234-5678_45|513 - 530|A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.||||MoleculeNet: a benchmark for molecular machine learning|2017|9957625;2378027;5932099;145986494;2347660128;5929246;40867019;1806271|Zhenqin Wu;Bharath Ramsundar;Evan N. Feinberg;Joseph Gomes;Caleb Geniesse;Aneesh S. Pappu;K. Leswing;V. Pande|Chemistry;Physics;Medicine;Computer Science;Mathematics|f4a5503783487eba5c5e34b1d02c09016b244b1d;611544418ca53cdad254df444addc7814abcfddc;10f919b1a5161b560504c225cfb2d1b3a4768f80;12d1d070a53d4084d88a77b8b143bad51c40c38f;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;9f387ce140c59a44eaeeea590087351461345164;b3de1062d8a462dfdc2938558258f8884abe9f4e;0b544dfe355a5070b60986319a3f51fb45d1348e;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;61e27dbae190b82639c57f180ecf97e4c46fcad9;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593|145928182;121557847;33340370|True;False;True|desc;desc;desc
9f387ce140c59a44eaeeea590087351461345164|10.19026/RJASET.5.4644||||4168-4182|Recommender Systems are software tools and techniques for suggesting items to users by considering their preferences in an automated fashion. The suggestions provided are aimed at support users in various decision- making processes. Technically, recommender system has their origins in different fields such as Information Retrieval (IR), text classification, machine learning and Decision Support Systems (DSS). Recommender systems are used to address the Information Overload (IO) problem by recommending potentially interesting or useful items to users. They have proven to be worthy tools for online users to deal with the IO and have become one of the most popular and powerful tools in E-commerce. Many existing recommender systems rely on the Collaborative Filtering (CF) and have been extensively used in E-commerce .They have proven to be very effective with powerful techniques in many famous E-commerce companies. This study presents an overview of the field of recommender systems with current generation of recommendation methods and examines comprehensively CF systems with its algorithms.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Rio_de_Janeiro_2013|Rio de Janeiro|Collaborative filtering recommender systems|2013|3095698;19381012;66083398;40397208;70627370;70547150|M. Nilashi;Karamollah Bagherifard;O. Ibrahim;H. Alizadeh;L. Nojeem;Nazanin Roozegar|Computer Science|d133cb102ad0f81e3fd17a7db090b28afc124c4a;12d1d070a53d4084d88a77b8b143bad51c40c38f;61e27dbae190b82639c57f180ecf97e4c46fcad9;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;98c25683fc8d6446448b734b1bcf08e1457f8d85;a7a407968c13ced804a063259d72315a43b84f29;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;24e6c5bfe9bb0751e5708b501d04e860011b2953;3fea7ba9490c306484a8fdfe94323ff4e009e1c7;9b539d413393047b28bb7be9b195f142aaf7a80e;64be9999b68e12d260ba7423f6b55ffd41552ad3;693914b7f38c19585e35668fd626aecf62d4c5e7;9d46dc975aeed3f96bddb144079b50238f746ecd|48999874;46486553;134905027|True;True;False|desc;desc;desc
7b2dd79083a74699e4e0509ac3f0a8a302b4eabe|10.1090/S0273-0979-01-00923-5||||1-49|(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In|AAMAS|AAMAS_Sydney_2001|Sydney|On the mathematical foundations of learning|2001|1755208;34911188|F. Cucker;S. Smale|Computer Science;Mathematics;Education|dd9b99fac67c18be82d7763a8fbf231fc3512423;a486e2839291111bb44fa1f07731ada123539f75|1772842;46322047;1709512|False;True;True|desc;desc;desc
dd971c07879e1ce12b06991319528c06280eeb9b|10.5167/UZH-185139|The Lancet|48.0|6789-0123_48|154-180|Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.||||Event-Based Vision: A Survey|2019|144036711;1694635;33780923;1897771;1736425;1860631;2864731;2052135690;3302681;1751586;2075371|Guillermo Gallego;T. Delbrück;G. Orchard;C. Bartolozzi;B. Taba;A. Censi;Stefan Leutenegger;A. Davison;J. Conradt;Kostas Daniilidis;D. Scaramuzza|Computer Science;Medicine;Physics;Engineering|b5887d18420e8ac4f4fa4c83c4952138fd956702;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;d133cb102ad0f81e3fd17a7db090b28afc124c4a;d0ab11de3077490c80a08abd0fb8827bac84c454;0b544dfe355a5070b60986319a3f51fb45d1348e;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;546785490ac417be1f83ced6a8272e934934f411;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;9b539d413393047b28bb7be9b195f142aaf7a80e;a486e2839291111bb44fa1f07731ada123539f75;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;38f23fe236b152cd4983c8f30d305a568afd0d3e;d05d86db86a4ac0d95e6dcd951b42a9651939793;61e27dbae190b82639c57f180ecf97e4c46fcad9|2117937034;15309498;2110603432|True;True;True|desc;desc;desc
cd49acefc8d51e324aa562e5337e1c2aff067053|10.1093/NSR/NWX105|The Lancet|86.0|6789-0123_86|30-43|As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.||||An Overview of Multi-task Learning|2018|46867608;153096457|Yu Zhang;Qiang Yang|Computer Science|7ea35b35392c6ef5738635cec7d17b24fe3e4f04;4b61c25a86083c20730c9b12737ac6ac4178c364;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;d05d86db86a4ac0d95e6dcd951b42a9651939793;265644f1b6740ca34bfbe9762b90b33021adde62;a27089efabc5f4abd5ddf2be2a409bff41f31199;2bc3644ce4de7fce5812c1455e056649a47c1bbf;5a4631d5d75e3610037f87839628a4d166581e01;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;9f387ce140c59a44eaeeea590087351461345164;a7a407968c13ced804a063259d72315a43b84f29|123257978;40459179;2115670|True;False;True|desc;desc;desc
f8b7a3434f887ce4570b7e98c7f1b91c008042d4|10.1093/nar/gkx1067||||D296 - D302|Abstract MicroRNAs (miRNAs) are small non-coding RNAs of ∼ 22 nucleotides that are involved in negative regulation of mRNA at the post-transcriptional level. Previously, we developed miRTarBase which provides information about experimentally validated miRNA-target interactions (MTIs). Here, we describe an updated database containing 422 517 curated MTIs from 4076 miRNAs and 23 054 target genes collected from over 8500 articles. The number of MTIs curated by strong evidence has increased ∼1.4-fold since the last update in 2016. In this updated version, target sites validated by reporter assay that are available in the literature can be downloaded. The target site sequence can extract new features for analysis via a machine learning approach which can help to evaluate the performance of miRNA-target prediction tools. Furthermore, different ways of browsing enhance user browsing specific MTIs. With these improvements, miRTarBase serves as more comprehensively annotated, experimentally validated miRNA-target interactions databases in the field of miRNA related research. miRTarBase is available at http://miRTarBase.mbc.nctu.edu.tw/.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Beijing_2017|Beijing|miRTarBase update 2018: a resource for experimentally validated microRNA-target interactions|2017|2115670;4334655;2087279;2065682157;1742112;144580498;2112109655;1393662331;37257989;2111210;1393935479;46385221;2068205528;2536315;151487590;2118105521;3207228;2451356;2184327;40995886;49180504;2107949987;47482843;2145670006;2207015236;2152662291;89103582;8243291;7889032;152912047;2110603432;2274883;35556316;143869250;144505734;2146046695|Chih-Hung Chou;S. Shrestha;Chi-Dung Yang;Nai-Wen Chang;Yu-Ling Lin;K. Liao;Wei-Chih Huang;Ting-Hsuan Sun;Siang-Jyun Tu;Wei-Hsiang Lee;Men-Yee Chiew;Chun-San Tai;Ting-Yen Wei;Tzi-Ren Tsai;Hsin-Tzu Huang;Chung-Yu Wang;Hsin‐Yi Wu;S. Ho;Pin-Rong Chen;Cheng-Hsun Chuang;Pei-Jung Hsieh;Yi-Shin Wu;Wen-Liang Chen;Mengge Li;Yu-chun Wu;Xin-Yi Huang;Fung-Ling Ng;W. Buddhakosai;P. Huang;K. Lan;Chia-Yen Huang;Shun-Long Weng;Yeong-Nan Cheng;Chao Liang;W. Hsu;Hsien-Da Huang|Computer Science;Medicine;Biology|3cee40494377c0e7d9c7c23a3811b481e55bce39;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;9670485f526f2254c0f34e64d9ca06f665a0bd17;0b544dfe355a5070b60986319a3f51fb45d1348e;24e6c5bfe9bb0751e5708b501d04e860011b2953;64be9999b68e12d260ba7423f6b55ffd41552ad3;2bc3644ce4de7fce5812c1455e056649a47c1bbf|145836900;3450866;1694635|True;True;True|desc;desc;desc
2369db9921078c4bb76072ef7d6426e9f1dbfdb5|10.1155/2015/258619||||258619:1-258619:12|Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Beijing_2015|Beijing|Deep Convolutional Neural Networks for Hyperspectral Image Classification|2015|145066193;7524887;2157336032;31292557;51330074|Wei Hu;Yangyu Huang;Wei Li;Fan Zhang;Hengchao Li|Computer Science;Environmental Science|0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;395de0bd3837fdf4b4b5e5f04835bcc69c279481;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;2bc3644ce4de7fce5812c1455e056649a47c1bbf;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;a486e2839291111bb44fa1f07731ada123539f75;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;9b539d413393047b28bb7be9b195f142aaf7a80e;d133cb102ad0f81e3fd17a7db090b28afc124c4a;98c25683fc8d6446448b734b1bcf08e1457f8d85;44c7d9fe583e3d317a619297e7e949070710799f;8c8215b7f8111839f0066010a530a3a9f57ba15e;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;611544418ca53cdad254df444addc7814abcfddc;f94455176857303605ad423599385a2341c568eb;0b544dfe355a5070b60986319a3f51fb45d1348e;7ea35b35392c6ef5738635cec7d17b24fe3e4f04|1669767608;1772842;1709512|False;True;False|desc;desc;desc
9d3e0fce253a4ae4a4456b2f24c03329a2b74621|10.1109/JBHI.2016.2636665|NEJM|48.0|9012-3456_48|4-21|With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.||||Deep Learning for Health Informatics|2017|2347855602;1905807;2775904;3163767;1443783456;1745644;144574968|Daniele Ravì;Charence Wong;F. Deligianni;M. Berthelot;Javier Andreu-Perez;Benny P. L. Lo;Guang-Zhong Yang|Computer Science;Medicine||2213770;9936842;47479574|True;False;True|desc;desc;desc
3cee40494377c0e7d9c7c23a3811b481e55bce39|10.1093/nar/gkq275||||e132 - e132|We describe an algorithm for gene identification in DNA sequences derived from shotgun sequencing of microbial communities. Accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters. While several machine learning approaches could be proposed to bypass this difficulty, one effective method is to estimate parameters from dependencies, formed in evolution, between frequencies of oligonucleotides in protein-coding regions and genome nucleotide composition. Original version of the method was proposed in 1999 and has been used since for (i) reconstructing codon frequency vector needed for gene finding in viral genomes and (ii) initializing parameters of self-training gene finding algorithms. With advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies, as well as by separating models for bacteria and archaea. These advances have increased the accuracy of model reconstruction and, subsequently, gene prediction. We describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences. Also, we show that as a result of application of the new method, several thousands of new genes could be added to existing annotations of several human and mouse gut metagenomes.|CVPR|CVPR_Cairo_2010|Cairo|Ab initio gene identification in metagenomic sequences|2010|2113953;3194485;2715148|Wenhan Zhu;A. Lomsadze;M. Borodovsky|Computer Science;Medicine;Biology;Environmental Science|f94455176857303605ad423599385a2341c568eb;0b544dfe355a5070b60986319a3f51fb45d1348e;dd971c07879e1ce12b06991319528c06280eeb9b;a27089efabc5f4abd5ddf2be2a409bff41f31199;d133cb102ad0f81e3fd17a7db090b28afc124c4a;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;9b539d413393047b28bb7be9b195f142aaf7a80e;f4a5503783487eba5c5e34b1d02c09016b244b1d;cd49acefc8d51e324aa562e5337e1c2aff067053;dd9b99fac67c18be82d7763a8fbf231fc3512423;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;d0ab11de3077490c80a08abd0fb8827bac84c454;98c25683fc8d6446448b734b1bcf08e1457f8d85;cbac8b0d82ea8e9251d5530695841d816cb196b9;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;4b61c25a86083c20730c9b12737ac6ac4178c364;a486e2839291111bb44fa1f07731ada123539f75;12d1d070a53d4084d88a77b8b143bad51c40c38f|145684421;35099951;3177854|True;False;True|desc;desc;desc
d133cb102ad0f81e3fd17a7db090b28afc124c4a|10.1103/PhysRevLett.120.145301||||"
          145301
        "|The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_2017|Lisbon|Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.|2017|49902007;2721737|T. Xie;J. Grossman|Chemistry;Physics;Materials Science;Medicine;Computer Science|5ded2b8c64491b4a67f6d39ce473d4b9347a672e;9d46dc975aeed3f96bddb144079b50238f746ecd;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;546785490ac417be1f83ced6a8272e934934f411;cbac8b0d82ea8e9251d5530695841d816cb196b9|48607963;2217848089;4198024|True;True;True|desc;desc;desc
dd9b99fac67c18be82d7763a8fbf231fc3512423|10.1109/TEVC.2015.2504420||||606-626|Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_New_York_2016|New York|A Survey on Evolutionary Computation Approaches to Feature Selection|2016|144395433;145269712;2309030;143901532|Bing Xue;Mengjie Zhang;Will N. Browne;X. Yao|Computer Science|10f919b1a5161b560504c225cfb2d1b3a4768f80;dd971c07879e1ce12b06991319528c06280eeb9b;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;0b544dfe355a5070b60986319a3f51fb45d1348e;b5887d18420e8ac4f4fa4c83c4952138fd956702;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;d133cb102ad0f81e3fd17a7db090b28afc124c4a;5a4631d5d75e3610037f87839628a4d166581e01;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;611544418ca53cdad254df444addc7814abcfddc;d05d86db86a4ac0d95e6dcd951b42a9651939793|39455775;1753652328;145979410|True;True;True|desc;desc;desc
b3de1062d8a462dfdc2938558258f8884abe9f4e|10.1080/01431161.2018.1433343|NEJM|55.0|9012-3456_55|2784 - 2817|ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.||||Implementation of machine-learning classification in remote sensing: an applied review|2018|26339328;5875612;2014814754|Aaron E. Maxwell;T. Warner;Fang Fang|Computer Science;Environmental Science|42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;24e6c5bfe9bb0751e5708b501d04e860011b2953;395de0bd3837fdf4b4b5e5f04835bcc69c279481|3302681;2220290014;1784123|True;True;True|desc;desc;desc
38f23fe236b152cd4983c8f30d305a568afd0d3e|10.1109/TNNLS.2020.3027314|PNAS|70.0|4567-8901_70|4793-4813|Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.||||A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI|2019|71352570;145836900|Erico Tjoa;Cuntai Guan|Computer Science;Medicine|3cee40494377c0e7d9c7c23a3811b481e55bce39|2340666;2070956511;1742112|True;False;True|desc;desc;desc
0ef9ae1ce8c91ce671a211bdda792bf3752d1522|10.1109/ACCESS.2017.2762418||||21954-21961|Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Toronto_2017|Toronto|A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks|2017|30796665;1733625;2191560;50046030|Chuanlong Yin;Yuefei Zhu;Jin-long Fei;Xin-Zheng He|Computer Science;Engineering|0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa|37936174;50009466;1852549|True;True;True|desc;desc;desc
f4a5503783487eba5c5e34b1d02c09016b244b1d|10.18653/v1/D18-1547||||5016-5026|Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.|IJCAI|IJCAI_Moscow_2018|Moscow|MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling|2018|17895970;144256365;33870107;3450866;2295429;2065760904;1768624|Paweł Budzianowski;Tsung-Hsien Wen;Bo-Hsiang Tseng;I. Casanueva;Stefan Ultes;Osman Ramadan;Milica Gasic|Computer Science;Linguistics|a27089efabc5f4abd5ddf2be2a409bff41f31199;d0ab11de3077490c80a08abd0fb8827bac84c454;64be9999b68e12d260ba7423f6b55ffd41552ad3;9b539d413393047b28bb7be9b195f142aaf7a80e;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;a486e2839291111bb44fa1f07731ada123539f75;d133cb102ad0f81e3fd17a7db090b28afc124c4a;9f387ce140c59a44eaeeea590087351461345164|144203200;2295429;2207015236|True;True;True|desc;desc;desc
693914b7f38c19585e35668fd626aecf62d4c5e7|10.18637/JSS.V025.I05|Nature|87.0|1234-5678_87|1-54|During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysis methods, text clustering, text classification and string kernels.||||Text Mining Infrastructure in R|2008|2711164;1764952;153939986|Ingo Feinerer;K. Hornik;David Meyer|Computer Science;Mathematics|12d1d070a53d4084d88a77b8b143bad51c40c38f;611544418ca53cdad254df444addc7814abcfddc;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;5a4631d5d75e3610037f87839628a4d166581e01;4b61c25a86083c20730c9b12737ac6ac4178c364;24e6c5bfe9bb0751e5708b501d04e860011b2953;395de0bd3837fdf4b4b5e5f04835bcc69c279481;f94455176857303605ad423599385a2341c568eb;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;8c8215b7f8111839f0066010a530a3a9f57ba15e|2056204271;3698245;117461896|True;True;True|desc;desc;desc
5a4631d5d75e3610037f87839628a4d166581e01|10.1093/nar/gkaa379||||W449 - W454|Abstract Major histocompatibility complex (MHC) molecules are expressed on the cell surface, where they present peptides to T cells, which gives them a key role in the development of T-cell immune responses. MHC molecules come in two main variants: MHC Class I (MHC-I) and MHC Class II (MHC-II). MHC-I predominantly present peptides derived from intracellular proteins, whereas MHC-II predominantly presents peptides from extracellular proteins. In both cases, the binding between MHC and antigenic peptides is the most selective step in the antigen presentation pathway. Therefore, the prediction of peptide binding to MHC is a powerful utility to predict the possible specificity of a T-cell immune response. Commonly MHC binding prediction tools are trained on binding affinity or mass spectrometry-eluted ligands. Recent studies have however demonstrated how the integration of both data types can boost predictive performances. Inspired by this, we here present NetMHCpan-4.1 and NetMHCIIpan-4.0, two web servers created to predict binding between peptides and MHC-I and MHC-II, respectively. Both methods exploit tailored machine learning strategies to integrate different training data types, resulting in state-of-the-art performance and outperforming their competitors. The servers are available at http://www.cbs.dtu.dk/services/NetMHCpan-4.1/ and http://www.cbs.dtu.dk/services/NetMHCIIpan-4.0/.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Beijing_2020|Beijing|NetMHCpan-4.1 and NetMHCIIpan-4.0: improved predictions of MHC antigen presentation by concurrent motif deconvolution and integration of MS MHC eluted ligand data|2020|89715134;144020846;3071266;145580998;143898643|Birkir Reynisson;Bruno Alvarez;S. Paul;Bjoern Peters;M. Nielsen|Computer Science;Medicine;Biology|b5887d18420e8ac4f4fa4c83c4952138fd956702;9d46dc975aeed3f96bddb144079b50238f746ecd;b3de1062d8a462dfdc2938558258f8884abe9f4e;dd971c07879e1ce12b06991319528c06280eeb9b;693914b7f38c19585e35668fd626aecf62d4c5e7;a27089efabc5f4abd5ddf2be2a409bff41f31199;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;f94455176857303605ad423599385a2341c568eb;9b539d413393047b28bb7be9b195f142aaf7a80e|1876480;1683459;4023283|False;True;True|desc;desc;desc
9670485f526f2254c0f34e64d9ca06f665a0bd17|10.21037/jtd.2020.02.64||||165 - 174|Background The coronavirus disease 2019 (COVID-19) outbreak originating in Wuhan, Hubei province, China, coincided with chunyun, the period of mass migration for the annual Spring Festival. To contain its spread, China adopted unprecedented nationwide interventions on January 23 2020. These policies included large-scale quarantine, strict controls on travel and extensive monitoring of suspected cases. However, it is unknown whether these policies have had an impact on the epidemic. We sought to show how these control measures impacted the containment of the epidemic. Methods We integrated population migration data before and after January 23 and most updated COVID-19 epidemiological data into the Susceptible-Exposed-Infectious-Removed (SEIR) model to derive the epidemic curve. We also used an artificial intelligence (AI) approach, trained on the 2003 SARS data, to predict the epidemic. Results We found that the epidemic of China should peak by late February, showing gradual decline by end of April. A five-day delay in implementation would have increased epidemic size in mainland China three-fold. Lifting the Hubei quarantine would lead to a second epidemic peak in Hubei province in mid-March and extend the epidemic to late April, a result corroborated by the machine learning prediction. Conclusions Our dynamic SEIR model was effective in predicting the COVID-19 epidemic peaks and sizes. The implementation of control measures on January 23 2020 was indispensable in reducing the eventual COVID-19 epidemic size.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Madrid_2020|Madrid|Modified SEIR and AI prediction of the epidemics trend of COVID-19 in China under public health interventions|2020|48598873;30489614;1753652328;113837271;2340666;40459179;2217848089;2125517703;2292805636;134905027;1492076802;1491359904;1390903788;89843211;145371438;40527353;2108989415;2115375292;1576228528;1663359881;123624101;1626824540;2145443647;2119049448;145002796;144869300|Zi-feng Yang;Zhiqi Zeng;Ke Wang;Sook-san Wong;W. Liang;Mark Zanin;Peng Liu;Xudong Cao;Zhongqiang Gao;Zhitong Mai;Jingyi Liang;Xiaoqing Liu;Shiyue Li;Yimin Li;F. Ye;W. Guan;Yifan Yang;Fei Li;S. Luo;Yuqi Xie;Bin Liu;Zhoulang Wang;Shaobo Zhang;Yao-dong Wang;N. Zhong;Jianxing He|Medicine;Environmental Science|61e27dbae190b82639c57f180ecf97e4c46fcad9;d0ab11de3077490c80a08abd0fb8827bac84c454;d517b13f2b152c913b81ce534a149493517dbdad;9b539d413393047b28bb7be9b195f142aaf7a80e;8c8215b7f8111839f0066010a530a3a9f57ba15e;38f23fe236b152cd4983c8f30d305a568afd0d3e;cd49acefc8d51e324aa562e5337e1c2aff067053;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;265644f1b6740ca34bfbe9762b90b33021adde62;2bc3644ce4de7fce5812c1455e056649a47c1bbf|2607675;2112890973;16126046|True;False;True|desc;desc;desc
4b61c25a86083c20730c9b12737ac6ac4178c364|10.1561/2200000071||||219-354|Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Sydney_2018|Sydney|An Introduction to Deep Reinforcement Learning|2018|1389921282;40068904;18014232;1792298;145134886|Vincent François-Lavet;Peter Henderson;Riashat Islam;Marc G. Bellemare;Joelle Pineau|Computer Science;Mathematics|44c7d9fe583e3d317a619297e7e949070710799f;d05d86db86a4ac0d95e6dcd951b42a9651939793;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5|1578100923;1979489;6486577|True;True;False|desc;desc;desc
42ed4a9994e6121a9f325f5b901c5b3d7ce104f5|10.18653/v1/P19-1334||||3428-3448|A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.|ICCV|ICCV_Buenos_Aires_2019|Buenos Aires|Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference|2019|145534175;2949185;2467508|R. Thomas McCoy;Ellie Pavlick;Tal Linzen|Computer Science;Linguistics|7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;0b544dfe355a5070b60986319a3f51fb45d1348e;3cee40494377c0e7d9c7c23a3811b481e55bce39;9f387ce140c59a44eaeeea590087351461345164;44c7d9fe583e3d317a619297e7e949070710799f;d133cb102ad0f81e3fd17a7db090b28afc124c4a;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;12d1d070a53d4084d88a77b8b143bad51c40c38f;24e6c5bfe9bb0751e5708b501d04e860011b2953;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;a27089efabc5f4abd5ddf2be2a409bff41f31199;98c25683fc8d6446448b734b1bcf08e1457f8d85;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;693914b7f38c19585e35668fd626aecf62d4c5e7;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;395de0bd3837fdf4b4b5e5f04835bcc69c279481;b3de1062d8a462dfdc2938558258f8884abe9f4e;9d46dc975aeed3f96bddb144079b50238f746ecd|2191751245;2069534421;1982950|False;True;False|desc;desc;desc
3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0|10.1093/brain/awm319||||"
          681-9
        "|To be diagnostically useful, structural MRI must reliably distinguish Alzheimer's disease (AD) from normal aging in individual scans. Recent advances in statistical learning theory have led to the application of support vector machines to MRI for detection of a variety of disease states. The aims of this study were to assess how successfully support vector machines assigned individual diagnoses and to determine whether data-sets combined from multiple scanners and different centres could be used to obtain effective classification of scans. We used linear support vector machines to classify the grey matter segment of T1-weighted MR scans from pathologically proven AD patients and cognitively normal elderly individuals obtained from two centres with different scanning equipment. Because the clinical diagnosis of mild AD is difficult we also tested the ability of support vector machines to differentiate control scans from patients without post-mortem confirmation. Finally we sought to use these methods to differentiate scans between patients suffering from AD from those with frontotemporal lobar degeneration. Up to 96% of pathologically verified AD patients were correctly classified using whole brain images. Data from different centres were successfully combined achieving comparable results from the separate analyses. Importantly, data from one centre could be used to train a support vector machine to accurately differentiate AD and normal ageing scans obtained from another centre with different subjects and different scanner equipment. Patients with mild, clinically probable AD and age/sex matched controls were correctly separated in 89% of cases which is compatible with published diagnosis rates in the best clinical centres. This method correctly assigned 89% of patients with post-mortem confirmed diagnosis of either AD or frontotemporal lobar degeneration to their respective group. Our study leads to three conclusions: Firstly, support vector machines successfully separate patients with AD from healthy aging subjects. Secondly, they perform well in the differential diagnosis of two different forms of dementia. Thirdly, the method is robust and can be generalized across different centres. This suggests an important role for computer based diagnostic image analysis for clinical practice.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Los_Angeles_2008|Los Angeles|Automatic classification of MR scans in Alzheimer's disease.|2008|144225920;2609123;145606181;7447906;116292660;1772842;1764768;144402064;3985221;52216909|S. Klöppel;C. Stonnington;C. Chu;Bogdan Draganski;R. Scahill;J. Rohrer;Nick C Fox;C. Jack;J. Ashburner;Richard S. J. Frackowiak|Computer Science;Medicine;Psychology|b5887d18420e8ac4f4fa4c83c4952138fd956702;0b544dfe355a5070b60986319a3f51fb45d1348e;5a4631d5d75e3610037f87839628a4d166581e01;9670485f526f2254c0f34e64d9ca06f665a0bd17;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa;d0ab11de3077490c80a08abd0fb8827bac84c454;339a8e4cb0eba77675711ac255ac2a5d7ede1d53;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;98c25683fc8d6446448b734b1bcf08e1457f8d85;a27089efabc5f4abd5ddf2be2a409bff41f31199;cd49acefc8d51e324aa562e5337e1c2aff067053;d517b13f2b152c913b81ce534a149493517dbdad;2369db9921078c4bb76072ef7d6426e9f1dbfdb5|1974599;2112109655;9957625|True;False;False|desc;desc;desc
546785490ac417be1f83ced6a8272e934934f411|10.1093/annonc/mdy166||||1836–1842|"Background
Deep learning convolutional neural networks (CNN) may facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking.


Methods
Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge.


Results
In level-I dermatologists achieved a mean (±standard deviation) sensitivity and specificity for lesion classification of 86.6% (±9.3%) and 71.3% (±11.2%), respectively. More clinical information (level-II) improved the sensitivity to 88.9% (±9.6%, P = 0.19) and specificity to 75.7% (±11.7%, P < 0.05). The CNN ROC curve revealed a higher specificity of 82.5% when compared with dermatologists in level-I (71.3%, P < 0.01) and level-II (75.7%, P < 0.01) at their sensitivities of 86.6% and 88.9%, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P < 0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.


Conclusions
For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they may benefit from assistance by a CNN's image classification.


Clinical trial number
This study was registered at the German Clinical Trial Register (DRKS-Study-ID: DRKS00013570; https://www.drks.de/drks_web/)."|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cape_Town_2018|Cape Town|Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists|2018|5016554;49903386;5580228;2220290014;2223688580;2154835838;34445787;2080112167;2113670271;5840101;2213770;153454907;4023283;6486577;32147295;8735139;2154835838;1404268156;40398486;2135937978;2056204271;1422494434;3026399;4039707;8720212;2135937679;50009466;1471206276;33340370;144821090;1401706768;6248858;46486553;51289782;46560749;49551702;51952495;32516743;32084575;3698245;16126046;2348967;2842984;12155787;12565475;2056679749;46322047;36030514;2420136;4198024;145295514;2135990561;123257978;38676513;37936174;49785210;2102253685;5580228;1485389319;2220290014;41065919;2136001275;146218865;2055574299;117461896;6288635|H. Haenssle;C. Fink;R. Schneiderbauer;F. Toberer;T. Buhl;A. Blum;Aadi Kalloo;A. Hassen;L. Thomas;A. Enk;L. Uhlmann;C. Alt;M. Arenbergerova;R. Bakos;Anne Baltzer;I. Bertlich;A. Blum;Therezia Bokor-Billmann;J. Bowling;Naira Braghiroli;R. Braun;K. Buder-Bakhaya;T. Buhl;H. Cabo;L. Čabrijan;Naciye Cevic;A. Classen;David Deltgen;C. Fink;I. Georgieva;L. Hakim-Meibodi;Susanne Hanner;Franziska Hartmann;J. Hartmann;G. Haus;E. Hoxha;R. Karls;H. Koga;J. Kreusch;A. Lallas;P. Majenka;A. Marghoob;C. Massone;L. Mekokishvili;D. Mestel;Volker Meyer;A. Neuberger;K. Nielsen;M. Oliviero;R. Pampena;J. Paoli;Erika Pawlik;B. Rao;Adriana Rendon;T. Russo;Ahmed Sadek;K. Samhaber;R. Schneiderbauer;A. Schweizer;F. Toberer;L. Trennheuser;Lyobomira Vlahova;Alexander Wald;J. Winkler;Priscila Wölbing;I. Zalaudek|Computer Science;Medicine|dd9b99fac67c18be82d7763a8fbf231fc3512423;f94455176857303605ad423599385a2341c568eb;b5887d18420e8ac4f4fa4c83c4952138fd956702;9d46dc975aeed3f96bddb144079b50238f746ecd;94549a171a61039ed1f9b5954ce42181c574ccc3;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;d05d86db86a4ac0d95e6dcd951b42a9651939793;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;0b544dfe355a5070b60986319a3f51fb45d1348e;44c7d9fe583e3d317a619297e7e949070710799f;38f23fe236b152cd4983c8f30d305a568afd0d3e;8c8215b7f8111839f0066010a530a3a9f57ba15e;9670485f526f2254c0f34e64d9ca06f665a0bd17;dd971c07879e1ce12b06991319528c06280eeb9b;693914b7f38c19585e35668fd626aecf62d4c5e7;d133cb102ad0f81e3fd17a7db090b28afc124c4a;395de0bd3837fdf4b4b5e5f04835bcc69c279481|3194485;120704132;1734053|True;True;True|desc;desc;desc
8a0f17e0ee66ad5f50cd35932747e6a806ef03cf|10.1177/117693510600200030||||59 - 77|Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.|AAMAS|AAMAS_Berlin_2006|Berlin|Applications of Machine Learning in Cancer Prediction and Prognosis|2006|34468738;2066145|Joseph A. Cruz;D. Wishart|Computer Science;Medicine|24e6c5bfe9bb0751e5708b501d04e860011b2953;94549a171a61039ed1f9b5954ce42181c574ccc3;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1|8735139;2320509;1413140200|True;True;True|desc;desc;desc
a7a407968c13ced804a063259d72315a43b84f29|10.1109/ACCESS.2020.2988510|Cell|84.0|5678-9012_84|75264-75278|The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.||||Artificial Intelligence in Education: A Review|2020|1669767608;47978704;144264986|Lijia Chen;Pingping Chen;Zhijian Lin|Computer Science;Education|395de0bd3837fdf4b4b5e5f04835bcc69c279481|2048712;2474250;2056204271|True;False;True|desc;desc;desc
61e27dbae190b82639c57f180ecf97e4c46fcad9|10.1109/ACCESS.2020.2990567||||89497-89509|Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Lisbon_2020|Lisbon|Pymoo: Multi-Objective Optimization in Python|2020|31519649;145080287|Julian Blank;K. Deb|Computer Science||2594256;2258061;145624000|True;True;True|desc;desc;desc
b5887d18420e8ac4f4fa4c83c4952138fd956702|10.1109/ACCESS.2020.2988796||||80716-80727|The k-means algorithm is generally the most known and used clustering method. There are various extensions of k-means to be proposed in the literature. Although it is an unsupervised learning to clustering in pattern recognition and machine learning, the k-means algorithm and its extensions are always influenced by initializations with a necessary number of clusters a priori. That is, the k-means algorithm is not exactly an unsupervised clustering method. In this paper, we construct an unsupervised learning schema for the k-means algorithm so that it is free of initializations without parameter selection and can also simultaneously find an optimal number of clusters. That is, we propose a novel unsupervised k-means (U-k-means) clustering algorithm with automatically finding an optimal number of clusters without giving any initialization and parameter selection. The computational complexity of the proposed U-k-means clustering algorithm is also analyzed. Comparisons between the proposed U-k-means and other existing methods are made. Experimental results and comparisons actually demonstrate these good aspects of the proposed U-k-means clustering algorithm.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Beijing_2020|Beijing|Unsupervised K-Means Clustering Algorithm|2020|2300368826;1741064|Kristina P. Sinaga;Miin-Shen Yang|Computer Science|7ea35b35392c6ef5738635cec7d17b24fe3e4f04;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;2bc3644ce4de7fce5812c1455e056649a47c1bbf;9f970c93549a1e69cdb9ba0f28b8ad1b25ba2f3a|1974599;15186612;1393662331|False;True;True|desc;desc;desc
24e6c5bfe9bb0751e5708b501d04e860011b2953|10.21873/CGP.20063||||"
          41-51
        "|Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.|AAAI|AAAI_Rome_2018|Rome|Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.|2018|47156522;27122362;47479574;32286482;50232365|Shujun Huang;Nianguang Cai;Pedro Penzuti Pacheco;Shavira Narrandes;Yang Wang;Wayne W. Xu|Computer Science;Medicine;Biology|3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;2369db9921078c4bb76072ef7d6426e9f1dbfdb5;5a4631d5d75e3610037f87839628a4d166581e01;d0ab11de3077490c80a08abd0fb8827bac84c454;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;42ed4a9994e6121a9f325f5b901c5b3d7ce104f5;f94455176857303605ad423599385a2341c568eb;611544418ca53cdad254df444addc7814abcfddc;2bc3644ce4de7fce5812c1455e056649a47c1bbf;12d1d070a53d4084d88a77b8b143bad51c40c38f|144869300;153787929;153939986|False;True;False|desc;desc;desc
0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa|10.18653/V1/E17-1042|BMJ|3.0|8901-2345_3|438-449|Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.||||A Network-based End-to-End Trainable Task-oriented Dialogue System|2016|1388702112;51175233;3334541;2131709;2295429;144256365;145259603;92480907|L. Rojas-Barahona;M. Gašić;N. Mrksic;Pei-hao Su;Stefan Ultes;Tsung-Hsien Wen;S. Young;David Vandyke|Computer Science;Mathematics|9b539d413393047b28bb7be9b195f142aaf7a80e;265644f1b6740ca34bfbe9762b90b33021adde62;546785490ac417be1f83ced6a8272e934934f411;a27089efabc5f4abd5ddf2be2a409bff41f31199;44c7d9fe583e3d317a619297e7e949070710799f|121557847;1764952;1399133087|True;True;True|desc;desc;desc
d517b13f2b152c913b81ce534a149493517dbdad|10.1109/ACCESS.2014.2325029|PNAS|85.0|4567-8901_85|514-525|Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.||||Big Data Deep Learning: Challenges and Perspectives|2014|2145447101;39376164|Xue-wen Chen;Xiaotong Lin|Computer Science|cd49acefc8d51e324aa562e5337e1c2aff067053;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;9d46dc975aeed3f96bddb144079b50238f746ecd;3fea7ba9490c306484a8fdfe94323ff4e009e1c7;a486e2839291111bb44fa1f07731ada123539f75;dd971c07879e1ce12b06991319528c06280eeb9b;4b61c25a86083c20730c9b12737ac6ac4178c364;dd9b99fac67c18be82d7763a8fbf231fc3512423;339a8e4cb0eba77675711ac255ac2a5d7ede1d53|5875612;1873696;1974599|False;True;True|desc;desc;desc
d05d86db86a4ac0d95e6dcd951b42a9651939793|10.1109/ACCESS.2019.2895334||||41525-41550|Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01–0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.|Workshop on AI for Social Impact|Workshop_on_AI_for_Social_Impact_Paris_2019|Paris|Deep Learning Approach for Intelligent Intrusion Detection System|2019|51162051;2474250;2285465435;2916235;1399133087;145710905|R. Vinayakumar;M. Alazab;I. K. P. S. Senior Member;P. Poornachandran;Ameer Al-Nemrat;S. Venkatraman|Computer Science;Engineering|66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;a486e2839291111bb44fa1f07731ada123539f75;94549a171a61039ed1f9b5954ce42181c574ccc3;8c8215b7f8111839f0066010a530a3a9f57ba15e;f94455176857303605ad423599385a2341c568eb;546785490ac417be1f83ced6a8272e934934f411|2446311;33340370;146218865|False;True;True|desc;desc;desc
cbac8b0d82ea8e9251d5530695841d816cb196b9|10.21105/JOSS.01026||||1026|Python is currently the fastest growing programming language in the world, thanks to its ease-of-use, fast learning curve and its numerous high quality packages for data science and machine-learning. Surprisingly however, Python is far behind the R programming language when it comes to general statistics and for this reason many scientists still rely heavily on R to perform their statistical analyses.|ICCV|ICCV_Cairo_2018|Cairo|Pingouin: statistics in Python|2018|2095214091|Raphael Vallat|Computer Science;Mathematics|339a8e4cb0eba77675711ac255ac2a5d7ede1d53|2152662291;2066769956;1792298|True;True;True|desc;desc;desc
66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593|10.1093/nar/gkab447|BMJ|52.0|8901-2345_52|W317 - W325|Abstract Gene set enrichment (GSE) analysis plays an essential role in extracting biological insight from genome-scale experiments. ORA (overrepresentation analysis), FCS (functional class scoring), and PT (pathway topology) approaches are three generations of GSE methods along the timeline of development. Previous versions of KOBAS provided services based on just the ORA method. Here we presented version 3.0 of KOBAS, which is named KOBAS-i (short for KOBAS intelligent version). It introduced a novel machine learning-based method we published earlier, CGPS, which incorporates seven FCS tools and two PT tools into a single ensemble score and intelligently prioritizes the relevant biological pathways. In addition, KOBAS has expanded the downstream exploratory visualization for selecting and understanding the enriched results. The tool constructs a novel view of cirFunMap, which presents different enriched terms and their correlations in a landscape. Finally, based on the previous version's framework, KOBAS increased the number of supported species from 1327 to 5944. For an easier local run, it also provides a prebuilt Docker image that requires no installation, as a supplementary to the source code version. KOBAS can be freely accessed at http://kobas.cbi.pku.edu.cn, and a mirror site is available at http://bioinfo.org/kobas.||||KOBAS-i: intelligent prioritization and exploratory visualization of biological functions for gene enrichment analysis|2021|1934073;47030051;2031130914;2108348149;2116575879;2040814523;2149122113;34990576;2108434617;2157957189;134712615;2031130601;2107009096;2109916949;145336368|Dechao Bu;Haitao Luo;Peipei Huo;Zhihao Wang;Shan Zhang;Zihao He;Yang Wu;Lianhe Zhao;Jingjia Liu;Jincheng Guo;Shuangsang Fang;Wanchen Cao;Lan Yi;Yi Zhao;Lei Kong|Computer Science;Medicine;Biology|d133cb102ad0f81e3fd17a7db090b28afc124c4a;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;64be9999b68e12d260ba7423f6b55ffd41552ad3;546785490ac417be1f83ced6a8272e934934f411;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa|1765169;2656573;1485389319|False;False;True|desc;desc;desc
2bc3644ce4de7fce5812c1455e056649a47c1bbf|10.1109/ACCESS.2019.2923707||||81542-81554|Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).|ICAPS|ICAPS_Madrid_2019|Madrid|Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques|2019|150302778;9727014;144369609|Senthilkumar Mohan;Chandrasegar Thirumalai;Gautam Srivastava|Computer Science;Medicine|7ea35b35392c6ef5738635cec7d17b24fe3e4f04;d517b13f2b152c913b81ce534a149493517dbdad;b3de1062d8a462dfdc2938558258f8884abe9f4e;12d1d070a53d4084d88a77b8b143bad51c40c38f;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;a27089efabc5f4abd5ddf2be2a409bff41f31199;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;61e27dbae190b82639c57f180ecf97e4c46fcad9;dd971c07879e1ce12b06991319528c06280eeb9b;9d3e0fce253a4ae4a4456b2f24c03329a2b74621;24e6c5bfe9bb0751e5708b501d04e860011b2953;5a4631d5d75e3610037f87839628a4d166581e01|46829048;46560749;31292557|True;True;True|desc;desc;desc
64be9999b68e12d260ba7423f6b55ffd41552ad3|10.1109/ACCESS.2017.2788044||||9375-9389|The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.|NIPS|NIPS_Auckland_2018|Auckland|Deep Learning Applications in Medical Image Analysis|2018|34812292;46659335;39917910;48508646|Justin Ker;Lipo Wang;J. Rao;Tchoyoson C. C. Lim|Computer Science;Medicine|5a4631d5d75e3610037f87839628a4d166581e01;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;9670485f526f2254c0f34e64d9ca06f665a0bd17;12d1d070a53d4084d88a77b8b143bad51c40c38f;3cee40494377c0e7d9c7c23a3811b481e55bce39;395de0bd3837fdf4b4b5e5f04835bcc69c279481;9f387ce140c59a44eaeeea590087351461345164;94549a171a61039ed1f9b5954ce42181c574ccc3|1751586;2295429;50693907|True;False;True|desc;desc;desc
ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2|10.1609/AAAI.V34I05.6311|NEJM|93.0|9012-3456_93|8018-8025|Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1||||Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment|2019|2068347799;8752221;10638646;1679873|Di Jin;Zhijing Jin;Joey Tianyi Zhou;Peter Szolovits|Computer Science|dd971c07879e1ce12b06991319528c06280eeb9b;ade03d0c772c35dc8e865bdb41d7bc54d5b782d1;38f23fe236b152cd4983c8f30d305a568afd0d3e;395de0bd3837fdf4b4b5e5f04835bcc69c279481;3cee40494377c0e7d9c7c23a3811b481e55bce39;546785490ac417be1f83ced6a8272e934934f411;5ded2b8c64491b4a67f6d39ce473d4b9347a672e;8c8215b7f8111839f0066010a530a3a9f57ba15e;cbac8b0d82ea8e9251d5530695841d816cb196b9;cd49acefc8d51e324aa562e5337e1c2aff067053;12d1d070a53d4084d88a77b8b143bad51c40c38f;2bc3644ce4de7fce5812c1455e056649a47c1bbf;94549a171a61039ed1f9b5954ce42181c574ccc3;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;611544418ca53cdad254df444addc7814abcfddc;10f919b1a5161b560504c225cfb2d1b3a4768f80;d133cb102ad0f81e3fd17a7db090b28afc124c4a;9b539d413393047b28bb7be9b195f142aaf7a80e;7b2dd79083a74699e4e0509ac3f0a8a302b4eabe;98c25683fc8d6446448b734b1bcf08e1457f8d85|2087279;11852405;145836900|True;True;True|desc;desc;desc
265644f1b6740ca34bfbe9762b90b33021adde62|10.3348/kjr.2017.18.4.570|Nature|99.0|1234-5678_99|570 - 584|The artificial neural network (ANN)–a machine learning technique inspired by the human neuronal synapse system–was introduced in the 1950s. However, the ANN was previously limited in its ability to solve actual problems, due to the vanishing gradient and overfitting problems with training of deep architecture, lack of computing power, and primarily the absence of sufficient data to train the computer system. Interest in this concept has lately resurfaced, due to the availability of big data, enhanced computing power with the current graphics processing units, and novel algorithms to train the deep neural network. Recent studies on this technology suggest its potentially to perform better than humans in some visual and auditory recognition tasks, which may portend its applications in medicine and healthcare, especially in medical imaging, in the foreseeable future. This review article offers perspectives on the history, development, and applications of deep learning technology, particularly regarding its applications in medical imaging.||||Deep Learning in Medical Imaging: General Overview|2017|120704132;2052576935;2116632761;46901084;38628528;46844846;145979410|June-Goo Lee;Sanghoon Jun;Younghoon Cho;Hyunna Lee;G. Kim;J. Seo;Namkug Kim|Computer Science;Medicine|d05d86db86a4ac0d95e6dcd951b42a9651939793;61e27dbae190b82639c57f180ecf97e4c46fcad9;d517b13f2b152c913b81ce534a149493517dbdad;dd971c07879e1ce12b06991319528c06280eeb9b;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;9f387ce140c59a44eaeeea590087351461345164;3d13d6f847e8f4c76c5a8d6f99fa49dbabe473c0;98c25683fc8d6446448b734b1bcf08e1457f8d85|2595483;1683459;3177854|True;True;True|desc;desc;desc
9d46dc975aeed3f96bddb144079b50238f746ecd|10.1080/21693277.2016.1192517|BMJ|19.0|8901-2345_19|23 - 45|The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.||||Machine learning in manufacturing: advantages, challenges, and applications|2016|2511781;2051446256;1852549;144199043|Thorsten Wuest;Daniel Weimer;C. Irgens;K. Thoben|Engineering;Computer Science|2bc3644ce4de7fce5812c1455e056649a47c1bbf;ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2;0b544dfe355a5070b60986319a3f51fb45d1348e;dd971c07879e1ce12b06991319528c06280eeb9b;0ef9ae1ce8c91ce671a211bdda792bf3752d1522;8a0f17e0ee66ad5f50cd35932747e6a806ef03cf;a27089efabc5f4abd5ddf2be2a409bff41f31199;96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;b3de1062d8a462dfdc2938558258f8884abe9f4e;3fea7ba9490c306484a8fdfe94323ff4e009e1c7;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;395de0bd3837fdf4b4b5e5f04835bcc69c279481;f8b7a3434f887ce4570b7e98c7f1b91c008042d4;10f919b1a5161b560504c225cfb2d1b3a4768f80;5a4631d5d75e3610037f87839628a4d166581e01;b5887d18420e8ac4f4fa4c83c4952138fd956702;d133cb102ad0f81e3fd17a7db090b28afc124c4a;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593;693914b7f38c19585e35668fd626aecf62d4c5e7|151487590;3450866;2107009096|True;True;True|desc;desc;desc
7ea35b35392c6ef5738635cec7d17b24fe3e4f04|10.1093/nsr/nwy108||||74 - 86|Abstract Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation.|Workshop on AI for Humanitarian Action|Workshop_on_AI_for_Humanitarian_Action_Los_Angeles_2017|Los Angeles|Deep forest|2017|145624000;2108993600|Zhi-Hua Zhou;Ji Feng|Computer Science;Medicine;Mathematics|96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7;9f387ce140c59a44eaeeea590087351461345164;693914b7f38c19585e35668fd626aecf62d4c5e7;a486e2839291111bb44fa1f07731ada123539f75;98c25683fc8d6446448b734b1bcf08e1457f8d85;d05d86db86a4ac0d95e6dcd951b42a9651939793|51162051;2055574299;46844846|True;True;True|desc;desc;desc
9b539d413393047b28bb7be9b195f142aaf7a80e|10.18653/v1/2021.eacl-main.24|PNAS|26.0|4567-8901_26|300-325|Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.||||Recipes for Building an Open-Domain Chatbot|2020|144745718;31461304;39589154;3092435;2066769956;11323179;2155954521;40511414;35752280;51324296;2656573;145183709|Stephen Roller;Emily Dinan;Naman Goyal;Da Ju;Mary Williamson;Yinhan Liu;Jing Xu;Myle Ott;Kurt Shuster;Eric Michael Smith;Y-Lan Boureau;J. Weston|Computer Science|dd9b99fac67c18be82d7763a8fbf231fc3512423;3cee40494377c0e7d9c7c23a3811b481e55bce39;61e27dbae190b82639c57f180ecf97e4c46fcad9;9f387ce140c59a44eaeeea590087351461345164;3fea7ba9490c306484a8fdfe94323ff4e009e1c7;a27089efabc5f4abd5ddf2be2a409bff41f31199;9670485f526f2254c0f34e64d9ca06f665a0bd17;12d1d070a53d4084d88a77b8b143bad51c40c38f;24e6c5bfe9bb0751e5708b501d04e860011b2953;7ea35b35392c6ef5738635cec7d17b24fe3e4f04;546785490ac417be1f83ced6a8272e934934f411;dd971c07879e1ce12b06991319528c06280eeb9b;10f919b1a5161b560504c225cfb2d1b3a4768f80;66183f1bef6e1a77b1f35e3eb7bd9d02fd8c9593|5580228;145624000;1979489|False;True;True|desc;desc;desc
96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7|10.21248/jlcl.20.2005.68||||19-62|The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.|Workshop on AI for Social Good|Workshop_on_AI_for_Social_Good_Cape_Town_2005|Cape Town|A Brief Survey of Text Mining|2005|1792623;1759689;1683459|A. Hotho;A. Nürnberger;G. Paass|Computer Science;Linguistics|265644f1b6740ca34bfbe9762b90b33021adde62;8c8215b7f8111839f0066010a530a3a9f57ba15e;9670485f526f2254c0f34e64d9ca06f665a0bd17;f8b7a3434f887ce4570b7e98c7f1b91c008042d4|2038173616;1751762;5875612|False;True;False|desc;desc;desc
