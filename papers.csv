paperID|doi|journal_name|pages|abstract|publicationVenue_id|publicationVenue_name|title|year|authorIDs|authorNames|fields
34f25a8704614163c4095b3ee2fc969b60de4698|10.5555/2627435.2670313|J. Mach. Learn. Res.|1929-1958|"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Dropout: a simple way to prevent neural networks from overfitting|2014|2897313;1695689;2064160;1701686;145124475|Nitish Srivastava;Geoffrey E. Hinton;A. Krizhevsky;I. Sutskever;R. Salakhutdinov|Computer Science
4c75b748911ddcd888c5122f7672f69caa5d661f|10.1080/00401706.1999.10485951|Technometrics|377-378|A machine learning system, in general, learns from the environment, but statistical machine learning programs (systems) learn from the data. This chapter presents techniques for statistical machine learning using Support Vector Machines (SVM) to recognize the patterns and classify them, predicting structured objects using SVM, k-nearest neighbor method for classification, and Naive Bayes classifiers. The artificial neural networks are presented with brief introduction to error-correction rules, Boltzmann learning, Hebbian rule, competitive learning rule, and deep learning. The instance-based learning is treated in details with its algorithm and learning task. The chapter concludes with a summary, and a set of practice exercises.|ca4b3aef-bda7-47fe-b7e5-4f9cd6cb1947|Technometrics|Statistical Learning Theory|2021|2348306915|Yuhai Wu|Mathematics;Computer Science
a25fbcbbae1e8f79c4360d26aa11a3abf1a11972|10.1109/TKDE.2009.191|IEEE Transactions on Knowledge and Data Engineering|1345-1359|A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|A Survey on Transfer Learning|2010|1746914;152290618|Sinno Jialin Pan;Qiang Yang|Computer Science
184ac0766262312ba76bbdece4e7ffad0aa8180b|10.1109/TPAMI.2013.50|IEEE Transactions on Pattern Analysis and Machine Intelligence|1798-1828|The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Representation Learning: A Review and New Perspectives|2012|1751762;1760871;145467703|Yoshua Bengio;Aaron C. Courville;Pascal Vincent|Medicine;Mathematics;Computer Science
339a8e4cb0eba77675711ac255ac2a5d7ede1d53|10.1093/molbev/msab120|Molecular Biology and Evolution|3022 - 3027|Abstract The Molecular Evolutionary Genetics Analysis (MEGA) software has matured to contain a large collection of methods and tools of computational molecular evolution. Here, we describe new additions that make MEGA a more comprehensive tool for building timetrees of species, pathogens, and gene families using rapid relaxed-clock methods. Methods for estimating divergence times and confidence intervals are implemented to use probability densities for calibration constraints for node-dating and sequence sampling dates for tip-dating analyses. They are supported by new options for tagging sequences with spatiotemporal sampling information, an expanded interactive Node Calibrations Editor, and an extended Tree Explorer to display timetrees. Also added is a Bayesian method for estimating neutral evolutionary probabilities of alleles in a species using multispecies sequence alignments and a machine learning method to test for the autocorrelation of evolutionary rates in phylogenies. The computer memory requirements for the maximum likelihood analysis are reduced significantly through reprogramming, and the graphical user interface has been made more responsive and interactive for very big data sets. These enhancements will improve the user experience, quality of results, and the pace of biological discovery. Natively compiled graphical user interface and command-line versions of MEGA11 are available for Microsoft Windows, Linux, and macOS from www.megasoftware.net.|52bd1f2f-5bb2-4bae-8674-c503b506c30e|Molecular biology and evolution|MEGA11: Molecular Evolutionary Genetics Analysis Version 11|2021|3066242;48226007;2110208881|K. Tamura;G. Stecher;Sudhir Kumar|Medicine;Biology;Computer Science
7bb6bdf4ed609e5e72d4206d1b308323e73dceec|10.1162/artl.1997.3.1.63|Artificial Life|63-65|"An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a ""Brief History"" and ""Future Directions."" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering."|5ed9f470-65a3-4077-8dab-163b0c5cb9e6|Artificial Life|An Introduction to Genetic Algorithms.|1997|1403108392|D. Heiss-Czedik|Mathematics;Computer Science
fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4|10.1198/jasa.2003.s270|IEEE Transactions on Neural Networks|781-781|Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.|2ac50919-507e-41c7-93a8-721c4b804757|IEEE Transactions on Neural Networks|Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond|2003|2285968829|Christopher K. I Williams|Computer Science;Mathematics
12d1d070a53d4084d88a77b8b143bad51c40c38f|10.1613/jair.301|J. Artif. Intell. Res.|237-285|"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ""reinforcement."" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."|aef12dca-60a0-4ca3-819b-cad26d309d4e|Journal of Artificial Intelligence Research|Reinforcement Learning: A Survey|1996|1709512;144885169;1760402|L. Kaelbling;M. Littman;A. Moore|Computer Science
a675fe5a7d99ac6f7ff91fa084462faefe616148|10.1145/950566.950595|Comput. Entertain.|20|Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.|ce5f5d49-2807-4aa0-9db3-64a03084444d|Conference on Computability in Europe|What video games have to teach us about learning and literacy|2007|34622402|J. Gee|Education;Sociology;Psychology;Computer Science
0e90a73f03902cbe915af1aff54ea7f0b3373680|10.1609/aimag.v22i2.1566|AI Mag.|103-104|This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.|6fedff74-7525-4b7f-bbb4-4df4e23948e4|The AI Magazine|An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods|2001|2146325920|Tong Zhang|Mathematics;Computer Science
df40ce107a71b770c9d0354b78fdd8989da80d2f|10.1109/SP.2017.49|2017 IEEE Symposium on Security and Privacy (SP)|39-57|Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|Towards Evaluating the Robustness of Neural Networks|2016|2483738;145394689|Nicholas Carlini;D. Wagner|Computer Science
88816ae492956f3004daa41357166f1181c0c1bf|10.1162/089976603321780317|Neural Computation|1373-1396|One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|Laplacian Eigenmaps for Dimensionality Reduction and Data Representation|2003|145520115;1770745|M. Belkin;P. Niyogi|Mathematics;Computer Science
81a4fd3004df0eb05d6c1cef96ad33d5407820df|10.1109/TNNLS.2020.2978386|IEEE Transactions on Neural Networks and Learning Systems|4-24|Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.|79c5a18d-0295-432c-aaa5-961d73de6d88|IEEE Transactions on Neural Networks and Learning Systems|A Comprehensive Survey on Graph Neural Networks|2019|2109557884;2585415;31370754;2062835;48934799;144019071|Zonghan Wu;Shirui Pan;Fengwen Chen;Guodong Long;Chengqi Zhang;Philip S. Yu|Mathematics;Medicine;Computer Science
0023582fde36430c7e3ae81611a14e558c8f4bae|10.1561/0400000042|Found. Trends Theor. Comput. Sci.|211-407|The problem of privacy-preserving data analysis has a long history spanning multiple disciplines. As electronic data about individuals becomes increasingly detailed, and as technology enables ever more powerful collection and curation of these data, the need increases for a robust, meaningful, and mathematically rigorous definition of privacy, together with a computationally rich class of algorithms that satisfy this definition. Differential Privacy is such a definition.After motivating and discussing the meaning of differential privacy, the preponderance of this monograph is devoted to fundamental techniques for achieving differential privacy, and application of these techniques in creative combinations, using the query-release problem as an ongoing example. A key point is that, by rethinking the computational goal, one can often obtain far better results than would be achieved by methodically replacing each step of a non-private computation with a differentially private implementation. Despite some astonishingly powerful computational results, there are still fundamental limitations — not just on what can be achieved with differential privacy but on what can be achieved with any method that protects against a complete breakdown in privacy. Virtually all the algorithms discussed herein maintain differential privacy against adversaries of arbitrary computational power. Certain algorithms are computationally intensive, others are efficient. Computational complexity for the adversary and the algorithm are both discussed.We then turn from fundamentals to applications other than queryrelease, discussing differentially private methods for mechanism design and machine learning. The vast majority of the literature on differentially private algorithms considers a single, static, database that is subject to many analyses. Differential privacy in other models, including distributed databases and computations on data streams is discussed.Finally, we note that this work is meant as a thorough introduction to the problems and techniques of differential privacy, but is not intended to be an exhaustive survey — there is by now a vast amount of work in differential privacy, and we can cover only a small portion of it.|9cd88f1a-8917-4156-b5d6-fb0cef6af4f5|Foundations and Trends® in Theoretical Computer Science|The Algorithmic Foundations of Differential Privacy|2014|1781565;1682008|C. Dwork;Aaron Roth|Mathematics;Computer Science
2e55ba6c97ce5eb55abd959909403fe8da7e9fe9|10.1073/pnas.1611835114|Proceedings of the National Academy of Sciences|3521 - 3526|Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Overcoming catastrophic forgetting in neural networks|2016|2066516991;1996134;3422052;144056327;2755582;2228824;8181864;34660073;34505275;1398898827;48987704;2388737;2106164;2315504|J. Kirkpatrick;Razvan Pascanu;Neil C. Rabinowitz;J. Veness;Guillaume Desjardins;Andrei A. Rusu;Kieran Milan;John Quan;Tiago Ramalho;A. Grabska-Barwinska;D. Hassabis;C. Clopath;D. Kumaran;R. Hadsell|Medicine;Mathematics;Computer Science
d422df8bff4e677a3077635db116679d25142bfc|10.1126/science.aaa8415|Science|255 - 260|Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Machine learning: Trends, perspectives, and prospects|2015|1694621;2066277988|Michael I. Jordan;T. Mitchell|Medicine;Computer Science
08b43d84e6747e370ef307e2ada50675b414514a|10.1109/TNN.2005.845141|IEEE Transactions on Neural Networks|645-678|Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.|2ac50919-507e-41c7-93a8-721c4b804757|IEEE Transactions on Neural Networks|Survey of clustering algorithms|2005|144996246;145033828|Rui Xu;D. Wunsch|Medicine;Mathematics;Computer Science
a1874aafa8730bdd4b28f29d025141c13ee28b58|10.1609/aimag.v17i3.1230|AI Mag.|37-54|■ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.|6fedff74-7525-4b7f-bbb4-4df4e23948e4|The AI Magazine|From Data Mining to Knowledge Discovery in Databases|1996|1695784;1398381803;50860274|U. Fayyad;G. Piatetsky-Shapiro;Padhraic Smyth|Mathematics;Computer Science
9e475a514f54665478aac6038c262e5a6bac5e64|10.1109/cvpr42600.2020.01164|2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|11618-11628|Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|nuScenes: A Multimodal Dataset for Autonomous Driving|2019|3078154;88740363;33242383;22254044;1754854;2149106173;2064366548;1959810281;46718993;3258919|Holger Caesar;Varun Bankiti;Alex H. Lang;Sourabh Vora;Venice Erin Liong;Qiang Xu;Anush Krishnan;Yuxin Pan;G. Baldan;Oscar Beijbom|Mathematics;Engineering;Computer Science;Environmental Science
a7976c2bacfbb194ddbe7fd10c2e50a545cf4081|10.1109/tnnls.2016.2582924|IEEE Transactions on Neural Networks and Learning Systems|2222-2232|Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.|79c5a18d-0295-432c-aaa5-961d73de6d88|IEEE Transactions on Neural Networks and Learning Systems|LSTM: A Search Space Odyssey|2015|3035541;2100612;2865775;1714059;145341374|Klaus Greff;R. Srivastava;Jan Koutník;Bas R. Steunebrink;J. Schmidhuber|Medicine;Computer Science
a3461eaf51016f9d6e85ea47173b27e019e801c4|10.1203/00006450-199704001-00009|Pediatric Research|17-19|We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …|ad2ae6f7-53e8-442e-900a-bd9ab7ff3865|Pediatric Research|State of the Art|1997|2262448783|Markus Voelter|Philosophy;Computer Science
e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772|10.1147/rd.33.0210|IBM J. Res. Dev.|206-227|Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the|555db9fd-8025-4984-8082-971e1e6bdb24|IBM Journal of Research and Development|Some Studies in Machine Learning Using the Game of Checkers|1967|7991309|A. Samuel|Computer Science
6f24d7a6e1c88828e18d16c6db20f5329f6a6827|10.1080/01621459.2017.1285773|Journal of the American Statistical Association|859 - 877|ABSTRACT One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.|1901e811-ee72-4b20-8f7e-de08cd395a10|arXiv.org|Variational Inference: A Review for Statisticians|2016|1796335;3081817;40411909|D. Blei;A. Kucukelbir;Jon D. McAuliffe|Computer Science;Mathematics
46f74231b9afeb0c290d6d550043c55045284e5f|10.1109/MSP.2012.2211477|IEEE Signal Processing Magazine|141-142|In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.|f62e5eab-173a-4e0a-a963-ed8de9835d22|IEEE Signal Processing Magazine|The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]|2012|144718788|L. Deng|Engineering;Computer Science
49bdeb07b045dd77f0bfe2b44436608770235a23|10.1109/MSP.2020.2975749|IEEE Signal Processing Magazine|50-60|Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.|f62e5eab-173a-4e0a-a963-ed8de9835d22|IEEE Signal Processing Magazine|Federated Learning: Challenges, Methods, and Future Directions|2019|145530218;2894821;145532827;145260024|Tian Li;Anit Kumar Sahu;Ameet Talwalkar;Virginia Smith|Mathematics;Computer Science
18d026ec5d0eebd17ee2c762da89540c0b3d7bde|10.1109/JPROC.2020.3004555|Proceedings of the IEEE|43-76|Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.|6faaccca-1cc4-45a9-aeb6-96a4901d2606|Proceedings of the IEEE|A Comprehensive Survey on Transfer Learning|2019|1799525;51052604;108237152;120791559;1864765978;1968806;144467554;144131273|Fuzhen Zhuang;Zhiyuan Qi;Keyu Duan;Dongbo Xi;Yongchun Zhu;Hengshu Zhu;Hui Xiong;Qing He|Mathematics;Computer Science
1dae4d61cd74cc919ecc638bde6b7125728ea97b|10.1109/TNN.2010.2091281|IEEE Transactions on Neural Networks|199-210|Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.|2ac50919-507e-41c7-93a8-721c4b804757|IEEE Transactions on Neural Networks|Domain Adaptation via Transfer Component Analysis|2009|1746914;1807998;145193332;152290618|Sinno Jialin Pan;I. Tsang;J. Kwok;Qiang Yang|Medicine;Mathematics;Computer Science
f986968735459e789890f24b6b277b0920a9725d|10.1109/TPAMI.2017.2723009|IEEE Transactions on Pattern Analysis and Machine Intelligence|1452-1464|The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Places: A 10 Million Image Database for Scene Recognition|2018|145291669;2677488;2556428;143868587;143805211|Bolei Zhou;Àgata Lapedriza;A. Khosla;A. Oliva;A. Torralba|Medicine;Engineering;Computer Science;Environmental Science
0090023afc66cd2741568599057f4e82b566137c|10.1145/3457607|ACM Computing Surveys (CSUR)|1 - 35|With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.|7b2adce0-d53f-49d6-8784-b0645604fe62|ACM Computing Surveys|A Survey on Bias and Fairness in Machine Learning|2019|51997673;2775559;51884035;1782658;143728483|Ninareh Mehrabi;Fred Morstatter;N. Saxena;Kristina Lerman;A. Galstyan|Mathematics;Computer Science
a34e35dbbc6911fa7b94894dffdc0076a261b6f0|10.1162/neco.1992.4.1.1|Neural Computation|1-58|Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|Neural Networks and the Bias/Variance Dilemma|1992|3194361;2246319;2330895|S. Geman;E. Bienenstock;R. Doursat|Mathematics;Computer Science
f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d|10.1109/SP.2017.41|2017 IEEE Symposium on Security and Privacy (SP)|3-18|"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|Membership Inference Attacks Against Machine Learning Models|2016|2520493;34828439;3469125;1723945|R. Shokri;M. Stronati;Congzheng Song;Vitaly Shmatikov|Mathematics;Computer Science
819167ace2f0caae7745d2f25a803979be5fbfae|10.1109/EUROSP.2016.36|2016 IEEE European Symposium on Security and Privacy (EuroS&P)|372-387|Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.|4c2b8cb8-e51c-4ece-9122-89595989b56f|European Symposium on Security and Privacy|The Limitations of Deep Learning in Adversarial Settings|2015|1967156;144061974;1680133;2623167;144643812;144231976|Nicolas Papernot;P. Mcdaniel;S. Jha;Matt Fredrikson;Z. B. Celik;A. Swami|Mathematics;Computer Science
86cff4d050beb90fed2e1ceac8940c8221b120aa|10.1109/mex.1987.4307079|IEEE Expert|110-111|Computers are being used more and more in all aspects of our lives and, programmed correctly, they are more accurate and precise than humans can ever be. Here, however, the myth of the superiority of artificial intelligence is examined and dispelled. The authors, one a philosopher and the other a computer scientist, argue that even highly advanced systems only correspond to the very early stages of human learning and that there are many human skills that computers will never be able to emulate. The mind will always be superior to the machine. To illustrate their point, they set forth a model documenting five distinct levels - novice, advanced beginner, competent, proficient and expert - through which human beings pass in acquiring and mastering a skill. The two final stages require a degree of intuitive intelligence far beyond the most ambitious projects being planned for the future. The authors acknowledge the huge progress made by computers and the massive advantages to be gained from using them, but they stress that their value can only lie in their use as aids, never as substitutes for the human mind.|f9679adc-2c92-4f80-b2f3-c4e395e79688|IEEE Expert|Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer|1987|3333658;2547681;2975527|H. Dreyfus;S. Dreyfus;L. Zadeh|Philosophy;Computer Science
48234756b7cf798bfeb47328f7c5d597fd4838c2|10.1109/IJCNN.2008.4633969|2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)|1322-1328|This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.|1786f28f-85e4-4441-bb5d-a7bbc1dd3f24|IEEE World Congress on Computational Intelligence|ADASYN: Adaptive synthetic sampling approach for imbalanced learning|2008|2198278;2115297828;2111388466;2116066317|Haibo He;Yang Bai;E. A. Garcia;Shutao Li|Computer Science
5d150cec2775f9bc863760448f14104cc8f42368|10.1073/pnas.1517384113|Proceedings of the National Academy of Sciences|3932 - 3937|Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Discovering governing equations from data by sparse identification of nonlinear dynamical systems|2015|3083169;2424683;144484982|S. Brunton;J. Proctor;J. Kutz|Medicine;Physics;Engineering;Computer Science;Mathematics;Environmental Science
e0535dedb8607d83cd2614317c99913378e89e26|10.1162/089976602760407955|Neural Computation|2531-2560|A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations|2002|145247053;1792142;1754307|W. Maass;T. Natschläger;H. Markram|Medicine;Computer Science
d516daff247f7157fccde6649ace91d969cd1973|10.1145/3233231|Communications of the ACM|36 - 43|In machine learning, the concept of interpretability is both important and slippery.|4bd27ad1-f6fa-4340-9d9d-c62ac5be6fc0|Queue|The mythos of model interpretability|2016|32219137|Zachary Chase Lipton|Mathematics;Philosophy;Computer Science
611544418ca53cdad254df444addc7814abcfddc|10.1080/24754269.2021.1980261|Statistical Theory and Related Fields|87 - 87|The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an|63d5660f-c56d-4a40-b401-f460e483e476|Statistical Theory and Related Fields|An introduction to statistical learning with applications in R|2021|151004165;2134162313;1840524|Fariha Sohil;Muhammad Umair Sohali;J. Shabbir|Mathematics;Computer Science
0e779fd59353a7f1f5b559b9d65fa4bfe367890c|10.1109/MSP.2017.2693418|IEEE Signal Process. Mag.|18-42|Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.|f62e5eab-173a-4e0a-a963-ed8de9835d22|IEEE Signal Processing Magazine|Geometric Deep Learning: Going beyond Euclidean data|2016|1732570;143627859;1688882;3149531;1697397|M. Bronstein;Joan Bruna;Yann LeCun;Arthur Szlam;P. Vandergheynst|Mathematics;Computer Science
2ea6a93199c9227fa0c1c7de13725f918c9be3a4|10.5555/1577069.1755843|J. Mach. Learn. Res.|1755-1758|There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Dlib-ml: A Machine Learning Toolkit|2009|2065224236|Davis E. King|Computer Science
9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746|10.3233/IDA-2002-6504|Intell. Data Anal.|429-449|In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.|5b12c785-cf12-40ca-b4d1-aefd13b26e39|Intelligent Data Analysis|The class imbalance problem: A systematic study|2002|1743642;144980086|N. Japkowicz;Shaju Stephen|Mathematics;Computer Science
6adf016e7531c91100d3cf4a74f5d4c87b26b528|10.1109/SP.2016.41|2016 IEEE Symposium on Security and Privacy (SP)|582-597|Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks|2015|1967156;144061974;37785191;1680133;144231976|Nicolas Papernot;P. Mcdaniel;Xi Wu;S. Jha;A. Swami|Mathematics;Computer Science
815c84ab906e43f3e6322f2ca3fd5e1360c64285|10.1126/science.aab3050|Science|1332 - 1338|Handwritten characters drawn by a model Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Human-level concept learning through probabilistic program induction|2015|2373318;145124475;1763295|B. Lake;R. Salakhutdinov;J. Tenenbaum|Medicine;Computer Science
2c47bd8bd699914e3535292b17ba46542800845c|10.5555/2627435.2697065|J. Mach. Learn. Res.|3133-3181|We evaluate 179 classiﬁers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classiﬁers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Matlab, including all the relevant classiﬁers available today. We use 121 data sets , which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve signiﬁcant conclusions about the classiﬁer behavior, not dependent on the data set collection. The classiﬁers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically signiﬁcant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classiﬁers (3 out of 5 bests classiﬁers are RF), followed by SVM (4 classiﬁers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Do we need hundreds of classifiers to solve real world classification problems?|2014|1942181;2746944;145676637;2061412126|M. Delgado;E. Cernadas;S. Barro;D. Amorim|Mathematics;Computer Science
d12864a8acbab1830be755bfb9cb177e31ca5e20|10.1109/34.824821|IEEE Trans. Pattern Anal. Mach. Intell.|63-84|Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction of new technologies. Given its ubiquity in human transactions, machine recognition of handwriting has practical significance, as in reading handwritten notes in a PDA, in postal addresses on envelopes, in amounts in bank checks, in handwritten fields in forms, etc. This overview describes the nature of handwritten language, how it is transduced into electronic data, and the basic concepts behind written language recognition algorithms. Both the online case (which pertains to the availability of trajectory data during writing) and the off-line case (which pertains to scanned images) are considered. Algorithms for preprocessing, character and word recognition, and performance with practical systems are indicated. Other fields of application, like signature verification, writer authentification, handwriting learning tools are also considered.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey|2000|144586498;1696384|R. Plamondon;S. Srihari|Computer Science
1696cbf7da0ee845c50591843993e6605adec177|10.1145/2347736.2347755|Communications of the ACM|78 - 87|"Tapping into the ""folk knowledge"" needed to advance machine learning applications."|4d9ce1c4-dc84-46b9-903e-e3751c00c7dd|Communications of the ACM|A few useful things to know about machine learning|2012|1740213|Pedro M. Domingos|Computer Science
3a84214cb69ea0b34352285029f368b75718c32b|10.1109/ICENGTECHNOL.2017.8308186|2017 International Conference on Engineering and Technology (ICET)|1-6|The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.|12b499eb-56c9-4d88-b792-6d90ceff3485|International Conference on Emerging Technologies|Understanding of a convolutional neural network|2017|2338621460;37517325;1410550919|Saad Albawi;T. Mohammed;Saad Al-Zawi|Computer Science
0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5|10.1109/TKDE.2013.39|IEEE Transactions on Knowledge and Data Engineering|1819-1837|Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|A Review on Multi-Label Learning Algorithms|2014|3039887;145624000|Min-Ling Zhang;Zhi-Hua Zhou|Computer Science
864e7db59f2ccfec1ee9f6eba79566ac7b0634df|10.1109/CVPR.2016.511|2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|4724-4732|Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Convolutional Pose Machines|2016|2797981;20569810;1733113;1774867|S. Wei;V. Ramakrishna;T. Kanade;Yaser Sheikh|Computer Science
6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91|10.1109/TPAMI.2018.2798607|IEEE Transactions on Pattern Analysis and Machine Intelligence|423-443|Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Multimodal Machine Learning: A Survey and Taxonomy|2017|1756344;118242121;49933077|T. Baltrušaitis;Chaitanya Ahuja;Louis-Philippe Morency|Medicine;Computer Science
ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e|10.5555/1756006.1756008|J. Mach. Learn. Res.|19-60|Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Online Learning for Matrix Factorization and Sparse Coding|2009|2599292;144570279;144189388;1699339|J. Mairal;F. Bach;J. Ponce;G. Sapiro|Mathematics;Computer Science
2de0a40e9a5d4f1feb07d61af5a5d87a069653f0|10.1109/69.553155|IEEE Trans. Knowl. Data Eng.|866-883|Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|Data Mining: An Overview from a Database Perspective|1996|153314745;145325584;144019071|Ming-Syan Chen;Jiawei Han;Philip S. Yu|Computer Science
10f919b1a5161b560504c225cfb2d1b3a4768f80|10.1136/svn-2017-000101|Stroke and Vascular Neurology|230 - 243|Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.|f46dbd50-8ba0-47aa-9649-1ba237101795|Stroke and vascular neurology|Artificial intelligence in healthcare: past, present and future|2017|67092021;2117937034;1976425918;1974599;36156845;119918227;47454309;46829048;2108094216|F. Jiang;Yong Jiang;Hui Zhi;Yi Dong;Hao Li;Sufeng Ma;Yilong Wang;Q. Dong;Haipeng Shen;Yongjun Wang|Medicine;Computer Science
fee4db01f6f981931dfd87376a8f861353d1e494|10.1037/bul0000084|Psychological Bulletin|187–232|Suicidal thoughts and behaviors (STBs) are major public health problems that have not declined appreciably in several decades. One of the first steps to improving the prevention and treatment of STBs is to establish risk factors (i.e., longitudinal predictors). To provide a summary of current knowledge about risk factors, we conducted a meta-analysis of studies that have attempted to longitudinally predict a specific STB-related outcome. This included 365 studies (3,428 total risk factor effect sizes) from the past 50 years. The present random-effects meta-analysis produced several unexpected findings: across odds ratio, hazard ratio, and diagnostic accuracy analyses, prediction was only slightly better than chance for all outcomes; no broad category or subcategory accurately predicted far above chance levels; predictive ability has not improved across 50 years of research; studies rarely examined the combined effect of multiple risk factors; risk factors have been homogenous over time, with 5 broad categories accounting for nearly 80% of all risk factor tests; and the average study was nearly 10 years long, but longer studies did not produce better prediction. The homogeneity of existing research means that the present meta-analysis could only speak to STB risk factor associations within very narrow methodological limits—limits that have not allowed for tests that approximate most STB theories. The present meta-analysis accordingly highlights several fundamental changes needed in future studies. In particular, these findings suggest the need for a shift in focus from risk factors to machine learning-based risk algorithms.|ca4dbc09-c47d-4a7c-8785-2a2d846cd0e6|Psychological bulletin|Risk Factors for Suicidal Thoughts and Behaviors: A Meta-Analysis of 50 Years of Research|2017|39865485;144945435;6643595;3527446;5769290;47932906;8256362;2493778;4751081;5027772|J. Franklin;J. Ribeiro;K. Fox;K. Bentley;Evan M. Kleiman;Xieyining Huang;Katherine M. Musacchio;A. Jaroszewski;B. Chang;M. Nock|Medicine;Psychology
5966d7c7f60898d610812e24c64d4d57855ad86a|10.1126/science.aal4230|Science|183 - 186|Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Semantics derived automatically from language corpora contain human-like biases|2016|144537437;145315445;47735253|Aylin Caliskan;J. Bryson;Arvind Narayanan|Medicine;Psychology;Computer Science
a86171e13f84fe32212dd7fb6a1c31a34a47155f|10.1073/PNAS.97.1.262|Proceedings of the National Academy of Sciences of the United States of America|"
          262-7
        "|We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their flexibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability to handle large feature spaces, and the ability to identify outliers. We test several SVMs that use different similarity metrics, as well as some other supervised learning methods, and find that the SVMs best identify sets of genes with a common function using expression data. Finally, we use SVMs to predict functional roles for uncharacterized yeast ORFs based on their expression data.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Knowledge-based analysis of microarray gene expression data by using support vector machines.|2000|143955418;2361327;2116443182;1685083;2070640;1716986;145729499;1733689|M. S. Brown;W. Grundy;D. Lin;N. Cristianini;C. Sugnet;T. Furey;M. Ares;D. Haussler|Medicine;Biology;Computer Science
62ed272e0e8b7be356c7f7595f5b7a22797a1c3e|10.1039/b918972f|The Analyst|"
          230-67
        "|The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.|59e5a209-e881-484a-a1e8-e395f7326743|In Analysis|Support vector machines for classification and regression.|2010|2793728;31847168|R. Brereton;G. Lloyd|Medicine;Computer Science;Chemistry
7ab0f0da686cd4094fd96f5a30e0b6072525fd09|10.1146/annurev-bioeng-071516-044442|Annual review of biomedical engineering|"
          221-248
        "|This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.|b1aef0b1-58ef-4dd9-aad5-2f40c42e19c4|Annual Review of Biomedical Engineering|Deep Learning in Medical Image Analysis.|2017|144986260;46531894;143802908|D. Shen;Guorong Wu;Heung-Il Suk|Medicine;Computer Science
049aca6228fb68a263369380eda6d9a4fcbdb382|10.1063/1.4822376|Proceedings of the IEEE|906-|"From the Publisher: 
As book review editor of the IEEE Transactions on Neural Networks, Mohamad Hassoun has had the opportunity to assess the multitude of books on artificial neural networks that have appeared in recent years. Now, in Fundamentals of Artificial Neural Networks, he provides the first systematic account of artificial neural network paradigms by identifying clearly the fundamental concepts and major methodologies underlying most of the current theory and practice employed by neural network researchers. 
Such a systematic and unified treatment, although sadly lacking in most recent texts on neural networks, makes the subject more accessible to students and practitioners. Here, important results are integrated in order to more fully explain a wide range of existing empirical observations and commonly used heuristics. There are numerous illustrative examples, over 200 end-of-chapter analytical and computer-based problems that will aid in the development of neural network analysis and design skills, and a bibliography of nearly 700 references. 
Proceeding in a clear and logical fashion, the first two chapters present the basic building blocks and concepts of artificial neural networks and analyze the computational capabilities of the basic network architectures involved. Supervised, reinforcement, and unsupervised learning rules in simple nets are brought together in a common framework in chapter three. The convergence and solution properties of these learning rules are then treated mathematically in chapter four, using the ""average learning equation"" analysis approach. This organization of material makes it natural to switch into learning multilayer nets using backpropand its variants, described in chapter five. Chapter six covers most of the major neural network paradigms, while associative memories and energy minimizing nets are given detailed coverage in the next chapter. The final chapter takes up Boltzmann machines and Boltzmann learning along with other global search/optimization algorithms such as stochastic gradient search, simulated annealing, and genetic algorithms."|6faaccca-1cc4-45a9-aeb6-96a4901d2606|Proceedings of the IEEE|Fundamentals of Artificial Neural Networks|1996|2679783;8909922;2096380087;2228894764|M. Hassoun;N. Intrator;S. Mckay;Wolfgang Christian|Computer Science
602f31242e577d2d05f918a3080fd50095e7faed|10.1109/ASPAA.2003.1285840|2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (IEEE Cat. No.03TH8684)|143-146|Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.|0bf449f6-ac85-4357-b5a8-0d5dc353c203|IEEE Workshop on Applications of Signal Processing to Audio and Acoustics|Factors in automatic musical genre classification of audio signals|2003|2286187694;1693065|Tao Li;G. Tzanetakis|Computer Science
fd8ce955dc0c570b66305dfbc65e4ed5f37658d0|10.1109/mex.1987.4307100|IEEE Expert|92-93|Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.|f9679adc-2c92-4f80-b2f3-c4e395e79688|IEEE Expert|Induction: Processes of Inference, Learning, and Discovery|1987|94653581;2009767;2518186;1756123;1743808|J. H. Holland;K. Holyoak;R. Nisbett;Paul Thagard;S. Smoliar|Philosophy;Computer Science
9691f67f5075bde2fd70da0135a4a70f25ef042b|10.1145/1273496.1273598|Mathematical Programming|3-30|We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\epsilon}$$ is $${\tilde{O}(1 / \epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\Omega(1 / \epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\tilde{O}(d/(\lambda \epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.|fc0a208c-acb7-47dc-a0d4-af8190e21d29|International Conference on Machine Learning|Pegasos: primal estimated sub-gradient solver for SVM|2007|1389955537;1740765;1706280;145658292|Shai Shalev-Shwartz;Y. Singer;N. Srebro;Andrew Cotter|Computer Science;Mathematics
72e93aa6767ee683de7f001fa72f1314e40a8f35|10.1109/ICASSP.2013.6639343|2013 IEEE International Conference on Acoustics, Speech and Signal Processing|8595-8598|We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.|0d6f7fba-7092-46b3-8039-93458dba736b|IEEE International Conference on Acoustics, Speech, and Signal Processing|Building high-level features using large scale unsupervised learning|2011|2827616;1706809;3089272;145139947;32131713;2118440152;49959210;34699434|Quoc V. Le;Marc'Aurelio Ranzato;R. Monga;M. Devin;G. Corrado;Kai Chen;J. Dean;A. Ng|Mathematics;Computer Science
48e752c719d33ff55b3b3bec3538727f8ce69399|10.1109/5254.920602|IEEE Intell. Syst.|72-79|The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.|7404efea-88b2-4c7c-8cb1-b3a8ced6363f|IEEE Intelligent Systems|Ontology Learning for the Semantic Web|2002|1806905;1752093|A. Maedche;Steffen Staab|Engineering;Computer Science
a6f835ca6e12245a835ab6074bc6ec2c3c60b85a|10.1109/TEVC.2019.2890858|IEEE Transactions on Evolutionary Computation|828-841|Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.|79644985-a91b-42a7-ac72-bb961c283f5e|IEEE Transactions on Evolutionary Computation|One Pixel Attack for Fooling Deep Neural Networks|2017|1730754;145197293;145106127|Jiawei Su;Danilo Vasconcellos Vargas;K. Sakurai|Mathematics;Computer Science
62ccd99a65bfc7c735ae1f33b75b107665de95df|10.1145/3298981|ACM Transactions on Intelligent Systems and Technology (TIST)|1 - 19|Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.|0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e|ACM Transactions on Intelligent Systems and Technology|Federated Machine Learning|2019|153096457;1614034792;11573257;8230559|Qiang Yang;Yang Liu;Tianjian Chen;Yongxin Tong|Computer Science
4d1fdd81f033cd58f3723bfc61e7d12079647a7a|10.1056/NEJMp1606181|The New England journal of medicine|"
          1216-9
        "|The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.|dc31f077-7737-4e33-baa3-bceeff44ec27|New England Journal of Medicine|Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.|2016|3797258;39714312|Z. Obermeyer;E. Emanuel|Medicine;Computer Science
afcc28d71be4ea6a48a339f9e4e5557d1b2b25be|10.1109/TSMCC.2011.2161285|IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)|463-484|Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.|ecb11fdd-9e59-482f-a3b6-0cb14372306c|IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)|A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches|2012|1924061;143741583;1695535;2256695032;2256697310|M. Galar;Alberto Fernández;E. Tartas;H. Bustince;F. Herrera|Computer Science
78989616eeeac55b202e3e4205225e7135054185|10.1109/TCCN.2017.2758370|IEEE Transactions on Cognitive Communications and Networking|563-575|We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.|65e58b80-9699-4da6-bd60-929b57b8533d|IEEE Transactions on Cognitive Communications and Networking|An Introduction to Deep Learning for the Physical Layer|2017|1388350203;1749686|Tim O'Shea;J. Hoydis|Mathematics;Engineering;Computer Science
a486e2839291111bb44fa1f07731ada123539f75|10.1162/tacl_a_00065|Transactions of the Association for Computational Linguistics|339-351|We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.|e0dbf116-86aa-418d-859f-a49952d7e44a|Transactions of the Association for Computational Linguistics|Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation|2016|2109675545;144927151;2827616;2048712;48607963;2545358;144203200;1765169;145233583;2227182886;48342565;2056946837|Melvin Johnson;M. Schuster;Quoc V. Le;M. Krikun;Yonghui Wu;Z. Chen;Nikhil Thorat;F. Viégas;M. Wattenberg;Gregory S. Corrado;Macduff Hughes;Jeffrey Dean|Linguistics;Computer Science
7ad66cba3b7e3abae7ef33122588512a146f7f77|10.1109/TKDE.2021.3070203|IEEE Transactions on Knowledge and Data Engineering|5586-5609|Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|A Survey on Multi-Task Learning|2017|46867608;152290618|Yu Zhang;Qiang Yang|Mathematics;Computer Science
3def68bd0f856886d34272840a7f81588f2bc082|10.1145/3571730|ACM Computing Surveys|1 - 38|Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.|7b2adce0-d53f-49d6-8784-b0645604fe62|ACM Computing Surveys|Survey of Hallucination in Natural Language Generation|2022|3391272;40221187;1397300094;1660855299;49678544;98271906;38524906;23672613;94210578;47653392;2111680936;40539650|Ziwei Ji;Nayeon Lee;Rita Frieske;Tiezheng Yu;D. Su;Yan Xu;Etsuko Ishii;Yejin Bang;Delong Chen;Wenliang Dai;Andrea Madotto;Pascale Fung|Linguistics;Computer Science
0f5a0fbd07155cbf81ae9a5e76a1bc78da10a376|10.1109/COMST.2019.2902862|IEEE Communications Surveys & Tutorials|2334-2360|The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. On the other hand, UAVs can operate as flying mobile terminals within a cellular network. Such cellular-connected UAVs can enable several applications ranging from real-time video streaming to item delivery. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as 3D deployment, performance analysis, channel modeling, and energy efficiency are explored along with representative results. Then, open problems and potential research directions pertaining to UAV communications are introduced. Finally, various analytical frameworks and mathematical tools, such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and Open Problems|2018|2250918;145412074;1702172;3223942;145118318|Mohammad Mozaffari;W. Saad;M. Bennis;Young-Han Nam;M. Debbah|Mathematics;Engineering;Computer Science
8515a302b8f389f8f1008cc2650e5ec0a6913e24|10.1109/MSP.2008.4408441|IEEE Signal Processing Magazine|41-56|Due to the volume conduction multichannel electroencephalogram (EEG) recordings give a rather blurred image of brain activity. Therefore spatial filters are extremely useful in single-trial analysis in order to improve the signal-to-noise ratio. There are powerful methods from machine learning and signal processing that permit the optimization of spatio-temporal filters for each subject in a data dependent fashion beyond the fixed filters based on the sensor geometry, e.g., Laplacians. Here we elucidate the theoretical background of the common spatial pattern (CSP) algorithm, a popular method in brain-computer interface (BCD research. Apart from reviewing several variants of the basic algorithm, we reveal tricks of the trade for achieving a powerful CSP performance, briefly elaborate on theoretical aspects of CSP, and demonstrate the application of CSP-type preprocessing in our studies of the Berlin BCI (BBCI) project.|f62e5eab-173a-4e0a-a963-ed8de9835d22|IEEE Signal Processing Magazine|Optimizing Spatial filters for Robust EEG Single-Trial Analysis|2008|3156886;2870603;3294736;1716788;72571095|B. Blankertz;Ryota Tomioka;S. Lemm;M. Kawanabe;K.-R. Muller|Engineering;Computer Science
21cea8f56a0d067d640f923b2d69e18ed5542f6d|10.1109/34.868684|IEEE Trans. Pattern Anal. Mach. Intell.|831-843|"We describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task. The system is particularly concerned with detecting when interactions between people occur, and classifying the type of interaction. Examples of interesting interaction behaviors include following another person, altering one's path to meet another, and so forth. Our system combines top-down with bottom-up information in a closed feedback loop, with both components employing a statistical Bayesian approach. We propose and compare two different state-based learning architectures, namely HMMs and CHMMs, for modeling behaviors and interactions. The CHMM model is shown to work much more efficiently and accurately. 
 
Finally, to deal with the problem of limited training data, a synthetic 'Alife-style' training system is used to develop flexible prior models for recognizing human interactions. We demonstrate the ability to use these a priori models to accurately classify real human behaviors and interactions with no additional tuning or training."|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|A Bayesian Computer Vision System for Modeling Human Interactions|1999|145709776;3199842;144994682|Nuria Oliver;Barbara Rosario;A. Pentland|Computer Science
b8ebda42e272d3617375118542d4675a0c0e501d|10.1109/CVPR.2017.572|2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|5385-5394|In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Deep Hashing Network for Unsupervised Domain Adaptation|2017|3151995;2072459779;2471253;1743991|Hemanth Venkateswara;José Eusébio;Shayok Chakraborty;S. Panchanathan|Computer Science
f7d997a640f2b804676cadb8030d8b2c7bd79d85|10.5555/1756006.1859921|J. Mach. Learn. Res.|2079-2107|Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation|2010|8974886;28280741|G. Cawley;N. L. C. Talbot|Computer Science
a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e|10.1109/TPAMI.2008.275|IEEE Transactions on Pattern Analysis and Machine Intelligence|105-119|The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Faster and Better: A Machine Learning Approach to Corner Detection|2008|1721991;145881323;144418842|E. Rosten;R. Porter;T. Drummond|Medicine;Computer Science
467568f1777bc51a15a5100516cd4fe8de62b9ab|10.5555/1577069.1755839|J. Mach. Learn. Res.|1633-1685|The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Transfer Learning for Reinforcement Learning Domains: A Survey|2009|39286677;144848112|Matthew E. Taylor;P. Stone|Computer Science
2521c3d76bc439c961b7003080f4a7a661949547|10.1126/SCIENCE.1105809|Science|523 - 529|Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data|2005|46599630;3289622;1397424343;1761370;1857137|K. Sachs;O. Perez;D. Pe’er;D. Lauffenburger;G. Nolan|Medicine;Biology;Computer Science
65b16da51891a6b98140d425804c8a0fd0299219|10.1109/CVPR.2017.35|2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|257-265|Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring|2016|40648435;2111251935;2135837|Seungjun Nah;Tae Hyun Kim;Kyoung Mu Lee|Engineering;Computer Science
e4a85af3f5dc41e13dc2cae9ee851953709b764e|10.1126/science.aag2302|Science|602 - 606|Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Solving the quantum many-body problem with artificial neural networks|2016|50666189;1752096|Giuseppe Carleo;M. Troyer|Medicine;Computer Science;Physics
85d727b119304dde458bcd8cf5cb87a906fb41ba|10.1109/TKDE.2005.50|IEEE Transactions on Knowledge and Data Engineering|299-310|The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|Using AUC and accuracy in evaluating learning algorithms|2005|2110151928;1688204|Jin Huang;C. Ling|Computer Science
1c00df1cb85fa7886b6666599eab59f2b301dd5d|10.1109/TNN.2006.880583|IEEE Transactions on Neural Networks|1411-1423|In this paper, we develop an online sequential learning algorithm for single hidden layer feedforward networks (SLFNs) with additive or radial basis function (RBF) hidden nodes in a unified framework. The algorithm is referred to as online sequential extreme learning machine (OS-ELM) and can learn data one-by-one or chunk-by-chunk (a block of data) with fixed or varying chunk size. The activation functions for additive nodes in OS-ELM can be any bounded nonconstant piecewise continuous functions and the activation functions for RBF nodes can be any integrable piecewise continuous functions. In OS-ELM, the parameters of hidden nodes (the input weights and biases of additive nodes or the centers and impact factors of RBF nodes) are randomly selected and the output weights are analytically determined based on the sequentially arriving data. The algorithm uses the ideas of ELM of Huang developed for batch learning which has been shown to be extremely fast with generalization performance better than other batch training methods. Apart from selecting the number of hidden nodes, no other control parameters have to be manually chosen. Detailed performance comparison of OS-ELM is done with other popular sequential learning algorithms on benchmark problems drawn from the regression, classification and time series prediction areas. The results show that the OS-ELM is faster than the other sequential algorithms and produces better generalization performance|2ac50919-507e-41c7-93a8-721c4b804757|IEEE Transactions on Neural Networks|A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks|2006|2972859;145678691;1800678;145411276|Nan-Ying Liang;G. Huang;P. Saratchandran;N. Sundararajan|Medicine;Computer Science
cc5afe344cc7ed7acd68a28b9774ea8023a162dc|10.1609/aimag.v38i3.2741|AI Mag.|50-57|We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.|6fedff74-7525-4b7f-bbb4-4df4e23948e4|The AI Magazine|"European Union Regulations on Algorithmic Decision-Making and a ""Right to Explanation"""|2016|11025540;2127497|B. Goodman;S. Flaxman|Law;Engineering;Computer Science;Mathematics
8c8215b7f8111839f0066010a530a3a9f57ba15e|10.1613/jair.530|J. Artif. Intell. Res.|317-365|This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.|aef12dca-60a0-4ca3-819b-cad26d309d4e|Journal of Artificial Intelligence Research|AntNet: Distributed Stigmergetic Control for Communications Networks|1998|1744127;153570946|G. D. Caro;M. Dorigo|Computer Science
bb144c04b9eb44579b19d21c3d5954401408440b|10.5555/2567709.2567736|J. Mach. Learn. Res.|2349-2353|Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Orange: data mining toolbox in python|2013|2701105;1772593;48828579;88256002;2820151;40337361;145703442;2448692;3110092;1851562;2789798;2256143;2761087;3105120;2095762;1775384|J. Demšar;Tomaž Curk;Ales Erjavec;C. Gorup;Tomaz Hocevar;Mitar Milutinovic;M. Mozina;M. Polajnar;Marko Toplak;A. Staric;Miha Stajdohar;Lan Umek;Lan Zagar;Jure Zbontar;M. Zitnik;B. Zupan|Computer Science
d13e466cbd7c8ac764b3dfd3e3a12c7e8736412f|10.1109/JPROC.2009.2035722|Proceedings of the IEEE|925-936|On the heels of compressed sensing, a new field has very recently emerged. This field addresses a broad range of problems of significant practical interest, namely, the recovery of a data matrix from what appears to be incomplete, and perhaps even corrupted, information. In its simplest form, the problem is to recover a matrix from a small sample of its entries. It comes up in many areas of science and engineering, including collaborative filtering, machine learning, control, remote sensing, and computer vision, to name a few. This paper surveys the novel literature on matrix completion, which shows that under some suitable conditions, one can recover an unknown low-rank matrix from a nearly minimal set of entries by solving a simple convex optimization problem, namely, nuclear-norm minimization subject to data constraints. Further, this paper introduces novel results showing that matrix completion is provably accurate even when the few observed entries are corrupted with a small amount of noise. A typical result is that one can recover an unknown matrix of low rank from just about log noisy samples with an error that is proportional to the noise level. We present numerical results that complement our quantitative analysis and show that, in practice, nuclear-norm minimization accurately fills in the many missing entries of large low-rank matrices from just a few noisy samples. Some analogies between matrix completion and compressed sensing are discussed throughout.|6faaccca-1cc4-45a9-aeb6-96a4901d2606|Proceedings of the IEEE|Matrix Completion With Noise|2009|2006869;1800179|E. Candès;Y. Plan|Mathematics;Engineering;Computer Science
bd898f483476e3dcacf83cd85efc64e6319da0e1|10.1109/RBME.2009.2034865|IEEE Reviews in Biomedical Engineering|147-171|Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe.|042aa85f-6dca-421f-809b-e0ea76c1e58b|IEEE Reviews in Biomedical Engineering|Histopathological Image Analysis: A Review|2009|5733445;2034769;145643855;1705442;2229652;7834060|M. Gurcan;L. Boucheron;A. Can;A. Madabhushi;N. Rajpoot;B. Yener|Medicine;Computer Science
d7701e78e0bfc92b03a89582e80cfb751ac03f26|10.1109/DSAA.2018.00018|2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)|80-89|There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.|770f1d88-52c2-43dc-a897-6881e70b2f32|International Conference on Data Science and Advanced Analytics|Explaining Explanations: An Overview of Interpretability of Machine Learning|2018|145019478;144159726;144002190;50397921;144417360;1735243|Leilani H. Gilpin;David Bau;Ben Z. Yuan;Ayesha Bajwa;Michael A. Specter;Lalana Kagal|Mathematics;Computer Science
574449170f293dfa868771e9ee0403b56a19b9e9|10.1080/00404969.2019.1653638|Textile History|212 - 217|. The article presents artistic projects in which the issue of gender is actualized. The above was carried out from the position of post-humanist ideas through the study of the impact of the development of technologies on modern socio-cultural processes. Thus, the projects created in different genres were analyzed, including post-cinema (“Night Walk for Edinburgh”, “A Total Jizzfest”) aimed at chang-ing gender perceptions in society, Ellen Pearlman’s opera “Emotionally intelligent” Artificially Intelligent Brainwave Opera” (AIBO), in which, thanks to the involvement of artificial intelligence as an actor, gender opposition is reinforced. The Female Laptop Orchestra (FLO) project is dedicated to the support of female creativity, which, thanks to the involvement of information and communication tools, com-bines music creation with research in its activity. An important aspect of the considered projects is defined as interactivity, which is im-plemented by involving the viewer in the process of their creation. It was found that, on the one hand, the digital virtual space made it possible to reduce the importance of social roles or gender affili-ation, thereby leveling the stereotype of the opposition between the author’s masculinity and femininity. On the other hand, with the development of machine learning technologies, a new opposition between the human author and the artificial intelligence in the same ca-pacity is gradually emerging.|53a15011-a562-4f8c-ac7f-4a51e74359ea|Textile History|Gender|2019|144126414|A. Hood|Art
2b7f9117eb6608a58be4c078ca3d69c0e5ccb875|10.1109/SP.2017.12|2017 IEEE Symposium on Security and Privacy (SP)|19-38|Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|SecureML: A System for Scalable Privacy-Preserving Machine Learning|2017|1773836;2108473999|Payman Mohassel;Yupeng Zhang|Computer Science
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3|10.1145/3386252|ACM Computing Surveys (CSUR)|1 - 34|Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1|7b2adce0-d53f-49d6-8784-b0645604fe62|ACM Computing Surveys|Generalizing from a Few Examples|2019|2115793087;3259992;145193332;1726587|Yaqing Wang;Quanming Yao;J. Kwok;L. Ni|Computer Science
61394599ed0aabe04b724c7ca3a778825c7e776f|10.1109/TPAMI.2015.2509974|IEEE Transactions on Pattern Analysis and Machine Intelligence|2096-2109|Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Struck: Structured Output Tracking with Kernels|2016|1837057;143777501;1741702;143729959;37535930;2445538;143635540|Sam Hare;S. Golodetz;Amir Saffari;Vibhav Vineet;Ming-Ming Cheng;S. Hicks;Philip H. S. Torr|Medicine;Computer Science
b16408a97170785fb216c9e8b7920d64f478fbc8|10.1109/MGRS.2016.2540798|IEEE Geoscience and Remote Sensing Magazine|22-40|Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input?output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding.In this technical tutorial, a general framework of DL for RS data is provided, and the state-of-the-art DL methods in RS are regarded as special cases of input-output data combined with various deep networks and tuning tricks. Although extensive experimental results confirm the excellent performance of the DL-based algorithms in RS big data analysis, even more exciting prospects can be expected for DL in RS. Key bottlenecks and potential directions are also indicated in this article, guiding further research into DL for RS data.|f3b9cfef-ea93-4f7f-a14f-95fbf796875e|IEEE Geoscience and Remote Sensing Magazine|Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art|2016|9802604;2107901992;145728792|Liangpei Zhang;Lefei Zhang;Bo Du|Computer Science;Environmental Science
f3203d0bdefc9670ed508ca776d08aa9f024bafa|10.1109/tnn.1997.623228|IEEE Transactions on Neural Networks|1219-1219|Included in Prentice Hall's MATLAB Curriculum Series, this text provides a comprehensive treatment of the methodologies underlying neuro-fuzzy and soft computing. The book places equal emphasis on theoretical aspects of covered methodologies, empirical observations, and verifications of various applications in practice.|2ac50919-507e-41c7-93a8-721c4b804757|IEEE Transactions on Neural Networks|Neuro-fuzzy And Soft Computing: A Computational Approach To Learning And Machine Intelligence [Books in Brief]|1997|144293175;2216207827|J. Jang;Chuen-Tsai Sun|Computer Science
01b24de15cf337c55b9866c4b534596ca3d93abe|10.1073/PNAS.0500334102|Proceedings of the National Academy of Sciences of the United States of America|"
          7426-31
        "|We provide a framework for structural multiscale geometric organization of graphs and subsets of R(n). We use diffusion semigroups to generate multiscale geometries in order to organize and represent complex structures. We show that appropriately selected eigenfunctions or scaling functions of Markov matrices, which describe local transitions, lead to macroscopic descriptions at different scales. The process of iterating or diffusing the Markov matrix is seen as a generalization of some aspects of the Newtonian paradigm, in which local infinitesimal transitions of a system lead to global macroscopic descriptions by integration. We provide a unified view of ideas from data analysis, machine learning, and numerical analysis.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps.|2005|1780112;37805393;1832448;34207023;1786884;49818480;1698824|R. Coifman;Stéphane Lafon;Ann B. Lee;M. Maggioni;B. Nadler;F. Warner;S. Zucker|Medicine;Mathematics;Physics;Computer Science
aaf9069be5a498179cbd2932d793ea1b9d0092de|10.1109/COMST.2020.2986024|IEEE Communications Surveys & Tutorials|2031-2063|In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|Federated Learning in Mobile Edge Networks: A Comprehensive Survey|2019|1753717562;3046954;2233724;2878072;144791622;153096457;2266084696;1679209|Wei Yang Bryan Lim;Nguyen Cong Luong;D. Hoang;Yutao Jiao;Ying-Chang Liang;Qiang Yang;Dusist Niyato;C. Miao|Engineering;Computer Science
2a3842f6070b4554ff21fe62b2a486657d9a304a|10.1126/science.1192788|Science|1279 - 1285|In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|How to Grow a Mind: Statistics, Structure, and Abstraction|2011|2238317928;2249973698;2247786963;2250210463|J. B. Tenenbaum;Charles Kemp;Thomas L. Griffiths;Noah D. Goodman|Medicine;Psychology;Philosophy;Computer Science
c2b381b24aabf237394059fed7920cd6fd0e67b8|10.1109/69.250074|IEEE Trans. Knowl. Data Eng.|914-925|The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|Database Mining: A Performance Perspective|1993|144947410;1733797;31536502|R. Agrawal;T. Imielinski;A. Swami|Computer Science
d0ab11de3077490c80a08abd0fb8827bac84c454|10.1039/c7sc02664a|Chemical Science|513 - 530|A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.|2a0713ba-a4af-4a0a-ae49-81b8edeca660|Chemical Science|MoleculeNet: a benchmark for molecular machine learning|2017|9957625;2378027;5932099;145986494;2347660128;5929246;40867019;1806271|Zhenqin Wu;Bharath Ramsundar;Evan N. Feinberg;Joseph Gomes;Caleb Geniesse;Aneesh S. Pappu;K. Leswing;V. Pande|Chemistry;Medicine;Physics;Mathematics;Computer Science
9071775ebcfebddd54d879fe7e6c627673e4d305|10.1145/3007787.3001139|2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)|14-26|A number of recent efforts have attempted to design accelerators for popular machine learning algorithms, such as those involving convolutional and deep neural networks (CNNs and DNNs). These algorithms typically involve a large number of multiply-accumulate (dot-product) operations. A recent project, DaDianNao, adopts a near data processing approach, where a specialized neural functional unit performs all the digital arithmetic operations and receives input weights from adjacent eDRAM banks. This work explores an in-situ processing approach, where memristor crossbar arrays not only store input weights, but are also used to perform dot-product operations in an analog manner. While the use of crossbar memory as an analog dot-product engine is well known, no prior work has designed or characterized a full-fledged accelerator based on crossbars. In particular, our work makes the following contributions: (i) We design a pipelined architecture, with some crossbars dedicated for each neural network layer, and eDRAM buffers that aggregate data between pipeline stages. (ii) We define new data encoding techniques that are amenable to analog computations and that can reduce the high overheads of analog-to-digital conversion (ADC). (iii) We define the many supporting digital components required in an analog CNN accelerator and carry out a design space exploration to identify the best balance of memristor storage/compute, ADCs, and eDRAM storage on a chip. On a suite of CNN and DNN workloads, the proposed ISAAC architecture yields improvements of 14.8×, 5.5×, and 7.5× in throughput, energy, and computational density (respectively), relative to the state-of-the-art DaDianNao architecture.|deedf64a-dd5c-4b33-b345-ff83bfb93d71|International Symposium on Computer Architecture|ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars|2016|1433007765;1783032;2358911;1777422;33102961;143707095;1761983;3052879|Ali Shafiee;Anirban Nag;N. Muralimanohar;R. Balasubramonian;J. Strachan;Miao Hu;R. S. Williams;Vivek Srikumar|Engineering;Computer Science
7da323e7103245eeaed32367c46abe3f4913df86|10.1109/SURV.2008.080406|IEEE Communications Surveys & Tutorials|56-76|The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|A survey of techniques for internet traffic classification using machine learning|2008|1716282;145027304|Thuy T. T. Nguyen;G. Armitage|Computer Science
e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da|10.1109/JSAC.2019.2904348|IEEE Journal on Selected Areas in Communications|1205-1221|Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.|68f20e73-515e-4c73-9cd5-5684926b45f7|IEEE Journal on Selected Areas in Communications|Adaptive Federated Learning in Resource Constrained Edge Computing Systems|2018|50695457;40917131;2522394;145353889;1702283;145299837;46998035|Shiqiang Wang;Tiffany Tuor;Theodoros Salonidis;K. Leung;C. Makaya;T. He;K. Chan|Mathematics;Engineering;Computer Science
e9126a98de0c39dcffe4c4f5158e037460196724|10.1063/1.5019779|The Journal of chemical physics|"
          241722
        "|Deep learning has led to a paradigm shift in artificial intelligence, including web, text, and image search, speech recognition, as well as bioinformatics, with growing impact in chemical physics. Machine learning, in general, and deep learning, in particular, are ideally suitable for representing quantum-mechanical interactions, enabling us to model nonlinear potential-energy surfaces or enhancing the exploration of chemical compound space. Here we present the deep learning architecture SchNet that is specifically designed to model atomistic systems by making use of continuous-filter convolutional layers. We demonstrate the capabilities of SchNet by accurately predicting a range of properties across chemical space for molecules and materials, where our model learns chemically plausible embeddings of atom types across the periodic table. Finally, we employ SchNet to predict potential-energy surfaces and energy-conserving force fields for molecular dynamics simulations of small molecules and perform an exemplary study on the quantum-mechanical properties of C20-fullerene that would have been infeasible with regular ab initio molecular dynamics.|1bb63b2b-3f57-4387-aaf6-b2a33dfcdcc5|Journal of Chemical Physics|SchNet - A deep learning architecture for molecules and materials.|2017|33075217;10667063;51005232;2462983;145034054|Kristof T. Schütt;H. Sauceda;P. Kindermans;A. Tkatchenko;K. Müller|Chemistry;Medicine;Physics;Materials Science;Computer Science
de2be42659be5c43c1a992b5d7fe6daf14e571dd|10.1109/TGRS.2017.2685945|IEEE Transactions on Geoscience and Remote Sensing|3965-3981|Aerial scene classification, which aims to automatically label an aerial image with a specific semantic category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent years, it has become an active task in the remote sensing area, and numerous algorithms have been proposed for this task, including many machine learning and data-driven approaches. However, the existing data sets for aerial scene classification, such as UC-Merced data set and WHU-RS19, contain relatively small sizes, and the results on them are already saturated. This largely limits the development of scene classification algorithms. This paper describes the Aerial Image data set (AID): a large-scale data set for aerial scene classification. The goal of AID is to advance the state of the arts in scene classification of remote sensing images. For creating AID, we collect and annotate more than 10000 aerial scene images. In addition, a comprehensive review of the existing aerial scene classification techniques as well as recent widely used deep learning methods is given. Finally, we provide a performance analysis of typical aerial scene classification and deep learning approaches on AID, which can be served as the baseline results on this benchmark.|70628d6a-97aa-4571-9701-bc0eb3989c32|IEEE Transactions on Geoscience and Remote Sensing|AID: A Benchmark Data Set for Performance Evaluation of Aerial Scene Classification|2016|39943835;49268477;144322708;2276155;145905113;2798207;9802604;7828998|Gui-Song Xia;Jingwen Hu;Fan Hu;Baoguang Shi;X. Bai;Yanfei Zhong;Liangpei Zhang;Xiaoqiang Lu|Computer Science;Environmental Science
0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c|10.1145/1380584.1380586|ACM Comput. Surv.|8:1-8:49|Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.|cd648e4a-f86b-4c14-8c82-0fc488d998ff|Conference of the Association for Machine Translation in the Americas|Statistical machine translation|2008|2244001304;143758717|Hany Hassan;Kareem Darwish|Linguistics;Computer Science
dd971c07879e1ce12b06991319528c06280eeb9b|10.5167/UZH-185139|IEEE Transactions on Pattern Analysis and Machine Intelligence|154-180|Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of $\mu$μs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Event-Based Vision: A Survey|2019|144036711;1694635;33780923;1897771;1736425;1860631;2864731;2052135690;3302681;1751586;2075371|Guillermo Gallego;T. Delbrück;G. Orchard;C. Bartolozzi;B. Taba;A. Censi;Stefan Leutenegger;A. Davison;J. Conradt;Kostas Daniilidis;D. Scaramuzza|Medicine;Engineering;Physics;Computer Science
f86f1748d1b6d22870f4347fd5d65314ba800583|10.1073/pnas.1903070116|Proceedings of the National Academy of Sciences|15849 - 15854|Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Reconciling modern machine-learning practice and the classical bias–variance trade-off|2018|145520115;143724861;143791100;151213231|M. Belkin;Daniel J. Hsu;Siyuan Ma;Soumik Mandal|Medicine;Computer Science;Mathematics
df2a7756382540e92895f10703cec32d50c4f316|10.1103/PhysRevLett.108.058301|Physical review letters|"
          058301
        "|We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.|16c9f9d4-bee1-435d-8c85-22a3deba109d|Physical Review Letters|Fast and accurate modeling of molecular atomization energies with machine learning.|2011|48041657;2462983;145034054;7847508|M. Rupp;A. Tkatchenko;K. Müller;O. A. von Lilienfeld|Chemistry;Medicine;Physics;Mathematics;Computer Science
033f25ad905ef2ed32a8331cf38b83953ff15922|10.1109/JPROC.2015.2483592|Proceedings of the IEEE|11-33|Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.|6faaccca-1cc4-45a9-aeb6-96a4901d2606|Proceedings of the IEEE|A Review of Relational Machine Learning for Knowledge Graphs|2015|1729762;1702318;1700754;1718798|Maximilian Nickel;K. Murphy;Volker Tresp;E. Gabrilovich|Computer Science;Mathematics
5c7e5248d9eb7f373f10277410bf8506160907ea|10.1126/science.aat8084|Science|1004 - 1008|All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|All-optical machine learning using diffractive deep neural networks|2018|143746845;48156295;3913651;4901769;5602194;2146359;2660014|Xing Lin;Y. Rivenson;N. Yardimci;Muhammed Veli;Yilin Luo;M. Jarrahi;A. Ozcan|Medicine;Engineering;Physics;Computer Science
4b149a326e38b9237077d794a0d5f5b4865efacf|10.1109/TAFFC.2017.2740923|IEEE Transactions on Affective Computing|18-31|Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1,250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.|a88e2a6c-903f-42e1-8dfc-f2547e32020a|IEEE Transactions on Affective Computing|AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild|2017|2314025;9706655;145531712|A. Mollahosseini;Behzad Hasani;M. Mahoor|Computer Science
8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92|10.1109/SP.2010.25|2010 IEEE Symposium on Security and Privacy|305-316|"In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational ""real world"" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection."|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|Outside the Closed World: On Using Machine Learning for Network Intrusion Detection|2010|1690799;1744800|Robin Sommer;V. Paxson|Computer Science
220ac48a22547a455d05f416e1fd22bbd0b0788d|10.1109/CVPR.2017.18|2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)|95-104|Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks|2016|2732737;2286640;35363891;1761978;1707347|Konstantinos Bousmalis;N. Silberman;David Dohan;D. Erhan;Dilip Krishnan|Computer Science
b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b|10.1109/JRPROC.1961.287775|Proceedings of the IRE|8-30|The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.|82b56675-f0f3-487f-b83e-155a762e2855|Proceedings of the IRE|Steps toward Artificial Intelligence|1995|1847175|M. Minsky|Computer Science
ffa6396d0749697fc11e24a5d9ce53f8bf10c1fc|10.1109/MVT.2019.2921208|IEEE Vehicular Technology Magazine|28-41|A key enabler for the intelligent information society of 2030, 6G networks are expected to provide performance superior to 5G and satisfy emerging services and applications. In this article, we present our vision of what 6G will be and describe usage scenarios and requirements for multi-terabyte per second (Tb/s) and intelligent 6G networks. We present a large-dimensional and autonomous network architecture that integrates space, air, ground, and underwater networks to provide ubiquitous and unlimited wireless connectivity. We also discuss artificial intelligence (AI) and machine learning [1], [2] for autonomous networks and innovative air-interface design. Finally, we identify several promising technologies for the 6G ecosystem, including terahertz (THz) communications, very-large-scale antenna arrays [i.e., supermassive (SM) multiple-input, multiple-output (MIMO)], large intelligent surfaces (LISs) and holographic beamforming (HBF), orbital angular momentum (OAM) multiplexing, laser and visible-light communications (VLC), blockchain-based spectrum sharing, quantum communications and computing, molecular communications, and the Internet of Nano-Things.|5f700bd7-638a-4bee-8ca7-3c7bff9eaf9d|IEEE Vehicular Technology Magazine|6G Wireless Networks: Vision, Requirements, Architecture, and Key Technologies|2019|103010565;1749827;1730232;2263629384;144095199;2301538;144015029;145563833|Zhengquan Zhang;Yue Xiao;Zheng Ma;Ming Xiao;Z. Ding;Xianfu Lei;G. Karagiannidis;P. Fan|Engineering;Computer Science;Environmental Science
7380e343dd4547e21d5118b16daf03d021d98c4e|10.1109/ICCV.2017.371|2017 IEEE International Conference on Computer Vision (ICCV)|3449-3457|As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.|7654260e-79f9-45c5-9663-d72027cf88f3|IEEE International Conference on Computer Vision|Interpretable Explanations of Black Boxes by Meaningful Perturbation|2017|25576460;1687524|Ruth C. Fong;A. Vedaldi|Mathematics;Computer Science
36d442f59c61ea2912d227c24dee76778c546b0a|10.1109/JPROC.2018.2820126|Proceedings of the IEEE|808-828|Research in graph signal processing (GSP) aims to develop tools for processing data defined on irregular graph domains. In this paper, we first provide an overview of core ideas in GSP and their connection to conventional digital signal processing, along with a brief historical perspective to highlight how concepts recently developed in GSP build on top of prior research in other areas. We then summarize recent advances in developing basic GSP tools, including methods for sampling, filtering, or graph learning. Next, we review progress in several application areas using GSP, including processing and analysis of sensor network data, biological data, and applications to image processing and machine learning.|6faaccca-1cc4-45a9-aeb6-96a4901d2606|Proceedings of the IEEE|Graph Signal Processing: Overview, Challenges, and Applications|2017|145029825;1703189;2442915;51283515;1697397|Antonio Ortega;P. Frossard;J. Kovacevic;J. Moura;P. Vandergheynst|Engineering;Computer Science;Environmental Science
afa778ba0ba6333e25671cfb691a4bdda13b2868|10.1109/TIFS.2020.2988575|IEEE Transactions on Information Forensics and Security|3454-3469|"Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients’ private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients’ side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula>-client random scheduling strategy, where <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula> (<inline-formula> <tex-math notation=""LaTeX"">$1\leq K< N$ </tex-math></inline-formula>) clients are randomly selected from the <inline-formula> <tex-math notation=""LaTeX"">$N$ </tex-math></inline-formula> overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula>-client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula> that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels."|d406a3f4-dc05-43be-b1f6-812f29de9c0e|IEEE Transactions on Information Forensics and Security|Federated Learning With Differential Privacy: Algorithms and Performance Analysis|2019|145158805;145409858;145573463;144496518;8896870;2083526329;145824256;1718541;145967056|Kang Wei;Jun Li;Ming Ding;Chuan Ma;H. Yang;Farokhi Farhad;Shi Jin;Tony Q. S. Quek;H. Poor|Mathematics;Computer Science
872bae24c109f7c30e052ac218b17a8b028d08a0|10.1162/NECO_a_00142|Neural Computation|1661-1674|Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|A Connection Between Score Matching and Denoising Autoencoders|2011|145467703|Pascal Vincent|Medicine;Mathematics;Computer Science
b57c54350769ffa59ff57f79ee5aad918844d298|10.5555/1953048.2021036|Journal of machine learning research : JMLR|"
          1069-1109
        "|Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Differentially Private Empirical Risk Minimization|2009|38120884;1806678;9208982|Kamalika Chaudhuri;C. Monteleoni;A. Sarwate|Medicine;Mathematics;Computer Science
9d3e0fce253a4ae4a4456b2f24c03329a2b74621|10.1109/JBHI.2016.2636665|IEEE Journal of Biomedical and Health Informatics|4-21|With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.|eac74c9c-a5c0-417d-8088-8164a6a8bfb3|IEEE journal of biomedical and health informatics|Deep Learning for Health Informatics|2017|2347855602;1905807;2775904;3163767;1443783456;1745644;144574968|Daniele Ravì;Charence Wong;F. Deligianni;M. Berthelot;Javier Andreu-Perez;Benny P. L. Lo;Guang-Zhong Yang|Medicine;Computer Science
3b7d120c0e801ef318bc9c607a0789f175637c7f|10.1109/FG.2018.00019|2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)|59-66|Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes.|b0c05768-6345-45d7-b541-235edf6ead54|IEEE International Conference on Automatic Face & Gesture Recognition|OpenFace 2.0: Facial Behavior Analysis Toolkit|2018|1756344;144802290;144529448;49933077|T. Baltrušaitis;Amir Zadeh;Y. Lim;Louis-Philippe Morency|Computer Science
771479c18b586eafae21baf262a220aaa7b2eef6|10.1109/TGRS.2016.2601622|IEEE Transactions on Geoscience and Remote Sensing|7405-7415|Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.|70628d6a-97aa-4571-9701-bc0eb3989c32|IEEE Transactions on Geoscience and Remote Sensing|Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images|2016|2152127024;2895041;7181955|Gong Cheng;Peicheng Zhou;Junwei Han|Engineering;Computer Science;Environmental Science
8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f|10.1126/science.aaw4741|Science|1026 - 1030|Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations|2020|145401977;37412357;1720124|M. Raissi;A. Yazdani;G. Karniadakis|Medicine;Engineering;Physics;Computer Science
cedea36fa3692281b3ac767335fe49a16d00957d|10.1111/AJPS.12103|American Journal of Political Science|1064-1082|Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.|17c57465-210d-4f43-a033-59d25938d6b6|American Journal of Political Science|Structural Topic Models for Open‐Ended Survey Responses|2014|2464550;28924497;3252940;2061550766;1403360325;13901928;50483051;2157480|Margaret E. Roberts;Brandon M Stewart;D. Tingley;Christopher Lucas;Jetson Leder-Luis;S. Gadarian;B. Albertson;David G. Rand|Computer Science
1a827052f01ef830cbc849c71e9da99791243a5f|10.1002/cpbi.11|Current Protocols in Bioinformatics|14.10.1 - 14.10.91|MetaboAnalyst (http://www.metaboanalyst.ca) is a comprehensive Web application for metabolomic data analysis and interpretation. MetaboAnalyst handles most of the common metabolomic data types from most kinds of metabolomics platforms (MS and NMR) for most kinds of metabolomics experiments (targeted, untargeted, quantitative). In addition to providing a variety of data processing and normalization procedures, MetaboAnalyst also supports a number of data analysis and data visualization tasks using a range of univariate, multivariate methods such as PCA (principal component analysis), PLS‐DA (partial least squares discriminant analysis), heatmap clustering and machine learning methods. MetaboAnalyst also offers a variety of tools for metabolomic data interpretation including MSEA (metabolite set enrichment analysis), MetPA (metabolite pathway analysis), and biomarker selection via ROC (receiver operating characteristic) curve analysis, as well as time series and power analysis. This unit provides an overview of the main functional modules and the general workflow of the latest version of MetaboAnalyst (MetaboAnalyst 3.0), followed by eight detailed protocols. © 2016 by John Wiley & Sons, Inc.|75a7291e-6312-43ec-b91d-fd0c368b868d|Current Protocols in Bioinformatics|Using MetaboAnalyst 3.0 for Comprehensive Metabolomics Data Analysis|2016|144545434;2066145|J. Xia;D. Wishart|Medicine;Chemistry;Computer Science
da048cdf883f2ac0551162cb1abd7e6d09e8e86a|10.1109/TPAMI.2009.187|IEEE Transactions on Pattern Analysis and Machine Intelligence|569-575|In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the k-fold cross-validation classification error estimator (k-cv). Our main contribution is a novel theoretical decomposition of the variance of the k-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of k. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of k-fold cross validation.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation|2010|2116899260;2110404467;144762651|J. D. Rodríguez;Aritz Pérez Martínez;J. A. Lozano|Medicine;Mathematics;Computer Science
30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70|10.1109/SP.2019.00029|2019 IEEE Symposium on Security and Privacy (SP)|691-706|Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses.|29b9c461-963e-4d11-b2ab-92c182243942|IEEE Symposium on Security and Privacy|Exploiting Unintended Feature Leakage in Collaborative Learning|2018|145557680;3469125;1728207;1723945|Luca Melis;Congzheng Song;Emiliano De Cristofaro;Vitaly Shmatikov|Computer Science
0ca26f9a98dda0abb737692f72ffa682df14cb2f|10.1109/TSP.2004.831016|IEEE Transactions on Signal Processing|2153-2164|Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.|1f6f3f05-6a23-42f0-8d31-98ab8089c1f2|IEEE Transactions on Signal Processing|Sparse Bayesian learning for basis selection|2004|2242717;144876925|D. Wipf;B. Rao|Mathematics;Computer Science
9d75cc322a4e06d0a3a868cb91b04219a289c12c|10.1257/JEP.31.2.87|Journal of Economic Perspectives|87-106|Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.|ff06c08d-4001-4156-91dc-db2dbc056067|Journal of Economic Perspectives|Machine Learning: An Applied Econometric Approach|2017|2062143;47281276|S. Mullainathan;Jann Spiess|Economics;Computer Science
3cee40494377c0e7d9c7c23a3811b481e55bce39|10.1093/nar/gkq275|Nucleic Acids Research|e132 - e132|We describe an algorithm for gene identification in DNA sequences derived from shotgun sequencing of microbial communities. Accurate ab initio gene prediction in a short nucleotide sequence of anonymous origin is hampered by uncertainty in model parameters. While several machine learning approaches could be proposed to bypass this difficulty, one effective method is to estimate parameters from dependencies, formed in evolution, between frequencies of oligonucleotides in protein-coding regions and genome nucleotide composition. Original version of the method was proposed in 1999 and has been used since for (i) reconstructing codon frequency vector needed for gene finding in viral genomes and (ii) initializing parameters of self-training gene finding algorithms. With advent of new prokaryotic genomes en masse it became possible to enhance the original approach by using direct polynomial and logistic approximations of oligonucleotide frequencies, as well as by separating models for bacteria and archaea. These advances have increased the accuracy of model reconstruction and, subsequently, gene prediction. We describe the refined method and assess its accuracy on known prokaryotic genomes split into short sequences. Also, we show that as a result of application of the new method, several thousands of new genes could be added to existing annotations of several human and mouse gut metagenomes.|da96c220-cd39-4894-bae6-d2d77c8e7aa9|Nucleic Acids Research|Ab initio gene identification in metagenomic sequences|2010|2113953;3194485;2715148|Wenhan Zhu;A. Lomsadze;M. Borodovsky|Medicine;Biology;Computer Science;Environmental Science
4f975da00a5b2a2f7236e34edcb7274e5fdab937|10.1126/science.aaf7894|Science|790 - 794|Measuring consumption and wealth remotely Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary. Science, this issue p. 790; see also p. 753 Satellites collect data that can be used to measure income and wealth. Reliable data on economic livelihoods remain scarce in the developing world, hampering efforts to study these outcomes and to design policies that improve them. Here we demonstrate an accurate, inexpensive, and scalable method for estimating consumption expenditure and asset wealth from high-resolution satellite imagery. Using survey and satellite data from five African countries—Nigeria, Tanzania, Uganda, Malawi, and Rwanda—we show how a convolutional neural network can be trained to identify image features that can explain up to 75% of the variation in local-level economic outcomes. Our method, which requires only publicly available data, could transform efforts to track and target poverty in developing countries. It also demonstrates how powerful machine learning techniques can be applied in a setting with limited training data, suggesting broad potential application across many scientific domains.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Combining satellite imagery and machine learning to predict poverty|2016|2752609;49240687;46215055;120334004;2465182;2490652|Neal Jean;M. Burke;Sang Michael Xie;W. Davis;D. Lobell;Stefano Ermon|Medicine;Environmental Science;Computer Science;Economics
d997919c30fa6711bc5c25cf8c8aea34fac27b91|10.1109/WACV.2016.7477553|2016 IEEE Winter Conference on Applications of Computer Vision (WACV)|1-10|Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.|acd15a6d-3248-41fb-8439-9a40aabe5608|IEEE Workshop/Winter Conference on Applications of Computer Vision|OpenFace: An open source facial behavior analysis toolkit|2016|1756344;2149814967;49933077|T. Baltrušaitis;P. Robinson;Louis-Philippe Morency|Computer Science
d133cb102ad0f81e3fd17a7db090b28afc124c4a|10.1103/PhysRevLett.120.145301|Physical review letters|"
          145301
        "|The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.|16c9f9d4-bee1-435d-8c85-22a3deba109d|Physical Review Letters|Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.|2017|49902007;2721737|T. Xie;J. Grossman|Chemistry;Medicine;Physics;Materials Science;Computer Science
a85e512d8845bd007b0866b4a97e8341463f8190|10.1109/TPAMI.2014.2321376|IEEE Transactions on Pattern Analysis and Machine Intelligence|2227-2240|For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Scalable Nearest Neighbor Algorithms for High Dimensional Data|2014|2658890;35238678|Marius Muja;D. Lowe|Medicine;Mathematics;Computer Science
b9518627db25f05930e931f56497602363a75491|10.1073/pnas.1900654116|Proceedings of the National Academy of Sciences|22071 - 22080|Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.|bb95bf2e-8383-4748-bf9d-d6906d091085|Proceedings of the National Academy of Sciences of the United States of America|Definitions, methods, and applications in interpretable machine learning|2019|144585578;145229121;19225295;1405625449;2116415778|W. James Murdoch;Chandan Singh;Karl Kumbier;R. Abbasi-Asl;Bin Yu|Medicine;Mathematics;Computer Science
4157ed3db4c656854e69931cb6089b64b08784b9|10.1109/MICRO.2014.58|2014 47th Annual IEEE/ACM International Symposium on Microarchitecture|609-622|Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.|0942fb86-c16f-4084-9902-10ddcfe18180|Micro|DaDianNao: A Machine-Learning Supercomputer|2014|7377735;2068286576;39419985;2145407329;37167270;2110368816;3353457;144049725;1719934;145550877;1731764|Yunji Chen;Tao Luo;Shaoli Liu;Shijin Zhang;Liqiang He;Jia Wang;Ling Li;Tianshi Chen;Zhiwei Xu;Ninghui Sun;O. Temam|Engineering;Computer Science
dd9b99fac67c18be82d7763a8fbf231fc3512423|10.1109/TEVC.2015.2504420|IEEE Transactions on Evolutionary Computation|606-626|Feature selection is an important task in data mining and machine learning to reduce the dimensionality of the data and increase the performance of an algorithm, such as a classification algorithm. However, feature selection is a challenging task due mainly to the large search space. A variety of methods have been applied to solve feature selection problems, where evolutionary computation (EC) techniques have recently gained much attention and shown some success. However, there are no comprehensive guidelines on the strengths and weaknesses of alternative approaches. This leads to a disjointed and fragmented field with ultimately lost opportunities for improving performance and successful applications. This paper presents a comprehensive survey of the state-of-the-art work on EC for feature selection, which identifies the contributions of these different algorithms. In addition, current issues and challenges are also discussed to identify promising areas for future research.|79644985-a91b-42a7-ac72-bb961c283f5e|IEEE Transactions on Evolutionary Computation|A Survey on Evolutionary Computation Approaches to Feature Selection|2016|144395433;145269712;2309030;143901532|Bing Xue;Mengjie Zhang;Will N. Browne;X. Yao|Computer Science
605402e235bd62437baf3c9ebefe77fb4d92ee95|10.1162/neco.1995.7.5.889|Neural Computation|889-904|Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|The Helmholtz Machine|1995|1790646;1695689;1764325;1804104|P. Dayan;Geoffrey E. Hinton;Radford M. Neal;R. Zemel|Mathematics;Medicine;Physics;Computer Science
ae523e2f137fa2a4f5a6cbcc443ba63db2642a96|10.1109/TASLP.2018.2842159|IEEE/ACM Transactions on Audio, Speech, and Language Processing|1702-1726|Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.|309e00f7-4bbd-461f-ab37-a90cd14ef21d|IEEE/ACM Transactions on Audio Speech and Language Processing|Supervised Speech Separation Based on Deep Learning: An Overview|2017|38053687;2855690|Deliang Wang;Jitong Chen|Medicine;Engineering;Computer Science
45557cc70cd6989ab6b03e5aeb787e34299099f7|10.1109/CVPR46437.2021.01501|2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|15257-15266|We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets’ real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Natural Adversarial Examples|2019|3422872;2074109526;104444594;5164568;143711382|Dan Hendrycks;Kevin Zhao;Steven Basart;J. Steinhardt;D. Song|Mathematics;Computer Science
175e37bca3762b3a52c6a0e153060b98a251d061|10.1126/science.aat2663|Science|360 - 365|The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Inverse molecular design using machine learning: Generative models for matter engineering|2018|1380248978;1380248954|Benjamín Sánchez-Lengeling;Alán Aspuru-Guzik|Medicine;Physics;Materials Science;Engineering;Computer Science
38f23fe236b152cd4983c8f30d305a568afd0d3e|10.1109/TNNLS.2020.3027314|IEEE Transactions on Neural Networks and Learning Systems|4793-4813|Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.|79c5a18d-0295-432c-aaa5-961d73de6d88|IEEE Transactions on Neural Networks and Learning Systems|A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI|2019|71352570;145836900|Erico Tjoa;Cuntai Guan|Medicine;Computer Science
eed9fa4483cab37eacd59db0fac4b1441431ee85|10.1109/TSP.2017.2690524|IEEE Transactions on Signal Processing|3551-3582|"Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=""LaTeX"">$(i,j,k,\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=""LaTeX"">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning."|1f6f3f05-6a23-42f0-8d31-98ab8089c1f2|IEEE Transactions on Signal Processing|Tensor Decomposition for Signal Processing and Machine Learning|2016|73776482;2217213;144406546;2349460;3000659;1702392|N. Sidiropoulos;L. D. Lathauwer;Xiao Fu;Kejun Huang;E. Papalexakis;C. Faloutsos|Mathematics;Computer Science
0ef9ae1ce8c91ce671a211bdda792bf3752d1522|10.1109/ACCESS.2017.2762418|IEEE Access|21954-21961|Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks|2017|30796665;1733625;2191560;50046030|Chuanlong Yin;Yuefei Zhu;Jin-long Fei;Xin-Zheng He|Engineering;Computer Science
7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7|10.1177/1745691617693393|Perspectives on Psychological Science|1100 - 1122|Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.|8ec18bc5-9c95-446c-a293-735d7ae1e3e9|Perspectives on Psychological Science|Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning|2017|2075675;48804181|T. Yarkoni;Jacob Westfall|Medicine;Psychology;Computer Science
694bdf6e5906992dad2987a3cc8d1a176de691c9|10.1109/CVPR46437.2021.01018|2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|10313-10322|Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be available at [1].|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|D-NeRF: Neural Radiance Fields for Dynamic Scenes|2020|49107901;3425624;1403428213;1397181875|Albert Pumarola;Enric Corona;Gerard Pons-Moll;F. Moreno-Noguer|Computer Science
8a5d0579590465494c9aba58a857af43b190b6a6|10.1109/COMST.2019.2904897|IEEE Communications Surveys & Tutorials|2224-2287|The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|Deep Learning in Mobile and Wireless Networking: A Survey|2018|3194878;144555592;1763096|Chaoyun Zhang;P. Patras;H. Haddadi|Engineering;Computer Science
8d1c588d202f150e1797ed113fba7e67bfa43ecb|10.1056/NEJMoa1204471|The New England journal of medicine|"
          1388-97
        "|"BACKGROUND
Persistent pain is measured by means of self-report, the sole reliance on which hampers diagnosis and treatment. Functional magnetic resonance imaging (fMRI) holds promise for identifying objective measures of pain, but brain measures that are sensitive and specific to physical pain have not yet been identified.


METHODS
In four studies involving a total of 114 participants, we developed an fMRI-based measure that predicts pain intensity at the level of the individual person. In study 1, we used machine-learning analyses to identify a pattern of fMRI activity across brain regions--a neurologic signature--that was associated with heat-induced pain. The pattern included the thalamus, the posterior and anterior insulae, the secondary somatosensory cortex, the anterior cingulate cortex, the periaqueductal gray matter, and other regions. In study 2, we tested the sensitivity and specificity of the signature to pain versus warmth in a new sample. In study 3, we assessed specificity relative to social pain, which activates many of the same brain regions as physical pain. In study 4, we assessed the responsiveness of the measure to the analgesic agent remifentanil.


RESULTS
In study 1, the neurologic signature showed sensitivity and specificity of 94% or more (95% confidence interval [CI], 89 to 98) in discriminating painful heat from nonpainful warmth, pain anticipation, and pain recall. In study 2, the signature discriminated between painful heat and nonpainful warmth with 93% sensitivity and specificity (95% CI, 84 to 100). In study 3, it discriminated between physical pain and social pain with 85% sensitivity (95% CI, 76 to 94) and 73% specificity (95% CI, 61 to 84) and with 95% sensitivity and specificity in a forced-choice test of which of two conditions was more painful. In study 4, the strength of the signature response was substantially reduced when remifentanil was administered.


CONCLUSIONS
It is possible to use fMRI to assess pain elicited by noxious heat in healthy persons. Future studies are needed to assess whether the signature predicts clinical pain. (Funded by the National Institute on Drug Abuse and others.)."|dc31f077-7737-4e33-baa3-bceeff44ec27|New England Journal of Medicine|An fMRI-based neurologic signature of physical pain.|2013|2549424;35244773;1754035;143761817;38550277;2334455|T. Wager;L. Atlas;M. Lindquist;M. Roy;Choong-Wan Woo;E. Kross|Medicine
c6bbfb4fcaecc779c899af4bb52083870f4b996a|10.1109/FIT.2012.53|2012 10th International Conference on Frontiers of Information Technology|257-260|The Internet is continuously changing and evolving. The main communication form of present Internet is human-human. The Internet of Things (IoT) can be considered as the future evaluation of the Internet that realizes machine-to-machine (M2M) learning. Thus, IoT provides connectivity for everyone and everything. The IoT embeds some intelligence in Internet-connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. This paper addresses the existing development trends, the generic architecture of IoT, its distinguishing features and possible future applications. This paper also forecast the key challenges associated with the development of IoT. The IoT is getting increasing popularity for academia, industry as well as government that has the potential to bring significant personal, professional and economic benefits.|b998506c-75bd-4118-8593-bbaaac35c3b4|International Conference on Frontiers of Information Technology|Future Internet: The Internet of Things Architecture, Possible Applications and Key Challenges|2012|1874286;1727042;33498709;2111266186|Rafiullah Khan;Sarmad Ullah Khan;R. Zaheer;Shahid Khan|Engineering;Computer Science;Environmental Science
37a67228271527037c9250ae3fd220199275e42e|10.1109/TSP.2016.2601299|IEEE Transactions on Signal Processing|794-816|This paper gives an overview of the majorization-minimization (MM) algorithmic framework, which can provide guidance in deriving problem-driven algorithms with low computational cost. A general introduction of MM is presented, including a description of the basic principle and its convergence results. The extensions, acceleration schemes, and connection to other algorithmic frameworks are also covered. To bridge the gap between theory and practice, upperbounds for a large number of basic functions, derived based on the Taylor expansion, convexity, and special inequalities, are provided as ingredients for constructing surrogate functions. With the pre-requisites established, the way of applying MM to solving specific problems is elaborated by a wide range of applications in signal processing, communications, and machine learning.|1f6f3f05-6a23-42f0-8d31-98ab8089c1f2|IEEE Transactions on Signal Processing|Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning|2017|48186551;145284465;1743931|Ying Sun;P. Babu;D. Palomar|Engineering;Computer Science;Mathematics
9670485f526f2254c0f34e64d9ca06f665a0bd17|10.21037/jtd.2020.02.64|Journal of Thoracic Disease|165 - 174|Background The coronavirus disease 2019 (COVID-19) outbreak originating in Wuhan, Hubei province, China, coincided with chunyun, the period of mass migration for the annual Spring Festival. To contain its spread, China adopted unprecedented nationwide interventions on January 23 2020. These policies included large-scale quarantine, strict controls on travel and extensive monitoring of suspected cases. However, it is unknown whether these policies have had an impact on the epidemic. We sought to show how these control measures impacted the containment of the epidemic. Methods We integrated population migration data before and after January 23 and most updated COVID-19 epidemiological data into the Susceptible-Exposed-Infectious-Removed (SEIR) model to derive the epidemic curve. We also used an artificial intelligence (AI) approach, trained on the 2003 SARS data, to predict the epidemic. Results We found that the epidemic of China should peak by late February, showing gradual decline by end of April. A five-day delay in implementation would have increased epidemic size in mainland China three-fold. Lifting the Hubei quarantine would lead to a second epidemic peak in Hubei province in mid-March and extend the epidemic to late April, a result corroborated by the machine learning prediction. Conclusions Our dynamic SEIR model was effective in predicting the COVID-19 epidemic peaks and sizes. The implementation of control measures on January 23 2020 was indispensable in reducing the eventual COVID-19 epidemic size.|8d7ff3bd-2385-47a4-8f55-b2c48363704b|Journal of Thoracic Disease|Modified SEIR and AI prediction of the epidemics trend of COVID-19 in China under public health interventions|2020|48598873;30489614;1753652328;113837271;2340666;40459179;2217848089;2125517703;2292805636;134905027;1492076802;1491359904;1390903788;89843211;145371438;40527353;2108989415;2115375292;1576228528;1663359881;123624101;1626824540;2145443647;2119049448;145002796;144869300|Zi-feng Yang;Zhiqi Zeng;Ke Wang;Sook-san Wong;W. Liang;Mark Zanin;Peng Liu;Xudong Cao;Zhongqiang Gao;Zhitong Mai;Jingyi Liang;Xiaoqing Liu;Shiyue Li;Yimin Li;F. Ye;W. Guan;Yifan Yang;Fei Li;S. Luo;Yuqi Xie;Bin Liu;Zhoulang Wang;Shaobo Zhang;Yao-dong Wang;N. Zhong;Jianxing He|Medicine;Environmental Science
2878d9936f494ed7d0c8aec47e9bcc5e51609f9a|10.1109/TNNLS.2015.2424995|IEEE Transactions on Neural Networks and Learning Systems|809-821|Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.|79c5a18d-0295-432c-aaa5-961d73de6d88|IEEE Transactions on Neural Networks and Learning Systems|Extreme Learning Machine for Multilayer Perceptron|2016|2967405;7175017;145678691|Jiexiong Tang;Chenwei Deng;G. Huang|Medicine;Computer Science
05fd1da7b2e34f86ec7f010bef068717ae964332|10.5555/1577069.1577070|J. Mach. Learn. Res.|1-40|Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Exploring Strategies for Training Deep Neural Networks|2009|1777528;1751762;2373952;3087941|H. Larochelle;Yoshua Bengio;J. Louradour;Pascal Lamblin|Mathematics;Computer Science
4f71ab367eb37cfd145d41327f7bb14077e5e7c5|10.1109/TGRS.2019.2907932|IEEE Transactions on Geoscience and Remote Sensing|6690-6709|Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral–spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.|70628d6a-97aa-4571-9701-bc0eb3989c32|IEEE Transactions on Geoscience and Remote Sensing|Deep Learning for Hyperspectral Image Classification: An Overview|2019|2116066317;145273596;38140728;2597809;2370080;1682001|Shutao Li;Weiwei Song;Leyuan Fang;Yushi Chen;Pedram Ghamisi;J. Benediktsson|Engineering;Computer Science;Environmental Science
5c5be36e3111e42247d78a6d529e4b1d7d2ced12|10.1109/ISBI.2011.5872394|2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro|230-233|Segmentation is the process of partitioning digital images into meaningful regions. The analysis of biological high content images often requires segmentation as a first step. We propose ilastik as an easy-to-use tool which allows the user without expertise in image processing to perform segmentation and classification in a unified way. ilastik learns from labels provided by the user through a convenient mouse interface. Based on these labels, ilastik infers a problem specific segmentation. A random forest classifier is used in the learning step, in which each pixel's neighborhood is characterized by a set of generic (nonlinear) features. ilastik supports up to three spatial plus one spectral dimension and makes use of all dimensions in the feature calculation. ilastik provides realtime feedback that enables the user to interactively refine the segmentation result and hence further fine-tune the classifier. An uncertainty measure guides the user to ambiguous regions in the images. Real time performance is achieved by multi-threading which fully exploits the capabilities of modern multi-core machines. Once a classifier has been trained on a set of representative images, it can be exported and used to automatically process a very large number of images (e.g. using the CellProfiler pipeline). ilastik is an open source project and released under the BSD license at www.ilastik.org.|a38e0d3d-6929-4868-b4e4-af8bbacf711e|IEEE International Symposium on Biomedical Imaging|Ilastik: Interactive learning and segmentation toolkit|2011|2059204163;145486652;1708103;1685187|Christoph Sommer;C. Straehle;U. Köthe;F. Hamprecht|Biology;Computer Science
5cbe278b65a81602a864184bbca37de91448a5f5|10.1126/science.abq1158|Science|1092 - 1097|Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.|f59506a8-d8bb-4101-b3d4-c4ac3ed03dad|Science|Competition-level code generation with AlphaCode|2022|47002813;2114950020;8270717;1684887;4337102;37212795;2152472076;2152469120;2058168486;49423009;2152469362;2067208983;2070068655;2154435638;2152471553;2256699276;1425082935;2421691;1851564;2071666;2079024030;152394142;2065370007;3187297;2152471960;143967473;2152472162;2152469135;2645384;1689108|Yujia Li;David Choi;Junyoung Chung;Nate Kushman;Julian Schrittwieser;Rémi Leblond;Tom;Eccles;James Keeling;Felix Gimeno;A. D. Lago;T. Hubert;Peter Choy;Cyprien de;Masson d’Autume;Igor Babuschkin;Xinyun Chen;Po-Sen Huang;Johannes Welbl;Sven Gowal;Alexey;Cherepanov;James Molloy;D. Mankowitz;Esme Sutherland Robson;Pushmeet Kohli;Nando de;Freitas;K. Kavukcuoglu;O. Vinyals|Medicine;Computer Science
2077d0f30507d51a0d3bbec4957d55e817d66a59|10.1109/CVPR.2005.160|2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)|860-867 vol. 2|We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Fields of Experts: a framework for learning image priors|2005|145920814;2105795|S. Roth;Michael J. Black|Computer Science
759d9a6c9206c366a8d94a06f4eb05659c2bb7f2|10.1109/TPAMI.2012.256|IEEE Transactions on Pattern Analysis and Machine Intelligence|1757-1772|To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of “closed set” recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is “open set” recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel “1-vs-set machine,” which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Toward Open Set Recognition|2013|2613438;145603848;27469806;32163276|W. Scheirer;A. Rocha;Archana Sapkota;T. Boult|Medicine;Computer Science
1904d633fca15140e35d893637232803b6dde6d9|10.1109/TKDE.2018.2876857|IEEE Transactions on Knowledge and Data Engineering|2346-2363|Concept drift describes unforeseeable changes in the underlying distribution of streaming data over time. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|Learning under Concept Drift: A Review|2019|144864069;1471737704;46279367;2056183624;143931014;46266495|Jie Lu;Anjin Liu;Fan Dong;Feng Gu;João Gama;Guangquan Zhang|Mathematics;Computer Science
86f0b58404a264a6216e29c78a5c113d900ca461|10.1109/TBME.2015.2496264|IEEE Transactions on Biomedical Engineering|1455-1462|Today, medical image analysis papers require solid experiments to prove the usefulness of proposed methods. However, experiments are often performed on data selected by the researchers, which may come from different institutions, scanners, and populations. Different evaluation measures may be used, making it difficult to compare the methods. In this paper, we introduce a dataset of 7909 breast cancer histopathology images acquired on 82 patients, which is now publicly available from http://web.inf.ufpr.br/vri/breast-cancer-database. The dataset includes both benign and malignant images. The task associated with this dataset is the automated classification of these images in two classes, which would be a valuable computer-aided diagnosis tool for the clinician. In order to assess the difficulty of this task, we show some preliminary results obtained with state-of-the-art image classification systems. The accuracy ranges from 80% to 85%, showing room for improvement is left. By providing this dataset and a standardized evaluation protocol to the scientific community, we hope to gather researchers in both the medical and the machine learning field to advance toward this clinical application.|7c832ea7-321d-4d93-bbaf-281e9d6649d7|IEEE Transactions on Biomedical Engineering|A Dataset for Breast Cancer Histopathological Image Classification|2016|32786132;144925520;144518944;1804638|F. Spanhol;Luiz Oliveira;C. Petitjean;L. Heutte|Medicine;Computer Science
546785490ac417be1f83ced6a8272e934934f411|10.1093/annonc/mdy166|Annals of Oncology|1836–1842|"Background
Deep learning convolutional neural networks (CNN) may facilitate melanoma detection, but data comparing a CNN's diagnostic performance to larger groups of dermatologists are lacking.


Methods
Google's Inception v4 CNN architecture was trained and validated using dermoscopic images and corresponding diagnoses. In a comparative cross-sectional reader study a 100-image test-set was used (level-I: dermoscopy only; level-II: dermoscopy plus clinical information and images). Main outcome measures were sensitivity, specificity and area under the curve (AUC) of receiver operating characteristics (ROC) for diagnostic classification (dichotomous) of lesions by the CNN versus an international group of 58 dermatologists during level-I or -II of the reader study. Secondary end points included the dermatologists' diagnostic performance in their management decisions and differences in the diagnostic performance of dermatologists during level-I and -II of the reader study. Additionally, the CNN's performance was compared with the top-five algorithms of the 2016 International Symposium on Biomedical Imaging (ISBI) challenge.


Results
In level-I dermatologists achieved a mean (±standard deviation) sensitivity and specificity for lesion classification of 86.6% (±9.3%) and 71.3% (±11.2%), respectively. More clinical information (level-II) improved the sensitivity to 88.9% (±9.6%, P = 0.19) and specificity to 75.7% (±11.7%, P < 0.05). The CNN ROC curve revealed a higher specificity of 82.5% when compared with dermatologists in level-I (71.3%, P < 0.01) and level-II (75.7%, P < 0.01) at their sensitivities of 86.6% and 88.9%, respectively. The CNN ROC AUC was greater than the mean ROC area of dermatologists (0.86 versus 0.79, P < 0.01). The CNN scored results close to the top three algorithms of the ISBI 2016 challenge.


Conclusions
For the first time we compared a CNN's diagnostic performance with a large international group of 58 dermatologists, including 30 experts. Most dermatologists were outperformed by the CNN. Irrespective of any physicians' experience, they may benefit from assistance by a CNN's image classification.


Clinical trial number
This study was registered at the German Clinical Trial Register (DRKS-Study-ID: DRKS00013570; https://www.drks.de/drks_web/)."|9777e708-5066-48fd-ab6a-57496cbca033|Annals of Oncology|Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists|2018|5016554;49903386;5580228;2220290014;2223688580;2154835838;34445787;2080112167;2113670271;5840101;2213770;153454907;4023283;6486577;32147295;8735139;2154835838;1404268156;40398486;2135937978;2056204271;1422494434;3026399;4039707;8720212;2135937679;50009466;1471206276;33340370;144821090;1401706768;6248858;46486553;51289782;46560749;49551702;51952495;32516743;32084575;3698245;16126046;2348967;2842984;12155787;12565475;2056679749;46322047;36030514;2420136;4198024;145295514;2135990561;123257978;38676513;37936174;49785210;2102253685;5580228;1485389319;2220290014;41065919;2136001275;146218865;2055574299;117461896;6288635|H. Haenssle;C. Fink;R. Schneiderbauer;F. Toberer;T. Buhl;A. Blum;Aadi Kalloo;A. Hassen;L. Thomas;A. Enk;L. Uhlmann;C. Alt;M. Arenbergerova;R. Bakos;Anne Baltzer;I. Bertlich;A. Blum;Therezia Bokor-Billmann;J. Bowling;Naira Braghiroli;R. Braun;K. Buder-Bakhaya;T. Buhl;H. Cabo;L. Čabrijan;Naciye Cevic;A. Classen;David Deltgen;C. Fink;I. Georgieva;L. Hakim-Meibodi;Susanne Hanner;Franziska Hartmann;J. Hartmann;G. Haus;E. Hoxha;R. Karls;H. Koga;J. Kreusch;A. Lallas;P. Majenka;A. Marghoob;C. Massone;L. Mekokishvili;D. Mestel;Volker Meyer;A. Neuberger;K. Nielsen;M. Oliviero;R. Pampena;J. Paoli;Erika Pawlik;B. Rao;Adriana Rendon;T. Russo;Ahmed Sadek;K. Samhaber;R. Schneiderbauer;A. Schweizer;F. Toberer;L. Trennheuser;Lyobomira Vlahova;Alexander Wald;J. Winkler;Priscila Wölbing;I. Zalaudek|Medicine;Computer Science
1f6d30772a94d978c9f81e2f7c1f4b0bdec117dd|10.1109/CVPR.2019.00046|2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|374-382|Modern machine learning suffers from \textit{catastrophic forgetting} when learning new classes incrementally. The performance dramatically degrades due to the missing data of old classes. Incremental learning methods have been proposed to retain the knowledge acquired from the old classes, by using knowledge distilling and keeping a few exemplars from the old classes. However, these methods struggle to \textbf{scale up to a large number of classes}. We believe this is because of the combination of two factors: (a) the data imbalance between the old and new classes, and (b) the increasing number of visually similar classes. Distinguishing between an increasing number of visually similar classes is particularly challenging, when the training data is unbalanced. We propose a simple and effective method to address this data imbalance issue. We found that the last fully connected layer has a strong bias towards the new classes, and this bias can be corrected by a linear model. With two bias parameters, our method performs remarkably well on two large datasets: ImageNet (1000 classes) and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms by 11.1\% and 13.2\% respectively.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|Large Scale Incremental Learning|2019|2119299240;2109306087;29957038;3105254;2145253136;3133575;46956675|Yue Wu;Yinpeng Chen;Lijuan Wang;Yuancheng Ye;Zicheng Liu;Yandong Guo;Y. Fu|Computer Science
91c380406f5a862b5937e70e720802e5c787968d|10.1162/coli.2008.34.1.125|Computational Linguistics|125-127|Text mining is the process of discovering information in large text collections, and automatically identifying interesting patterns and relationships in textual data. It is a relatively new research area, which has recently raised much interest among the research and industry communities, mainly due to the continuously increasing amount of information available on the Web and elsewhere. Text mining is a highly interdisci-plinary research area, bringing together research insights from the fields of data mining, natural language processing, machine learning, and information retrieval. In particular, text mining is closely related to the older area of data mining, which targets the extraction of interesting information from data records, although text mining is allegedly more difficult, as the source data consists of unstructured collections of documents rather than structured databases. The book by Feldman and Sanger is a thorough introduction to text mining, covering the general architecture of text mining systems, along with the main techniques used by such systems. It addresses both the theory and practice of text mining, and it illustrates the different techniques with real-world scenarios and practical applications. It is particularly relevant for students and professional practitioners, being structured as a self-contained handbook that does not require previous experience in any of the research fields involved. The book is structured into twelve chapters, which gradually introduce the area of text mining and related topics, starting with an introduction to the task of text mining, and ending with examples of practical applications from three different domains. The first chapter can be regarded as an overview of the book. It starts by defining the problem of text mining and the key elements in text mining: the document collections, the document features (words, terms, and concepts), and the role of background knowledge in text mining. It then briefly touches upon the possible applications of text mining, such as pattern discovery and trend analysis, and shortly discusses the interface layer of text mining systems. The second part of the chapter lays down the general architecture of a text mining system, which also serves as a rough guide for the rest of the book, as it describes the main components of a text mining system that are described in detail in subsequent chapters. Chapter 2 is one of the longest chapters in the book, and also one of the most dense in terms of newly introduced concepts. Despite being a more difficult read compared …|30a8645d-22d4-42e2-b3f6-304bf4ce3a02|International Conference on Computational Logic|Book Reviews: The Text Mining Handbook: Advanced Approaches to Analyzing Unstructured Data by Ronen Feldman and James Sanger|2008|145557251|Rada Mihalcea|Computer Science
6df11b0bb0244d4d36e8955436067cc5d19734fa|10.1109/TNNLS.2016.2599820|IEEE Transactions on Neural Networks and Learning Systems|2660-2673|Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.|79c5a18d-0295-432c-aaa5-961d73de6d88|IEEE Transactions on Neural Networks and Learning Systems|Evaluating the Visualization of What a Deep Neural Network Has Learned|2015|1699054;49345823;144535526;3633358;145034054|W. Samek;Alexander Binder;G. Montavon;S. Lapuschkin;K. Müller|Medicine;Computer Science
8a0f17e0ee66ad5f50cd35932747e6a806ef03cf|10.1177/117693510600200030|Cancer Informatics|59 - 77|Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.|9bb66e11-4952-4228-997b-bdf48fdc94d5|Cancer Informatics|Applications of Machine Learning in Cancer Prediction and Prognosis|2006|34468738;2066145|Joseph A. Cruz;D. Wishart|Medicine;Computer Science
06645d735b59b14479ae1d0392136bbf44227d0f|10.1609/AIMAG.V40I2.2850|AI Mag.|44-58|Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.|6fedff74-7525-4b7f-bbb4-4df4e23948e4|The AI Magazine|DARPA's Explainable Artificial Intelligence (XAI) Program|2019|2121780;1969847|David Gunning;D. Aha|Computer Science
5c5e69387020d7ca7d49487ca841958dc5e08ce6|10.1198/tech.2006.s353|Technometrics|147 - 148|counterpart as a special case, which leads to the construction of more universal ﬁlters that expand the capability and the robustness of data processing|ca4b3aef-bda7-47fe-b7e5-4f9cd6cb1947|Technometrics|The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning|2006|40618053|L. Deng|Mathematics;Computer Science
ea58af907495e97c93997119db4a59fab5cd3683|10.1109/MCI.2010.938364|IEEE Computational Intelligence Magazine|13-18|"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work."|ee372de7-efda-4907-a03f-359292ea27f6|IEEE Computational Intelligence Magazine|Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]|2010|1804314;2483864;1970334|I. Arel;Derek C. Rose;T. Karnowski|Computer Science
a20bfec3c95aad003dcb45a21a220c19cca8bb66|10.1162/089120101753342653|Computational Linguistics|521-544|In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.|30a8645d-22d4-42e2-b3f6-304bf4ce3a02|International Conference on Computational Logic|A Machine Learning Approach to Coreference Resolution of Noun Phrases|2001|2248565;34789794;3216372|Wee Meng Soon;H. Ng;Chung Yong Lim|Computer Science
12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4|10.1109/TSP.2004.830991|IEEE Transactions on Signal Processing|2165-2176|Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.|1f6f3f05-6a23-42f0-8d31-98ab8089c1f2|IEEE Transactions on Signal Processing|Online learning with kernels|2001|1700597;46234526;143957317|Jyrki Kivinen;Alex Smola;R. C. Williamson|Mathematics;Computer Science
a7a407968c13ced804a063259d72315a43b84f29|10.1109/ACCESS.2020.2988510|IEEE Access|75264-75278|The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Artificial Intelligence in Education: A Review|2020|1669767608;47978704;144264986|Lijia Chen;Pingping Chen;Zhijian Lin|Education;Computer Science
21dfbc88b21b27fe8a245ab1df98edd45f655ae7|10.1056/NEJMra1814259|The New England Journal of Medicine|1347–1358|Machine Learning in Medicine In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. The...|dc31f077-7737-4e33-baa3-bceeff44ec27|New England Journal of Medicine|Machine Learning in Medicine|2019|8638650;2056947059;1740538|A. Rajkomar;Jeffrey Dean;I. Kohane|Medicine;Computer Science
b5887d18420e8ac4f4fa4c83c4952138fd956702|10.1109/ACCESS.2020.2988796|IEEE Access|80716-80727|The k-means algorithm is generally the most known and used clustering method. There are various extensions of k-means to be proposed in the literature. Although it is an unsupervised learning to clustering in pattern recognition and machine learning, the k-means algorithm and its extensions are always influenced by initializations with a necessary number of clusters a priori. That is, the k-means algorithm is not exactly an unsupervised clustering method. In this paper, we construct an unsupervised learning schema for the k-means algorithm so that it is free of initializations without parameter selection and can also simultaneously find an optimal number of clusters. That is, we propose a novel unsupervised k-means (U-k-means) clustering algorithm with automatically finding an optimal number of clusters without giving any initialization and parameter selection. The computational complexity of the proposed U-k-means clustering algorithm is also analyzed. Comparisons between the proposed U-k-means and other existing methods are made. Experimental results and comparisons actually demonstrate these good aspects of the proposed U-k-means clustering algorithm.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Unsupervised K-Means Clustering Algorithm|2020|2300368826;1741064|Kristina P. Sinaga;Miin-Shen Yang|Computer Science
61e27dbae190b82639c57f180ecf97e4c46fcad9|10.1109/ACCESS.2020.2990567|IEEE Access|89497-89509|Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Pymoo: Multi-Objective Optimization in Python|2020|31519649;145080287|Julian Blank;K. Deb|Computer Science
16c0ef924da1f6b510c9c783ac764156f5a3d631|10.1109/TKDE.2020.2981314|IEEE Transactions on Knowledge and Data Engineering|50-70|Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|A Survey on Deep Learning for Named Entity Recognition|2018|39682944;1735962;3192562;2829009|J. Li;Aixin Sun;Jianglei Han;Chenliang Li|Computer Science
402f850dff86fb601d34b2841e6083ac0f928edd|10.1145/3079856.3080254|2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)|27-40|Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7× and 2.3×, respectively, over a comparably provisioned dense CNN accelerator.|deedf64a-dd5c-4b33-b345-ff83bfb93d71|International Symposium on Computer Architecture|SCNN: An accelerator for compressed-sparse convolutional neural networks|2017|1790421;1998820;3374545;2274681;3172075;2125244;1775477;1715863;80724002|A. Parashar;Minsoo Rhu;Anurag Mukkara;A. Puglielli;Rangharajan Venkatesan;Brucek Khailany;J. Emer;S. Keckler;W. Dally|Engineering;Computer Science
24e6c5bfe9bb0751e5708b501d04e860011b2953|10.21873/CGP.20063|Cancer genomics & proteomics|"
          41-51
        "|Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.|d03d0f51-5944-496b-9d0d-180dbc4edddc|Cancer Genomics & Proteomics|Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.|2018|47156522;27122362;47479574;32286482;50232365|Shujun Huang;Nianguang Cai;Pedro Penzuti Pacheco;Shavira Narrandes;Yang Wang;Wayne W. Xu|Medicine;Biology;Computer Science
93884d89dfc8c3886f642018227a43fb7b58044f|10.1109/TKDE.2022.3178128|IEEE Transactions on Knowledge and Data Engineering|8052-8072|Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.|c6840156-ee10-4d78-8832-7f8909811576|IEEE Transactions on Knowledge and Data Engineering|Generalizing to Unseen Domains: A Survey on Domain Generalization|2021|1519290245;40093162;2144545128;1796267433;143826491|Jindong Wang;Cuiling Lan;Chang Liu;Yidong Ouyang;Tao Qin|Computer Science
d517b13f2b152c913b81ce534a149493517dbdad|10.1109/ACCESS.2014.2325029|IEEE Access|514-525|Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Big Data Deep Learning: Challenges and Perspectives|2014|2145447101;39376164|Xue-wen Chen;Xiaotong Lin|Computer Science
4e6238c8613b5b81f81552939bce33296aedfbfe|10.5555/1756006.1859912|J. Mach. Learn. Res.|1803-1831|After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|How to Explain Individual Classification Decisions|2009|1742375;34954622;1734990;1716788;39960184;145034054|D. Baehrens;T. Schroeter;S. Harmeling;M. Kawanabe;K. Hansen;K. Müller|Mathematics;Computer Science
d05d86db86a4ac0d95e6dcd951b42a9651939793|10.1109/ACCESS.2019.2895334|IEEE Access|41525-41550|Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01–0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Deep Learning Approach for Intelligent Intrusion Detection System|2019|51162051;2474250;2285465435;2916235;1399133087;145710905|R. Vinayakumar;M. Alazab;I. K. P. S. Senior Member;P. Poornachandran;Ameer Al-Nemrat;S. Venkatraman|Engineering;Computer Science
877374c2913b787ee9f958f39e31c75d39ebcc15|10.1109/MSP.2012.2183771|IEEE Signal Processing Magazine|101-116|The ever-increasing demand for higher data rates in wireless communications in the face of limited or underutilized spectral resources has motivated the introduction of cognitive radio. Traditionally, licensed spectrum is allocated over relatively long time periods and is intended to be used only by licensees. Various measurements of spectrum utilization have shown substantial unused resources in frequency, time, and space [1], [2]. The concept behind cognitive radio is to exploit these underutilized spectral resources by reusing unused spectrum in an opportunistic manner [3], [4]. The phrase cognitive radio is usually attributed to Mitola [4], but the idea of using learning and sensing machines to probe the radio spectrum was envisioned several decades earlier (cf., [5]).|f62e5eab-173a-4e0a-a963-ed8de9835d22|IEEE Signal Processing Magazine|Spectrum Sensing for Cognitive Radio : State-of-the-Art and Recent Advances|2012|2983295;143668698;1701766;145967056|E. Axell;G. Leus;E. Larsson;H. Poor|Engineering;Computer Science
b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57|10.14778/2212351.2212354|Proc. VLDB Endow.|716-727|"While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. 
 
We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations."|fcbcaf18-8ab1-43e1-a973-604bbc7e344e|Proceedings of the VLDB Endowment|Distributed GraphLab: A Framework for Machine Learning in the Cloud|2012|1680638;2119113835;1717990;1741745;1730156;1695576|Yucheng Low;Joseph Gonzalez;Aapo Kyrola;Danny Bickson;Carlos Guestrin;J. Hellerstein|Computer Science
441c31274f4535a4a50892c1ad6e19eacfd17f8c|10.1063/1.4966192|The Journal of chemical physics|"
          170901
        "|Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.|1bb63b2b-3f57-4387-aaf6-b2a33dfcdcc5|Journal of Chemical Physics|Perspective: Machine learning potentials for atomistic simulations.|2016|144136091|J. Behler|Medicine;Chemistry;Computer Science;Materials Science
0ba86604228b555475496e200f31878df3aabd6e|10.1145/3191513|Communications of the ACM|103 - 115|Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.|bdc2e585-4e48-4e36-8af1-6d859763d405|AAAI Conference on Artificial Intelligence|Never-Ending Learning|2015|40975594;50056360;1842532;2406435;2119660368;31779043;143818235;40135250;40642935;16411658;2517825;1914797;2406799;35645263;3115592;144888672;1863425;32402038;1717452;2108772203;2129412;1726095131;39717886;2407368;2062798496;122360608|Tom Michael Mitchell;William W. Cohen;Estevam Hruschka;Partha P. Talukdar;Bo Yang;J. Betteridge;Andrew Carlson;Bhavana Dalvi;Matt Gardner;B. Kisiel;Jayant Krishnamurthy;N. Lao;Kathryn Mazaitis;Thahir Mohamed;Ndapandula Nakashole;Emmanouil Antonios Platanios;Alan Ritter;M. Samadi;Burr Settles;Richard C. Wang;Derry Tanti Wijaya;A. Gupta;Xinlei Chen;Abulhair Saparov;Malcolm Greaves;Joel Welling|Psychology;Linguistics;Computer Science
cbac8b0d82ea8e9251d5530695841d816cb196b9|10.21105/JOSS.01026|J. Open Source Softw.|1026|Python is currently the fastest growing programming language in the world, thanks to its ease-of-use, fast learning curve and its numerous high quality packages for data science and machine-learning. Surprisingly however, Python is far behind the R programming language when it comes to general statistics and for this reason many scientists still rely heavily on R to perform their statistical analyses.|1236e136-01b7-42d5-8c4a-593153a3ab37|Journal of Open Source Software|Pingouin: statistics in Python|2018|2095214091|Raphael Vallat|Mathematics;Computer Science
adc61e21eafecfbf6ebecc570f9f913659a2bfb2|10.1145/3439726|ACM Computing Surveys (CSUR)|1 - 40|Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.|7b2adce0-d53f-49d6-8784-b0645604fe62|ACM Computing Surveys|Deep Learning--based Text Classification|2020|2164604;49943757;48441311|Shervin Minaee;E. Cambria;Jianfeng Gao|Mathematics;Computer Science
abe8a57dc27598937c2cffde3fc21c1e6d1f11ce|10.1109/JBHI.2017.2767063|IEEE Journal of Biomedical and Health Informatics|1589-1604|The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research.|eac74c9c-a5c0-417d-8088-8164a6a8bfb3|IEEE journal of biomedical and health informatics|Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis|2017|3383528;40449634;5484714;1715006|B. Shickel;P. Tighe;A. Bihorac;Parisa Rashidi|Mathematics;Medicine;Computer Science
c6a83c4fcc99ba6753109301949c5b7cfa978079|10.1162/0891201042544884|Computational Linguistics|417-449|A phrase-based statistical machine translation approach the alignment template approach is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various system components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.|30a8645d-22d4-42e2-b3f6-304bf4ce3a02|International Conference on Computational Logic|The Alignment Template Approach to Statistical Machine Translation|2004|2002316;145322333|F. Och;H. Ney|Linguistics;Computer Science
94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b|10.1145/3472291|ACM Computing Surveys (CSUR)|1 - 40|Active learning (AL) attempts to maximize a model’s performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.|7b2adce0-d53f-49d6-8784-b0645604fe62|ACM Computing Surveys|A Survey of Deep Active Learning|2020|51056374;39924919;144950946;2319973;49969948;2466164;2153688316|Pengzhen Ren;Yun Xiao;Xiaojun Chang;Po-Yao (Bernie) Huang;Zhihui Li;Xiaojiang Chen;Xin Wang|Mathematics;Computer Science
3df952d4a724655f7520ff95d4b2cef90fff0cae|10.1145/3359786|Communications of the ACM|68 - 77|Uncovering the mysterious ways machine learning models make decisions.|4d9ce1c4-dc84-46b9-903e-e3751c00c7dd|Communications of the ACM|Techniques for interpretable machine learning|2018|3432460;47717322;48539382|Mengnan Du;Ninghao Liu;Xia Hu|Mathematics;Computer Science
01f29addca4dc6f189f903cb133dea7585813a6f|10.1109/COMST.2021.3077737|IEEE Communications Surveys & Tutorials|1546-1577|Reconfigurable intelligent surfaces (RISs), also known as intelligent reflecting surfaces (IRSs), or large intelligent surfaces (LISs),1 have received significant attention for their potential to enhance the capacity and coverage of wireless networks by smartly reconfiguring the wireless propagation environment. Therefore, RISs are considered a promising technology for the sixth-generation (6G) of communication networks. In this context, we provide a comprehensive overview of the state-of-the-art on RISs, with focus on their operating principles, performance evaluation, beamforming design and resource management, applications of machine learning to RIS-enhanced wireless networks, as well as the integration of RISs with other emerging technologies. We describe the basic principles of RISs both from physics and communications perspectives, based on which we present performance evaluation of multiantenna assisted RIS systems. In addition, we systematically survey existing designs for RIS-enhanced wireless networks encompassing performance analysis, information theory, and performance optimization perspectives. Furthermore, we survey existing research contributions that apply machine learning for tackling challenges in dynamic scenarios, such as random fluctuations of wireless channels and user mobility in RIS-enhanced wireless networks. Last but not least, we identify major issues and research opportunities associated with the integration of RISs and other emerging technologies for applications to next-generation networks.1Without loss of generality, we use the name of RIS in the remainder of this paper.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|Reconfigurable Intelligent Surfaces: Principles and Opportunities|2020|47909642;2111310469;7837350;49097095;2047464842;121644245;1390094118|Yuanwei Liu;Xiao Liu;Xidong Mu;Tianwei Hou;Jiaqi Xu;M. Di Renzo;N. Al-Dhahir|Engineering;Computer Science
33e46a618fdb22d46951f548d6ceeb384e7f1687|10.1109/TPAMI.2008.75|IEEE Transactions on Pattern Analysis and Machine Intelligence|1713-1727|We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches.|25248f80-fe99-48e5-9b8e-9baef3b8e23b|IEEE Transactions on Pattern Analysis and Machine Intelligence|Pedestrian Detection via Classification on Riemannian Manifolds|2008|2577513;29905643;145776090|Oncel Tuzel;F. Porikli;P. Meer|Medicine;Mathematics;Computer Science
c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7|10.1109/CSF.2018.00027|2018 IEEE 31st Computer Security Foundations Symposium (CSF)|268-282|Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.|a41e4fed-169a-464b-bdde-dc058a258159|IEEE Computer Security Foundations Symposium|Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting|2017|26378728;3025831;2623167;1680133|Samuel Yeom;Irene Giacomelli;Matt Fredrikson;S. Jha|Computer Science
d63b884d5ebc739f6e1bdf861fa9276260781404|10.1109/COMST.2018.2844341|IEEE Communications Surveys & Tutorials|2923-2960|In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|Deep Learning for IoT Big Data and Streaming Analytics: A Survey|2017|4488990;1404786833;1683936;145837053|M. Mohammadi;Ala I. Al-Fuqaha;Sameh Sorour;M. Guizani|Engineering;Computer Science;Environmental Science
2bc3644ce4de7fce5812c1455e056649a47c1bbf|10.1109/ACCESS.2019.2923707|IEEE Access|81542-81554|Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques|2019|150302778;9727014;144369609|Senthilkumar Mohan;Chandrasegar Thirumalai;Gautam Srivastava|Medicine;Computer Science
64be9999b68e12d260ba7423f6b55ffd41552ad3|10.1109/ACCESS.2017.2788044|IEEE Access|9375-9389|The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.|2633f5b2-c15c-49fe-80f5-07523e770c26|IEEE Access|Deep Learning Applications in Medical Image Analysis|2018|34812292;46659335;39917910;48508646|Justin Ker;Lipo Wang;J. Rao;Tchoyoson C. C. Lim|Medicine;Computer Science
0c3751db5a24c636c1aa8abfd9d63321b38cfce5|10.5555/2567709.2502598|J. Mach. Learn. Res.|567-599|Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Stochastic dual coordinate ascent methods for regularized loss|2012|1389955537;2300131692|Shai Shalev-Shwartz;Tong Zhang|Computer Science;Mathematics
1f87134a630c2dbb9a3645ba658954f00b620a77|10.1109/ICCV.2011.6126251|2011 International Conference on Computer Vision|263-270|Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (accurate estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we are able to avoid the need for an intermediate classification step. Our method uses a kernelized structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow for real-time application, we introduce a budgeting mechanism which prevents the unbounded growth in the number of support vectors which would otherwise occur during tracking. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased performance.|4144b5fb-0a80-4663-8ebf-80ca0c47231a|Vision|Struck: Structured output tracking with kernels|2011|1837057;1741702;143635540|Sam Hare;Amir Saffari;Philip H. S. Torr|Medicine;Computer Science
265644f1b6740ca34bfbe9762b90b33021adde62|10.3348/kjr.2017.18.4.570|Korean Journal of Radiology|570 - 584|The artificial neural network (ANN)–a machine learning technique inspired by the human neuronal synapse system–was introduced in the 1950s. However, the ANN was previously limited in its ability to solve actual problems, due to the vanishing gradient and overfitting problems with training of deep architecture, lack of computing power, and primarily the absence of sufficient data to train the computer system. Interest in this concept has lately resurfaced, due to the availability of big data, enhanced computing power with the current graphics processing units, and novel algorithms to train the deep neural network. Recent studies on this technology suggest its potentially to perform better than humans in some visual and auditory recognition tasks, which may portend its applications in medicine and healthcare, especially in medical imaging, in the foreseeable future. This review article offers perspectives on the history, development, and applications of deep learning technology, particularly regarding its applications in medical imaging.|8f35a116-5553-4e6a-8c69-58abf9971707|Korean Journal of Radiology|Deep Learning in Medical Imaging: General Overview|2017|120704132;2052576935;2116632761;46901084;38628528;46844846;145979410|June-Goo Lee;Sanghoon Jun;Younghoon Cho;Hyunna Lee;G. Kim;J. Seo;Namkug Kim|Medicine;Computer Science
e7e25fd534e9e024da329aea546484938df305a5|10.5555/1756006.1953029|J. Mach. Learn. Res.|3011-3015|The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Gaussian Processes for Machine Learning (GPML) Toolbox|2010|3472959;1748758|C. Rasmussen;H. Nickisch|Mathematics;Computer Science
65d53938a12c77e7920b8eb3a49df249c978ba3f|10.1109/TASLP.2020.3030497|IEEE/ACM Transactions on Audio, Speech, and Language Processing|2880-2894|Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.|309e00f7-4bbd-461f-ab37-a90cd14ef21d|IEEE/ACM Transactions on Audio Speech and Language Processing|PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition|2019|8391640;2107994160;35965227;2115828379;144144027;1804703|Qiuqiang Kong;Yin Cao;Turab Iqbal;Yuxuan Wang;Wenwu Wang;Mark D. Plumbley|Engineering;Computer Science
40f391bc3860e8cd71d7ea68ec11d80d5e12c02c|10.5555/2627435.2638581|J. Mach. Learn. Res.|1455-1459|"Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. 
 
The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. By dealing internally with most of the differential geometry, the package aims particularly at lowering the entrance barrier."|c22e7c36-3bfa-43e1-bb7b-edccdea2a780|Journal of machine learning research|Manopt, a matlab toolbox for optimization on manifolds|2013|2418520;37585320;50356391;1707966|Nicolas Boumal;Bamdev Mishra;P. Absil;R. Sepulchre|Mathematics;Engineering;Computer Science
18bc1d4271abe8dd6e16179cdb06524a4f396e16|10.14778/3157794.3157797|Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases|"
          269-282
        "|Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.|fcbcaf18-8ab1-43e1-a973-604bbc7e344e|Proceedings of the VLDB Endowment|Snorkel: Rapid Training Data Creation with Weak Supervision|2017|143711421;2870504;33918804;31592365;144766615;2114485554|Alexander J. Ratner;Stephen H. Bach;Henry R. Ehrenberg;Jason Alan Fries;Sen Wu;C. Ré|Medicine;Mathematics;Computer Science
5a391667242b4a631acdd5917681b16a86523987|10.1109/TGRS.2016.2636241|IEEE Transactions on Geoscience and Remote Sensing|3639-3655|In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.|70628d6a-97aa-4571-9701-bc0eb3989c32|IEEE Transactions on Geoscience and Remote Sensing|Deep Recurrent Neural Networks for Hyperspectral Image Classification|2017|35041003;2370080;46875441|Lichao Mou;Pedram Ghamisi;Xiaoxiang Zhu|Engineering;Computer Science;Environmental Science
885af28a751553be48a25b411a5d492767d4cf65|10.1109/TIFS.2011.2175919|IEEE Transactions on Information Forensics and Security|432-444|Today, the most accurate steganalysis methods for digital media are built as supervised classifiers on feature vectors extracted from the media. The tool of choice for the machine learning seems to be the support vector machine (SVM). In this paper, we propose an alternative and well-known machine learning tool-ensemble classifiers implemented as random forests-and argue that they are ideally suited for steganalysis. Ensemble classifiers scale much more favorably w.r.t. the number of training examples and the feature dimensionality with performance comparable to the much more complex SVMs. The significantly lower training complexity opens up the possibility for the steganalyst to work with rich (high-dimensional) cover models and train on larger training sets-two key elements that appear necessary to reliably detect modern steganographic algorithms. Ensemble classification is portrayed here as a powerful developer tool that allows fast construction of steganography detectors with markedly improved detection accuracy across a wide range of embedding methods. The power of the proposed framework is demonstrated on three steganographic methods that hide messages in JPEG images.|d406a3f4-dc05-43be-b1f6-812f29de9c0e|IEEE Transactions on Information Forensics and Security|Ensemble Classifiers for Steganalysis of Digital Media|2012|1808384;1751812;37127008|Jan Kodovský;J. Fridrich;Vojtech Holub|Computer Science
58a8bead87c8c1e37460dce28285c053c270f6e7|10.1145/2743025|ACM Transactions on Intelligent Systems and Technology (TIST)|1 - 41|The advances in location-acquisition and mobile computing techniques have generated massive spatial trajectory data, which represent the mobility of a diversity of moving objects, such as people, vehicles, and animals. Many techniques have been proposed for processing, managing, and mining trajectory data in the past decade, fostering a broad range of applications. In this article, we conduct a systematic survey on the major research into trajectory data mining, providing a panorama of the field as well as the scope of its research topics. Following a road map from the derivation of trajectory data, to trajectory data preprocessing, to trajectory data management, and to a variety of mining tasks (such as trajectory pattern mining, outlier detection, and trajectory classification), the survey explores the connections, correlations, and differences among these existing techniques. This survey also introduces the methods that transform trajectories into other data formats, such as graphs, matrices, and tensors, to which more data mining and machine learning techniques can be applied. Finally, some public trajectory datasets are presented. This survey can help shape the field of trajectory data mining, providing a quick understanding of this field to the community.|0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e|ACM Transactions on Intelligent Systems and Technology|Trajectory Data Mining|2015|145473095|Yu Zheng|Geography;Engineering;Computer Science
7e7eb0f93c9550d7336f4bbfad5fe89604295705|10.1103/PhysRevLett.122.040504|Physical review letters|"
          040504
        "|A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.|16c9f9d4-bee1-435d-8c85-22a3deba109d|Physical Review Letters|Quantum Machine Learning in Feature Hilbert Spaces.|2018|3048564;3399181|M. Schuld;N. Killoran|Medicine;Physics;Computer Science
bf7dcbee272428a2aa3c534200743ff7ab2047f8|10.1177/0037549705058073|Simulation|517 - 527|MASON is a fast, easily extensible, discrete-event multi-agent simulation toolkit in Java, designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments. MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run. This paper describes the MASON system, its motivation, and its basic architectural design. It then compares MASON to related multi-agent libraries in the public domain, and discusses six applications of the system built over the past year which suggest its breadth of utility.|54c097e2-05f2-4b31-9c77-1cc7b20ad1cf|International Conference on Advances in System Simulation|MASON: A Multiagent Simulation Environment|2005|1706276;1403820685;1703826;153850291;35149981|S. Luke;C. Cioffi-Revilla;Liviu Panait;Keith Sullivan;G. Balan|Engineering;Computer Science
f9d119346b0773ea83251598fa5305bc75bac8ab|10.1109/JSTARS.2015.2388577|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|2381-2392|Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification.|849b6687-df71-4d12-9c46-59f45d5ce951|IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing|Spectral–Spatial Classification of Hyperspectral Data Based on Deep Belief Network|2015|2597809;2143711163;144787387|Yushi Chen;Xing Zhao;X. Jia|Engineering;Computer Science;Environmental Science
7ea35b35392c6ef5738635cec7d17b24fe3e4f04|10.1093/nsr/nwy108|National Science Review|74 - 86|Abstract Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation.|67f7f831-711a-43c8-8785-1e09005359b5|International Joint Conference on Artificial Intelligence|Deep forest|2017|145624000;2108993600|Zhi-Hua Zhou;Ji Feng|Medicine;Mathematics;Computer Science
971766088dfaf63fb55e6f0190b14f28f2c98ad0|10.1109/COMST.2015.2494502|IEEE Communications Surveys & Tutorials|1153-1176|This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.|95d0dda7-5d58-4afd-b59f-315447b81992|IEEE Communications Surveys and Tutorials|A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection|2016|2343019;1922657|A. Buczak;Erhan Guven|Engineering;Computer Science
22adb2413901b74128f2a02584dafa77afbd8d60|10.1109/TII.2018.2864759|IEEE Transactions on Industrial Informatics|2446-2455|We develop a novel deep learning framework to achieve highly accurate machine fault diagnosis using transfer learning to enable and accelerate the training of deep neural network. Compared with existing methods, the proposed method is faster to train and more accurate. First, original sensor data are converted to images by conducting a Wavelet transformation to obtain time-frequency distributions. Next, a pretrained network is used to extract lower level features. The labeled time-frequency images are then used to fine-tune the higher levels of the neural network architecture. This paper creates a machine fault diagnosis pipeline and experiments are carried out to verify the effectiveness and generalization of the pipeline on three main mechanical datasets including induction motors, gearboxes, and bearings with sizes of 6000, 9000, and 5000 time series samples, respectively. We achieve state-of-the-art results on each dataset, with most datasets showing test accuracy near 100%, and in the gearbox dataset, we achieve significant improvement from 94.8% to 99.64%. We created a repository including these datasets located at mlmechanics.ics.uci.edu.|2135230a-3b24-4b71-9583-60624389377a|IEEE Transactions on Industrial Informatics|Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning|2019|9122533;143953836;35374692;144902513|Siyu Shao;S. McAleer;Ruqiang Yan;P. Baldi|Engineering;Computer Science
5aefde4203ce46ea900a96835a7c59a5f50800e7|10.1021/ci500747n|Journal of chemical information and modeling|"
          263-74
        "|Neural networks were widely used for quantitative structure-activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Merck's drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.|3f16aef5-6b9f-4f87-baca-cbf8147e352f|Journal of Chemical Information and Modeling|Deep Neural Nets as a Method for Quantitative Structure-Activity Relationships|2015|47793005;1838644;2587444;35188630;3305021|Junshui Ma;R. Sheridan;Andy Liaw;George E. Dahl;V. Svetnik|Medicine;Chemistry;Computer Science
96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7|10.21248/jlcl.20.2005.68|LDV Forum|19-62|The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.|627ec63d-e751-456d-852e-9431d007919a|Ldv Forum|A Brief Survey of Text Mining|2005|1792623;1759689;1683459|A. Hotho;A. Nürnberger;G. Paass|Linguistics;Computer Science
908cca0abefc35acc38033603714fbb1bcadc49d|10.1109/cvpr42600.2020.00028|2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|200-210|This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code is available at: https://github.com/facebookresearch/SlowFast.|768b87bb-8a18-4d9c-a161-4d483c776bcf|Computer Vision and Pattern Recognition|X3D: Expanding Architectures for Efficient Video Recognition|2020|2322150|Christoph Feichtenhofer|Computer Science
1051280d2b825c04f27d231aba0f8284bb297880|10.1109/TSE.2005.112|IEEE Transactions on Software Engineering|897-910|Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.|c99cfe66-b71c-4ca4-bedd-26267b9cb068|IEEE Transactions on Software Engineering|Empirical validation of object-oriented metrics on open source software for fault prediction|2005|1681248;3172077;1868524|T. Gyimóthy;R. Ferenc;István Siket|Computer Science
e3a617f1848f8df98d28472e32d9c2ec3dcb8ad3|10.1142/S0219622006002258|Int. J. Inf. Technol. Decis. Mak.|597-604|In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.|8a7c9905-f8cf-4711-bca6-0ef69aef9823|International Journal of Information Technology and Decision Making|10 Challenging Problems in Data Mining Research|2006|152290618;1748808|Qiang Yang;Xindong Wu|Computer Science
4854c63f0e20c01ee07b5ef4ffb8bdb8671e4895|10.1109/ROBOT.1989.100065|Neural Computation|253-262|Most animals have significant behavioral expertise built in without having to explicitly learn it all from scratch. This expertise is a product of evolution of the organism; it can be viewed as a very long-term form of learning which provides a structured system within which individuals might learn more specialized skills or abilities. This paper suggests one possible mechanism for analagous robot evolution by describing a carefully designed series of networks, each one being a strict augmentation of the previous one, which control a six-legged walking machine capable of walking over rough terrain and following a person passively sensed in the infrared spectrum. As the completely decentralized networks are augmented, the robot's performance and behavior repertoire demonstrably improve. The rationale for such demonstrations is that they may provide a hint as to the requirements for automatically building massive networks to carry out complex sensory-motor tasks. The experiments with an actual robot ensure that an essence of reality is maintained and that no critical disabling problems have been ignored.|69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3|Neural Computation|A Robot that Walks; Emergent Behaviors from a Carefully Evolved Network|1989|72419159|R. Brooks|Engineering;Computer Science
825ca26af5a2a510dbc1a7b97587212bc98ae968|10.1609/AIMAG.V35I4.2513|AI Mag.|105-120|Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.|6fedff74-7525-4b7f-bbb4-4df4e23948e4|The AI Magazine|Power to the People: The Role of Humans in Interactive Machine Learning|2014|1719124;35096370;144288136;1847827|Saleema Amershi;M. Cakmak;W. B. Knox;Todd Kulesza|Computer Science
799f927692a6c08c5e630bea78c087c5051528fc|10.1177/1745691612454304|Perspectives on Psychological Science|464 - 481|A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient “active learning” algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.|8ec18bc5-9c95-446c-a293-735d7ae1e3e9|Perspectives on Psychological Science|Self-Directed Learning|2012|3013567;3164019|T. Gureckis;D. Markant|Medicine;Psychology;Education;Computer Science
a62c2e5e1ebb68559a5287e1bce4f28f6b76d9c1|10.1561/0600000035|Found. Trends Comput. Graph. Vis.|81-227|"This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks. 
 
Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning, and active learning under the same decision forest framework. This gives us the opportunity to write and optimize the core implementation only once, with application to many diverse tasks. 
 
The proposed model may be used both in a discriminative or generative way and may be applied to discrete or continuous, labeled or unlabeled data. 
 
The main contributions of this review are: (1) Proposing a unified, probabilistic and efficient model for a variety of learning tasks; (2) Demonstrating margin-maximizing properties of classification forests; (3) Discussing probabilistic regression forests in comparison with other nonlinear regression algorithms; (4) Introducing density forests for estimating probability density functions; (5) Proposing an efficient algorithm for sampling from a density forest; (6) Introducing manifold forests for nonlinear dimensionality reduction; (7) Proposing new algorithms for transductive learning and active learning. Finally, we discuss how alternatives such as random ferns and extremely randomized trees stem from our more general forest model. 
 
This document is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in the new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state-of-the-art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed. The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility."|a21f6aaa-21ef-418d-b2dd-e56ce6e16570|Foundations and Trends in Computer Graphics and Vision|Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning|2012|1716777;143774737;1796918|A. Criminisi;J. Shotton;E. Konukoglu|Medicine;Computer Science
